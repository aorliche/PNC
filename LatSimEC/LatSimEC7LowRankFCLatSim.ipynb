{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524c02a9",
   "metadata": {},
   "source": [
    "# NOT the latest file, the latest is on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45617878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'wrat', 'missingage', 'missingsex', 'missingwrat', 'failedqc']\n",
      "['emoid', 'nback', 'rest']\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7805f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922\n",
      "(922, 264, 210)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get subjects that have all tasks and paras specified\n",
    "Functions for creating independent and response variables\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_subs(allts, metadict, tasks, paras):\n",
    "    # Get subs for all paras\n",
    "    for i,para in enumerate(paras):\n",
    "        tmpset = set([int(sub[4:]) for sub in allts[para].keys()])\n",
    "        if i == 0:\n",
    "            paraset = tmpset\n",
    "        else:\n",
    "            paraset = paraset.intersection(tmpset)\n",
    "    # Get subs for all tasks\n",
    "    for i,task in enumerate(tasks):\n",
    "        tmpset = set([sub for sub in metadict[task].keys()])\n",
    "        if i == 0:\n",
    "            taskset = tmpset\n",
    "        else:\n",
    "            taskset = paraset.intersection(tmpset)\n",
    "    # Remove QC failures\n",
    "    allsubs = taskset.intersection(paraset)\n",
    "    for badsub in metadict['failedqc']:\n",
    "        try:\n",
    "            allsubs.remove(int(badsub[4:]))\n",
    "        except:\n",
    "            pass\n",
    "    return allsubs\n",
    "\n",
    "def get_X(allts, paras, subs):\n",
    "    X = []\n",
    "    for para in paras:\n",
    "        pX = [allts[para][f'sub-{sub}'] for sub in subs]\n",
    "        pX = np.stack(pX)\n",
    "        X.append(pX)\n",
    "    return X\n",
    "\n",
    "def get_y(metadict, tasks, subs):\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        if task == 'age' or task == 'wrat':\n",
    "            var = [metadict[task][sub] for sub in subs]\n",
    "            var = np.array(var)\n",
    "            y.append(var)\n",
    "        if task == 'sex':\n",
    "            maleness = [metadict[task][sub] == 'M' for sub in subs]\n",
    "            maleness = np.array(maleness)\n",
    "            sex = np.stack([maleness, 1-maleness], axis=1)\n",
    "            y.append(sex)\n",
    "    return y\n",
    "\n",
    "subs = get_subs(allts, metadict, ['age'], ['emoid'])\n",
    "print(len(subs))\n",
    "\n",
    "X = get_X(allts, ['emoid'], subs)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbeb6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(922, 264, 264)\n"
     ]
    }
   ],
   "source": [
    "# TS to condensed FC\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_bandpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "tr = 1.83\n",
    "N = X[0].shape[0]\n",
    "\n",
    "def filter_design_ts(X):\n",
    "    Xs = []\n",
    "    for i in range(X.shape[0]):\n",
    "        nX = butter_bandpass_filter(X[i], [tr/20*N, 0.8*N], 2*N)\n",
    "        Xs.append(nX)\n",
    "    return np.stack(Xs)\n",
    "\n",
    "def ts_to_flat_fc(X):\n",
    "    p = np.corrcoef(X)\n",
    "    a,b = np.triu_indices(p[0].shape[0], 1)\n",
    "    p = p[a,b]\n",
    "    return p\n",
    "\n",
    "p = [np.corrcoef(ts) for ts in filter_design_ts(X[0])]\n",
    "p = np.stack(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a1567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14906658232212067 lr: [0.01]\n",
      "400 0.019359642639756203 lr: [0.01]\n",
      "800 0.01606043055653572 lr: [0.01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m xhat \u001b[38;5;241m=\u001b[39m lrw(book)\n\u001b[1;32m     69\u001b[0m xloss \u001b[38;5;241m=\u001b[39m mseLoss(xhat, xtr)\n\u001b[0;32m---> 70\u001b[0m \u001b[43m(\u001b[49m\u001b[43mxloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m sched\u001b[38;5;241m.\u001b[39mstep(xloss)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/anton/Documents/Tulane/Research/LatentSimilarity')\n",
    "\n",
    "from latsim import LatSim\n",
    "\n",
    "# outliers = np.array([167, 405, 58, 129, 602])\n",
    "outliers = np.array([141,429])\n",
    "admit = np.setdiff1d(np.arange(p.shape[0]), outliers)\n",
    "x = torch.from_numpy(p[admit]).float().cuda()\n",
    "xtr = x[:600]\n",
    "xt = x[600:]\n",
    "\n",
    "def mask(e):\n",
    "    return e - torch.diag(torch.diag(e.detach()))\n",
    "\n",
    "xtr = torch.stack([mask(xtr[i]) for i in range(xtr.shape[0])])\n",
    "\n",
    "class LowRankCodes(nn.Module):\n",
    "    def __init__(self, ranks):\n",
    "        super(LowRankCodes, self).__init__()\n",
    "        self.book = []\n",
    "        for rank in ranks:\n",
    "            self.book.append(nn.Parameter(1e-5*torch.randn(rank,264).float().cuda()))\n",
    "        self.book = nn.ParameterList(self.book)\n",
    "        \n",
    "    def forward(self):\n",
    "        book = []\n",
    "        for page in self.book:\n",
    "            book.append(mask(page.T@page))\n",
    "        return torch.stack(book)\n",
    "    \n",
    "class LowRankWeights(nn.Module):\n",
    "    def __init__(self, nsubs, nranks):\n",
    "        super(LowRankWeights, self).__init__()\n",
    "        self.w = nn.Parameter(torch.ones(nsubs, nranks).float().cuda())\n",
    "        \n",
    "    def forward(self, book):\n",
    "        w = F.relu(self.w)\n",
    "        return torch.einsum('nr,rab->nab',w,book)\n",
    "    \n",
    "nEpochs = 3000\n",
    "pPeriod = 400\n",
    "\n",
    "mseLoss = nn.MSELoss()\n",
    "    \n",
    "lrc = LowRankCodes(400*[1])\n",
    "lrw = LowRankWeights(xtr.shape[0], 400)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': itertools.chain(lrc.parameters(), lrw.parameters())}, \n",
    "    ], \n",
    "    lr=1e-2, \n",
    "    weight_decay=0\n",
    ")\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.95, eps=1e-7)\n",
    "    \n",
    "for epoch in range(nEpochs):\n",
    "    optim.zero_grad()\n",
    "    book = lrc()\n",
    "    xhat = lrw(book)\n",
    "    xloss = mseLoss(xhat, xtr)\n",
    "    (xloss).backward()\n",
    "    optim.step()\n",
    "    sched.step(xloss)\n",
    "    if epoch % pPeriod == 0 or epoch == nEpochs-1:\n",
    "        print(f'{epoch} {float(xloss)} lr: {sched._last_lr}')\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01682aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate test set\n",
    "\n",
    "xt = torch.stack([mask(xt[i]) for i in range(xt.shape[0])])\n",
    "\n",
    "lrw2 = LowRankWeights(xt.shape[0], 400)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    itertools.chain(lrw2.parameters()), \n",
    "    lr=1e-2, \n",
    "    weight_decay=0\n",
    ")\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.95, eps=1e-7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    book = lrc()\n",
    "    \n",
    "nEpochs = 1000\n",
    "\n",
    "for epoch in range(nEpochs):\n",
    "    optim.zero_grad()\n",
    "    xhat2 = lrw2(book)\n",
    "    loss = mseLoss(xhat2, xt)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    sched.step(loss)\n",
    "    if epoch % pPeriod == 0 or epoch == nEpochs-1:\n",
    "        print(f'{epoch} {float(loss)} lr: {sched._last_lr}')\n",
    "    \n",
    "print(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d75f75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41.051036796533076 lr: [0.0001]\n",
      "100 34.84720239342661 lr: [0.0001]\n",
      "200 31.170241968502587 lr: [9.5e-05]\n",
      "300 27.992952822359484 lr: [6.634204312890622e-05]\n",
      "400 28.078829289998453 lr: [5.133420832795048e-05]\n",
      "500 28.05702941179024 lr: [3.584859224085419e-05]\n",
      "600 27.847305293499673 lr: [2.2593554099256555e-05]\n",
      "700 26.654871557266507 lr: [1.4989025404881544e-05]\n",
      "800 27.366408023084386 lr: [9.446824413773763e-06]\n",
      "900 27.220272542806907 lr: [5.953855510552941e-06]\n",
      "1000 27.523520658758848 lr: [3.7524139211116024e-06]\n",
      "1100 26.647893528235727 lr: [2.3649566588229932e-06]\n",
      "1200 27.30782796675193 lr: [1.9262719795904457e-06]\n",
      "1300 26.644848226471826 lr: [1.9262719795904457e-06]\n",
      "1400 25.54381432353658 lr: [1.9262719795904457e-06]\n",
      "1500 26.71119684906266 lr: [1.9262719795904457e-06]\n",
      "1600 27.357292405388115 lr: [1.9262719795904457e-06]\n",
      "1700 26.51636273465464 lr: [1.9262719795904457e-06]\n",
      "1800 25.44363096992239 lr: [1.9262719795904457e-06]\n",
      "1900 26.773029432256426 lr: [1.9262719795904457e-06]\n",
      "1999 26.909813479907985 lr: [1.9262719795904457e-06]\n",
      "Complete\n",
      "tensor(30.5873, device='cuda:0', grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = get_y(metadict, ['age'], subs)[0]\n",
    "y_t = torch.from_numpy(y[admit]).float().cuda()\n",
    "ytr = y_t[:600]\n",
    "yt = y_t[600:]\n",
    "\n",
    "wtr = lrw.w.clone().detach().unsqueeze(1)\n",
    "wt = lrw2.w.clone().detach().unsqueeze(1)\n",
    "w = torch.cat([wtr, wt])\n",
    "\n",
    "nEpochs = 1000\n",
    "pPeriod = 100\n",
    "\n",
    "lrsim = LatSim(1, torch.zeros(1,1,400), dp=0.5, edp=0, temp=1)\n",
    "optim = torch.optim.Adam(\n",
    "    itertools.chain(lrsim.parameters()), \n",
    "    lr=1e-4, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.95, eps=1e-7)\n",
    "\n",
    "for epoch in range(nEpochs):\n",
    "    optim.zero_grad()\n",
    "    yhat = lrsim(wtr, [ytr])[0][0]\n",
    "    loss = mseLoss(yhat, ytr)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    sched.step(loss)\n",
    "    if epoch % pPeriod == 0 or epoch == nEpochs-1:\n",
    "        print(f'{epoch} {float(loss)**0.5} lr: {sched._last_lr}')\n",
    "    \n",
    "print('Complete')\n",
    "\n",
    "lrsim.eval()\n",
    "yhat = lrsim(w, [y_t], torch.arange(600,y_t.shape[0]))[0][0]\n",
    "print(mseLoss(yhat[600:], yt)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f3f8ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.7078, device='cuda:0')\n",
      "tensor(16.7623, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "w, _, _, _ = torch.linalg.lstsq(wtr.squeeze(), ytr)\n",
    "\n",
    "print(mseLoss(wtr.squeeze()@w, ytr)**0.5)\n",
    "print(mseLoss(wt.squeeze()@w, yt)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1aa87059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 50])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtr.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3ef88e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([305, 1, 50])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1554d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b19cae66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([305])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da4217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
