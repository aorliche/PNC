{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f28db1",
   "metadata": {},
   "source": [
    "# Baseline Static PCA and kSVD\n",
    "\n",
    "Can't do dynamic PCA/kSVD because each subject has at least 124 points of 34716 features,<br>\n",
    "and you can't enforce a notion of temporal proximity with these methods\n",
    "\n",
    "- Rest, nback, emoid individually (train codebook on 400)\n",
    "- and rest+nback+emoid together (train codebook on 200/200/200, same subjects from each)\n",
    "\n",
    "Response variables are are \n",
    "\n",
    "- age, \n",
    "- sex, \n",
    "- wrat\n",
    "\n",
    "Evaluation methods are\n",
    "\n",
    "- Ridge regression\n",
    "- Elastic MLP\n",
    "- LatSim\n",
    "\n",
    "Number of components are 10,50,100,200,300,400,500,600,800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dadc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get subjects that have all tasks and paras specified\n",
    "Functions for creating independent and response variables\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_subs(allts, metadict, tasks, paras):\n",
    "    # Get subs for all paras\n",
    "    for i,para in enumerate(paras):\n",
    "        tmpset = set([int(sub[4:]) for sub in allts[para].keys()])\n",
    "        if i == 0:\n",
    "            paraset = tmpset\n",
    "        else:\n",
    "            paraset = paraset.intersection(tmpset)\n",
    "    # Get subs for all tasks\n",
    "    for i,task in enumerate(tasks):\n",
    "        tmpset = set([sub for sub in metadict[task].keys()])\n",
    "        if i == 0:\n",
    "            taskset = tmpset\n",
    "        else:\n",
    "            taskset = paraset.intersection(tmpset)\n",
    "    # Remove QC failures\n",
    "    allsubs = taskset.intersection(paraset)\n",
    "    for badsub in metadict['failedqc']:\n",
    "        try:\n",
    "            allsubs.remove(int(badsub[4:]))\n",
    "        except:\n",
    "            pass\n",
    "    return allsubs\n",
    "\n",
    "def get_X(allts, paras, subs):\n",
    "    X = []\n",
    "    for para in paras:\n",
    "        pX = [allts[para][f'sub-{sub}'] for sub in subs]\n",
    "        pX = np.stack(pX)\n",
    "        X.append(pX)\n",
    "    return X\n",
    "\n",
    "def get_y(metadict, tasks, subs):\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        if task == 'age' or task == 'wrat':\n",
    "            var = [metadict[task][sub] for sub in subs]\n",
    "            var = np.array(var)\n",
    "            y.append(var)\n",
    "        if task == 'sex':\n",
    "            maleness = [metadict[task][sub] == 'M' for sub in subs]\n",
    "            maleness = np.array(maleness)\n",
    "            sex = np.stack([maleness, 1-maleness], axis=1)\n",
    "            y.append(sex)\n",
    "    return y\n",
    "\n",
    "subs = get_subs(allts, metadict, ['age'], ['rest', 'nback', 'emoid'])\n",
    "print(len(subs))\n",
    "\n",
    "X = get_X(allts, ['rest', 'nback', 'emoid'], subs)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS to condensed FC\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_bandpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "tr = 1.83\n",
    "\n",
    "def filter_design_ts(X):\n",
    "    Xs = []\n",
    "    for i in range(X.shape[0]):\n",
    "        nX = butter_bandpass_filter(X[i], [0.01, 0.2], 1/tr)\n",
    "        Xs.append(nX)\n",
    "    return np.stack(Xs)\n",
    "\n",
    "def ts_to_flat_fc(X):\n",
    "    p = np.corrcoef(X)\n",
    "    a,b = np.triu_indices(p[0].shape[0], 1)\n",
    "    p = p[a,b]\n",
    "    return p\n",
    "\n",
    "p = [np.stack([ts_to_flat_fc(ts) for ts in filter_design_ts(Xp)]) for Xp in X]\n",
    "print(p[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../LatentSimilarity')\n",
    "\n",
    "from latsim import LatSim\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b339b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import orthogonal_mp_gram\n",
    "import torch\n",
    "\n",
    "class ApproximateKSVD(object):\n",
    "    def __init__(self, n_components, max_iter=10, tol=1e-6,\n",
    "                 transform_n_nonzero_coefs=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components:\n",
    "            Number of dictionary elements\n",
    "\n",
    "        max_iter:\n",
    "            Maximum number of iterations\n",
    "\n",
    "        tol:\n",
    "            tolerance for error\n",
    "\n",
    "        transform_n_nonzero_coefs:\n",
    "            Number of nonzero coefficients to target\n",
    "        \"\"\"\n",
    "        self.components_ = None\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_components = n_components\n",
    "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n",
    "\n",
    "    def _update_dict(self, X, D, gamma):\n",
    "        for j in range(self.n_components):\n",
    "            I = gamma[:, j] > 0\n",
    "            if np.sum(I) == 0:\n",
    "                continue\n",
    "\n",
    "            D[j, :] = 0\n",
    "            g = gamma[I, j].T\n",
    "            r = X[I, :] - gamma[I, :].dot(D)\n",
    "            d = r.T.dot(g)\n",
    "            d /= np.linalg.norm(d)\n",
    "            g = r.dot(d)\n",
    "            D[j, :] = d\n",
    "            gamma[I, j] = g.T\n",
    "        return D, gamma\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        if min(X.shape) < self.n_components:\n",
    "            D = np.random.randn(self.n_components, X.shape[1])\n",
    "        else:\n",
    "            X = torch.from_numpy(X).float().cuda()\n",
    "            u, s, vt = torch.linalg.svd(X, full_matrices=False)\n",
    "            u, s, vt = [mat.detach().cpu().numpy() for mat in [u, s, vt]]\n",
    "#           #sp.sparse.linalg.svds(X, k=self.n_components)\n",
    "            D = np.dot(np.diag(s), vt)[:self.n_components,:] # Made a change here\n",
    "        D /= np.linalg.norm(D, axis=1)[:, np.newaxis]\n",
    "        return D\n",
    "\n",
    "    def _transform(self, D, X):\n",
    "        gram = D.dot(D.T)\n",
    "        Xy = D.dot(X.T)\n",
    "\n",
    "        n_nonzero_coefs = self.transform_n_nonzero_coefs\n",
    "        if n_nonzero_coefs is None:\n",
    "            n_nonzero_coefs = int(0.1 * X.shape[1])\n",
    "\n",
    "        return orthogonal_mp_gram(\n",
    "            gram, Xy, n_nonzero_coefs=n_nonzero_coefs).T\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: shape = [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        D = self._initialize(X)\n",
    "        for i in range(self.max_iter):\n",
    "            print(f'Start iter {i}')\n",
    "            gamma = self._transform(D, X)\n",
    "            e = np.linalg.norm(X - gamma.dot(D))\n",
    "            if e < self.tol:\n",
    "                break\n",
    "            D, gamma = self._update_dict(X, D, gamma)\n",
    "\n",
    "        self.components_ = D\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._transform(self.components_, X)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c7f39",
   "metadata": {},
   "source": [
    "# reminder: you changed dp 0.5->0.1 and dim 2->10\n",
    "Also, zeroed out infinities (and maybe nans) for kSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bda2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask_ml.decomposition import PCA\n",
    "import dask.array as da\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ncodes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(ncodes, 40).float().cuda()\n",
    "        self.l2 = nn.Linear(40,1).float().cuda()\n",
    "        \n",
    "    def train(self, xtr, ytr, nepochs=1000, lr=1e-1, l1=1e-1, l2=1e-4, pperiod=100, verbose=False):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.75, eps=1e-7)\n",
    "        mseLoss = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            optim.zero_grad()\n",
    "            yhat = self(xtr)\n",
    "            loss = mseLoss(yhat, ytr)**0.5\n",
    "            l1loss = l1*torch.sum(torch.abs(self.l1.weight))\n",
    "            (loss+l1loss).backward()\n",
    "            optim.step()\n",
    "            sched.step(loss)\n",
    "            if verbose:\n",
    "                if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                    print(f'{epoch} {[float(l) for l in [loss, l1loss]]} {sched._last_lr}')\n",
    "                    \n",
    "    def predict(self, xt, yt):\n",
    "        with torch.no_grad():\n",
    "            return mseLoss(self(xt), yt)**0.5\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x).squeeze()\n",
    "        return x\n",
    "\n",
    "ncodesall = [10,50,100,200,300,400,500,600,800]\n",
    "ntrainall = [400,400,400,400,400,400,500,600,800]\n",
    "\n",
    "modstr = 'rest2emoid'\n",
    "modidx = 0\n",
    "modidx2 = 2\n",
    "method = 'PCA'\n",
    "\n",
    "for ncodes,ntrain in zip(ncodesall, ntrainall):\n",
    "    for split in range(3):\n",
    "        idcs = np.arange(p[modidx].shape[0])\n",
    "        np.random.shuffle(idcs)\n",
    "        \n",
    "        # kSVD\n",
    "#         nnzero = min(100, ncodes)\n",
    "#         t0 = time.time()\n",
    "#         ksvd = ApproximateKSVD(n_components=ncodes, transform_n_nonzero_coefs=nnzero, max_iter=1)\n",
    "#         ksvd.fit(p[modidx][idcs[0:ntrain]])\n",
    "#         z = ksvd.transform(p[modidx2])\n",
    "#         t1 = time.time()\n",
    "#         print(f'{t1-t0} seconds')\n",
    "#         print(z.shape)\n",
    "        \n",
    "        # PCA\n",
    "        t0 = time.time()\n",
    "        dx = da.from_array(p[modidx], chunks=(1000,1000))\n",
    "        pca = PCA(n_components=ncodes)\n",
    "        pca.fit(dx[idcs[:ntrain]])\n",
    "        z = pca.transform(dx)\n",
    "        z = z.persist()\n",
    "        t1 = time.time()\n",
    "        print(z.shape)\n",
    "        print(f'pca time {t1-t0}')\n",
    "\n",
    "        nreps = 10\n",
    "        trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "        res = np.zeros((nreps,len(trainsizes)))\n",
    "        l2 = 1e0 #1e3 for kSVD, 1e2 for PCA\n",
    "\n",
    "        for rep in range(nreps):\n",
    "            losses = []\n",
    "\n",
    "            idcs = np.arange(z.shape[0])\n",
    "            np.random.shuffle(idcs)\n",
    "\n",
    "            for ntrain in trainsizes:\n",
    "                xps = torch.from_numpy(np.asarray(z)).float().cuda()\n",
    "                xps = torch.cat([xps, torch.ones(xps.shape[0], 1).float().cuda()], dim=1)\n",
    "                xtr = xps[idcs[:ntrain]]\n",
    "                xt = xps[idcs[ntrain:]]\n",
    "\n",
    "                y = get_y(metadict, ['age'], subs)[0]\n",
    "                y_t = torch.from_numpy(y).float().cuda()\n",
    "                ytr = y_t[idcs[:ntrain]]\n",
    "                yt = y_t[idcs[ntrain:]]\n",
    "\n",
    "                # REDUCE THIS TO GET GOOD RESULTS WITH SPARSITY 0.01->0.001 or 0.0001\n",
    "                w, _, _, _ = torch.linalg.lstsq(xtr.T@xtr + l2*torch.eye(ncodes+1).float().cuda(), xtr.T@ytr)\n",
    "\n",
    "                print(torch.mean((yt-xt@w)**2)**0.5)\n",
    "                losses.append(float(torch.mean((yt-xt@w)**2)**0.5))\n",
    "\n",
    "            print(f'Finished {rep}')\n",
    "            res[rep,:] = losses\n",
    "\n",
    "        print(np.mean(res, axis=0))\n",
    "        print(np.std(res, axis=0))\n",
    "\n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-lstsq-mean.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.mean(res, axis=0)])}\\n')\n",
    "\n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-lstsq-std.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.std(res, axis=0)])}\\n')\n",
    "            \n",
    "        print('Finished lstsq')\n",
    "\n",
    "        nreps = 10\n",
    "        trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "        res = np.zeros((nreps,len(trainsizes)))\n",
    "\n",
    "        for rep in range(nreps):\n",
    "\n",
    "            idcs = np.arange(z.shape[0])\n",
    "            np.random.shuffle(idcs)\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for ntrain in trainsizes:\n",
    "                xps = torch.from_numpy(np.asarray(z)).float().cuda()\n",
    "                xtr = xps[idcs[:ntrain]]\n",
    "                xt = xps[idcs[ntrain:]]\n",
    "\n",
    "                y = get_y(metadict, ['age'], subs)[0]\n",
    "                y_t = torch.from_numpy(y).float().cuda()\n",
    "                ytr = y_t[idcs[:ntrain]]\n",
    "                yt = y_t[idcs[ntrain:]]\n",
    "\n",
    "                mlp = MLP(ncodes)\n",
    "                # 1e-3 good for age 1e-2 good for wrat\n",
    "                mlp.train(xtr, ytr, lr=1e-2, nepochs=1000, l1=1e0, l2=1e-3)\n",
    "                loss = mlp.predict(xt, yt)\n",
    "\n",
    "                losses.append(float(loss))\n",
    "                print(float(loss))\n",
    "\n",
    "            res[rep,:] = losses\n",
    "            print(f'Finished {rep}')\n",
    "\n",
    "        print(np.mean(res, axis=0))\n",
    "        print(np.std(res, axis=0))\n",
    "        \n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-mlp-mean.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.mean(res, axis=0)])}\\n')\n",
    "\n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-mlp-std.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.std(res, axis=0)])}\\n')\n",
    "            \n",
    "        print('Finished MLP')\n",
    "        \n",
    "        nreps = 10\n",
    "        trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "        res = np.zeros((nreps,len(trainsizes)))\n",
    "\n",
    "        nepochs = 500\n",
    "        pperiod = 100\n",
    "        verbose = False\n",
    "\n",
    "        for rep in range(nreps):\n",
    "\n",
    "            idcs = np.arange(z.shape[0])\n",
    "            np.random.shuffle(idcs)\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for ntrain in trainsizes:\n",
    "                xps = torch.from_numpy(np.asarray(z))[idcs].unsqueeze(1).float().cuda()\n",
    "\n",
    "                mu = torch.mean(xps[:ntrain], dim=0, keepdims=True)\n",
    "                std = torch.std(xps[:ntrain], dim=0, keepdims=True)\n",
    "                xps = (xps-mu)/std\n",
    "                xps[torch.isnan(xps)] = 0\n",
    "                xps[torch.isinf(xps)] = 0\n",
    "\n",
    "                xtr = xps[:ntrain]\n",
    "                xt = xps[ntrain:]\n",
    "\n",
    "                y = get_y(metadict, ['age'], subs)[0]\n",
    "                y_t = torch.from_numpy(y[idcs]).float().cuda()\n",
    "                ytr = y_t[:ntrain]\n",
    "                yt = y_t[ntrain:]\n",
    "\n",
    "                # dp=0.5 for wrat\n",
    "                sim = LatSim(1, xps, dp=0.2, edp=0.1, wInit=1e-4, dim=10, temp=1)\n",
    "                optim = torch.optim.Adam(sim.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "                sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.75, eps=1e-7)\n",
    "\n",
    "                for epoch in range(nepochs):\n",
    "                    optim.zero_grad()\n",
    "                    yhat = sim(xtr, [ytr])[0][0]\n",
    "                    loss = mseLoss(yhat, ytr)**0.5\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    sched.step(loss)\n",
    "                    if verbose:\n",
    "                        if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                            print(f'{epoch} {float(loss)} {sched._last_lr}')\n",
    "\n",
    "                sim.eval()\n",
    "                yhat = sim(xps, [y_t], np.arange(ntrain,idcs.shape[0]))[0][0][ntrain:]\n",
    "                loss = mseLoss(yhat, yt)**0.5\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                print(float(loss))\n",
    "\n",
    "            res[rep,:] = losses\n",
    "            print(f'Finished {rep}')\n",
    "\n",
    "        print(np.mean(res, axis=0))\n",
    "        print(np.std(res, axis=0))\n",
    "        \n",
    "        print('Finished latsim')\n",
    "        \n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-latsim-mean.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.mean(res, axis=0)])}\\n')\n",
    "\n",
    "        with open(f'/home/anton/Documents/Tulane/Research/Work/LatSimEC2/DictEst/{method}/{modstr}-latsim-std.csv', 'a') as f:\n",
    "            f.write(f'{ncodes},{split},{\",\".join([str(val) for val in np.std(res, axis=0)])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e934e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nreps = 10\n",
    "trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "res = np.zeros((nreps,len(trainsizes)))\n",
    "l2 = 1e0\n",
    "\n",
    "for rep in range(nreps):\n",
    "    losses = []\n",
    "\n",
    "    idcs = np.arange(z.shape[0])\n",
    "    np.random.shuffle(idcs)\n",
    "\n",
    "    for ntrain in trainsizes:\n",
    "        xps = torch.from_numpy(np.asarray(z)).float().cuda()\n",
    "        xps = torch.cat([xps, torch.ones(xps.shape[0], 1).float().cuda()], dim=1)\n",
    "        xtr = xps[idcs[:ntrain]]\n",
    "        xt = xps[idcs[ntrain:]]\n",
    "\n",
    "        y = get_y(metadict, ['age'], subs)[0]\n",
    "        y_t = torch.from_numpy(y).float().cuda()\n",
    "        ytr = y_t[idcs[:ntrain]]\n",
    "        yt = y_t[idcs[ntrain:]]\n",
    "\n",
    "        # REDUCE THIS TO GET GOOD RESULTS WITH SPARSITY 0.01->0.001 or 0.0001\n",
    "        w, _, _, _ = torch.linalg.lstsq(xtr.T@xtr + l2*torch.eye(ncodes+1).float().cuda(), xtr.T@ytr)\n",
    "\n",
    "        print(torch.mean((yt-xt@w)**2)**0.5)\n",
    "        losses.append(float(torch.mean((yt-xt@w)**2)**0.5))\n",
    "            \n",
    "    print(f'Finished {rep}')\n",
    "    res[rep,:] = losses\n",
    "    \n",
    "print(np.mean(res, axis=0))\n",
    "print(np.std(res, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315abb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ncodes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(ncodes, 40).float().cuda()\n",
    "        self.l2 = nn.Linear(40,1).float().cuda()\n",
    "        \n",
    "    def train(self, xtr, ytr, nepochs=1000, lr=1e-1, l1=1e-1, l2=1e-4, pperiod=100, verbose=False):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.75, eps=1e-7)\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            optim.zero_grad()\n",
    "            yhat = self(xtr)\n",
    "            loss = mseLoss(yhat, ytr)**0.5\n",
    "            l1loss = l1*torch.sum(torch.abs(self.l1.weight))\n",
    "            (loss+l1loss).backward()\n",
    "            optim.step()\n",
    "            sched.step(loss)\n",
    "            if verbose:\n",
    "                if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                    print(f'{epoch} {[float(l) for l in [loss, l1loss]]} {sched._last_lr}')\n",
    "                    \n",
    "    def predict(self, xt, yt):\n",
    "        with torch.no_grad():\n",
    "            return mseLoss(self(xt), yt)**0.5\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x).squeeze()\n",
    "        return x\n",
    "    \n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "nreps = 10\n",
    "trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "res = np.zeros((nreps,len(trainsizes)))\n",
    "\n",
    "for rep in range(nreps):\n",
    "\n",
    "    idcs = np.arange(z.shape[0])\n",
    "    np.random.shuffle(idcs)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for ntrain in trainsizes:\n",
    "        xps = torch.from_numpy(np.asarray(z)).float().cuda()\n",
    "        xtr = xps[idcs[:ntrain]]\n",
    "        xt = xps[idcs[ntrain:]]\n",
    "\n",
    "        y = get_y(metadict, ['age'], subs)[0]\n",
    "        y_t = torch.from_numpy(y).float().cuda()\n",
    "        ytr = y_t[idcs[:ntrain]]\n",
    "        yt = y_t[idcs[ntrain:]]\n",
    "\n",
    "        mlp = MLP(ncodes)\n",
    "        # 1e-3 good for age 1e-2 good for wrat\n",
    "        mlp.train(xtr, ytr, lr=1e-2, nepochs=1000, l1=1e0, l2=1e-3)\n",
    "        loss = mlp.predict(xt, yt)\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        print(float(loss))\n",
    "    \n",
    "    res[rep,:] = losses\n",
    "    print(f'Finished {rep}')\n",
    "    \n",
    "print(np.mean(res, axis=0))\n",
    "print(np.std(res, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc789f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nreps = 10\n",
    "trainsizes = [30,50,100,200,300,400,500,600,700,800]\n",
    "res = np.zeros((nreps,len(trainsizes)))\n",
    "\n",
    "nepochs = 500\n",
    "pperiod = 100\n",
    "verbose = False\n",
    "\n",
    "for rep in range(nreps):\n",
    "\n",
    "    idcs = np.arange(z.shape[0])\n",
    "    np.random.shuffle(idcs)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for ntrain in trainsizes:\n",
    "        xps = torch.from_numpy(np.asarray(z))[idcs].unsqueeze(1).float().cuda()\n",
    "        \n",
    "        mu = torch.mean(xps[:ntrain], dim=0, keepdims=True)\n",
    "        std = torch.std(xps[:ntrain], dim=0, keepdims=True)\n",
    "        xps = (xps-mu)/std\n",
    "    \n",
    "        xtr = xps[:ntrain]\n",
    "        xt = xps[ntrain:]\n",
    "\n",
    "        y = get_y(metadict, ['age'], subs)[0]\n",
    "        y_t = torch.from_numpy(y[idcs]).float().cuda()\n",
    "        ytr = y_t[:ntrain]\n",
    "        yt = y_t[ntrain:]\n",
    "\n",
    "        # dp=0.5 for wrat\n",
    "        sim = LatSim(1, xps, dp=0.5, edp=0.1, wInit=1e-4, dim=10, temp=1)\n",
    "        optim = torch.optim.Adam(sim.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.75, eps=1e-7)\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            optim.zero_grad()\n",
    "            yhat = sim(xtr, [ytr])[0][0]\n",
    "            loss = mseLoss(yhat, ytr)**0.5\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            sched.step(loss)\n",
    "            if verbose:\n",
    "                if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                    print(f'{epoch} {float(loss)} {sched._last_lr}')\n",
    "                    \n",
    "        sim.eval()\n",
    "        yhat = sim(xps, [y_t], np.arange(ntrain,idcs.shape[0]))[0][0][ntrain:]\n",
    "        loss = mseLoss(yhat, yt)**0.5\n",
    "        losses.append(float(loss))\n",
    "        \n",
    "        print(float(loss))\n",
    "    \n",
    "    res[rep,:] = losses\n",
    "    print(f'Finished {rep}')\n",
    "    \n",
    "print(np.mean(res, axis=0))\n",
    "print(np.std(res, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66339736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
