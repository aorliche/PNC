{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058d7eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n",
      "830\n",
      "830\n"
     ]
    }
   ],
   "source": [
    "# Get FC all tasks in ImageNomer directory\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "snpsdir = '../../ImageNomer/data/anton/cohorts/test/fc/'\n",
    "\n",
    "fcs = dict(rest=dict(), nback=dict(), emoid=dict())\n",
    "\n",
    "for f in Path(snpsdir).iterdir():\n",
    "    mobj = re.match('([0-9]+)_task-([a-z]+)_fc.npy', f.name)\n",
    "    if not mobj:\n",
    "        continue\n",
    "    fc = np.load(f'{snpsdir}/{f.name}')\n",
    "    sub = mobj.group(1)\n",
    "    mod = mobj.group(2) \n",
    "    fcs[mod][sub] = fc\n",
    "\n",
    "for mod in fcs:\n",
    "    print(len(list(fcs[mod].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0abc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 830)\n",
      "(830, 34716)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from natsort import natsorted\n",
    "\n",
    "subs = natsorted(fcs['rest'].keys())\n",
    "\n",
    "task = 'emoid'\n",
    "\n",
    "x = []\n",
    "for sub in subs:\n",
    "    fc = fcs[task][sub]\n",
    "    x.append(fc)\n",
    "x = np.stack(x)\n",
    "pca = PCA()\n",
    "xt = pca.fit_transform(x)\n",
    "print(xt.shape)\n",
    "print(pca.components_.shape)\n",
    "\n",
    "dcomp = f'../../ImageNomer/data/anton/cohorts/test/decomp/{task}pca-comps'\n",
    "dws = f'../../ImageNomer/data/anton/cohorts/test/decomp/{task}pca-weights'\n",
    "\n",
    "for i,sub in enumerate(subs):\n",
    "    c = pca.components_[i]\n",
    "    w = xt[i]\n",
    "    np.save(f'{dcomp}/{task}pca_comp-{i}.npy', c)\n",
    "    np.save(f'{dws}/{sub}_comp-{task}pca_weights.npy', w)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66ee803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 34716)\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c855c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import itertools\n",
    "\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "# Use make_dict() followed by fit_weights()\n",
    "\n",
    "'''\n",
    "LowRankCodes - dictionary components (rank-N matrices)\n",
    "For now PSD (symmetric) components only\n",
    "'''\n",
    "class LowRankCodes(nn.Module):\n",
    "    '''\n",
    "    ranks: array of rank for each codebook matrix\n",
    "    '''\n",
    "    def __init__(self, ranks):\n",
    "        super(LowRankCodes, self).__init__()\n",
    "        self.As = []\n",
    "        for rank in ranks:\n",
    "            A = nn.Parameter(1e-2*torch.randn(rank,264).float().cuda())\n",
    "            self.As.append(A)\n",
    "        self.As = nn.ParameterList(self.As)\n",
    "\n",
    "    '''\n",
    "    Generate codebook\n",
    "    '''\n",
    "    def forward(self):\n",
    "        book = []\n",
    "        for A in self.As:\n",
    "            AA = A.T@A\n",
    "            book.append(AA)\n",
    "        return torch.stack(book)\n",
    "    \n",
    "'''\n",
    "LowRankWeights - weights for the LowRankCodes codebook entries\n",
    "'''\n",
    "class LowRankWeights(nn.Module):\n",
    "    '''\n",
    "    ncodes: number of pages in the codebook\n",
    "    Xs: list of inputs to LowRankWeights of size [(nsubs, nrois, nt)...]\n",
    "    subids: id of each subject in modlist (optional)\n",
    "    '''\n",
    "    def __init__(self, ncodes, Xs, subids=None):\n",
    "        super(LowRankWeights, self).__init__()\n",
    "        self.ncodes = ncodes\n",
    "        self.params = []\n",
    "        for i in range(len(Xs)):\n",
    "            nsubs = Xs[i].shape[0]\n",
    "            nt = Xs[i].shape[-1]\n",
    "            params = nn.Parameter(1e-2*torch.rand(nsubs, self.ncodes, nt).float().cuda())\n",
    "            self.params.append(params)\n",
    "        self.params = nn.ParameterList(self.params)\n",
    "        self.subids = subids\n",
    "\n",
    "    '''\n",
    "    Get estimated instantaneous FC from book\n",
    "    '''\n",
    "    def forward(self, sub, book, mod):\n",
    "        w = self.params[mod][sub]\n",
    "        return torch.einsum('pt,pab->abt', w, book) # PUT BACK leaky relu\n",
    "\n",
    "def get_recon_loss(x, xhat):\n",
    "    return mseLoss(xhat, x)\n",
    "\n",
    "def get_smooth_loss_fc(xhat):\n",
    "    before = xhat[:,:,:-1]\n",
    "    after = xhat[:,:,1:]\n",
    "    return torch.mean((before-after)**2)\n",
    "\n",
    "def get_mag_loss(lrc):\n",
    "    loss = [torch.mean((A-0.01)**2) for A in lrc.As]\n",
    "    return sum(loss)/len(loss)\n",
    "\n",
    "def get_sub_fc(subts):\n",
    "    return torch.einsum('at,bt->abt',subts,subts)\n",
    "\n",
    "def default_or_custom(kwargs, field, default):\n",
    "    if field not in kwargs:\n",
    "        kwargs[field] = default\n",
    "\n",
    "def make_dict(Xs, ranks=400*[1], **kwargs):\n",
    "    default_or_custom(kwargs, 'nbatch', 20)\n",
    "    default_or_custom(kwargs, 'smooth_mult', 0.1)\n",
    "    default_or_custom(kwargs, 'nepochs', 50)\n",
    "    default_or_custom(kwargs, 'pperiod', 5)\n",
    "    default_or_custom(kwargs, 'subids', None)\n",
    "    default_or_custom(kwargs, 'lr', 1e-2)\n",
    "    default_or_custom(kwargs, 'l2', 0)\n",
    "    default_or_custom(kwargs, 'patience', 20)\n",
    "    default_or_custom(kwargs, 'factor', 0.75)\n",
    "    default_or_custom(kwargs, 'eps', 1e-7)\n",
    "    default_or_custom(kwargs, 'verbose', False)\n",
    "\n",
    "    pperiod = kwargs['pperiod']\n",
    "    nepochs = kwargs['nepochs']\n",
    "    nbatch = kwargs['nbatch']\n",
    "    smooth_mult = kwargs['smooth_mult']\n",
    "    ncodes = len(ranks)\n",
    "    modlist = [dict(nsubs=X.shape[0], nt=X.shape[-1]) for X in Xs]\n",
    "\n",
    "    lrc = LowRankCodes(ranks)\n",
    "    lrw = LowRankWeights(ncodes, Xs, kwargs['subids'])\n",
    "\n",
    "    optim = torch.optim.Adam(\n",
    "        itertools.chain(lrc.parameters(), lrw.parameters()), \n",
    "        lr=kwargs['lr'], \n",
    "        weight_decay=kwargs['l2'])\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optim, \n",
    "        patience=kwargs['patience'], \n",
    "        factor=kwargs['factor'], \n",
    "        eps=kwargs['eps'])\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        for modidx in range(len(Xs)):\n",
    "            ntrain = Xs[modidx].shape[0]\n",
    "            for bstart in range(0,ntrain,nbatch):\n",
    "                bend = bstart+nbatch\n",
    "                if bend > ntrain:\n",
    "                    bend = ntrain\n",
    "                optim.zero_grad()\n",
    "                book = lrc()\n",
    "                recon_loss = 0\n",
    "                smooth_loss_fc = 0\n",
    "                for subidx in range(bstart, bend):\n",
    "                    xsub = get_sub_fc(Xs[modidx][subidx])\n",
    "                    xhat = lrw(subidx, book, modidx)   \n",
    "                    recon_loss += get_recon_loss(xsub, xhat)\n",
    "                    smooth_loss_fc += smooth_mult*get_smooth_loss_fc(xhat)\n",
    "                recon_loss /= (bend-bstart)\n",
    "                smooth_loss_fc /= (bend-bstart)\n",
    "                loss = recon_loss+smooth_loss_fc\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                sched.step(loss)\n",
    "\n",
    "        if not kwargs['verbose']:\n",
    "            continue\n",
    "        if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "            print(f'{epoch} {bstart} recon: {[float(ls)**0.5 for ls in [recon_loss, smooth_loss_fc]]} '\n",
    "                f'lr: {sched._last_lr}')\n",
    "\n",
    "    optim.zero_grad()\n",
    "    if not kwargs['verbose']:\n",
    "        print('Complete')\n",
    "\n",
    "    return lrc, lrw\n",
    "\n",
    "def fit_weights(low_rank_codes, Xs, **kwargs):\n",
    "    default_or_custom(kwargs, 'nepochs', 500)\n",
    "    default_or_custom(kwargs, 'pperiod', 50)\n",
    "    default_or_custom(kwargs, 'lr', 1e-1)\n",
    "    default_or_custom(kwargs, 'l1', 0)\n",
    "    default_or_custom(kwargs, 'l2', 1e-5)\n",
    "    default_or_custom(kwargs, 'patience', 10)\n",
    "    default_or_custom(kwargs, 'factor', 0.75)\n",
    "    default_or_custom(kwargs, 'eps', 1e-7)\n",
    "    default_or_custom(kwargs, 'verbose', False)\n",
    "    \n",
    "    nepochs = kwargs['nepochs']\n",
    "    pperiod = kwargs['pperiod']\n",
    "\n",
    "    book = low_rank_codes()\n",
    "    A = book.reshape(book.shape[0], -1).permute(1,0).detach()\n",
    "    AA = A.T@A\n",
    "    ws = []\n",
    "\n",
    "    for X in Xs:\n",
    "        AB = []\n",
    "        for sub in range(X.shape[0]):\n",
    "            B = get_sub_fc(X[sub]).reshape(-1, X.shape[-1])\n",
    "            AB.append(A.T@B)\n",
    "        AB = torch.stack(AB)\n",
    "\n",
    "        w = nn.Parameter(torch.rand(AB.shape[0],AA.shape[1],AB.shape[-1]).float().cuda())\n",
    "        ws.append(w)\n",
    "\n",
    "        optim = torch.optim.Adam(\n",
    "            [w], \n",
    "            lr=kwargs['lr'], \n",
    "            weight_decay=kwargs['l2'])\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optim, \n",
    "            patience=kwargs['patience'], \n",
    "            factor=kwargs['factor'], \n",
    "            eps=kwargs['eps'])\n",
    "\n",
    "        for epoch in range(nepochs):\n",
    "            optim.zero_grad()\n",
    "            ABhat = torch.matmul(AA.detach(),w) # PUT BACK LEAKY RELU\n",
    "            pred_loss = mseLoss(ABhat, AB.detach())**0.5\n",
    "            l1_loss = kwargs['l1']*torch.mean(torch.abs(w))\n",
    "            loss = pred_loss+l1_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            sched.step(loss)\n",
    "            if not kwargs['verbose']:\n",
    "                continue\n",
    "            if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                print(f'{epoch} {[float(ls) for ls in [pred_loss, l1_loss]]} {sched._last_lr}')\n",
    "\n",
    "        optim.zero_grad()\n",
    "        if not kwargs['verbose']:\n",
    "            print('Complete')\n",
    "\n",
    "    return ws\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ae453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'wrat', 'missingage', 'missingsex', 'missingwrat', 'failedqc']\n",
      "['emoid', 'nback', 'rest']\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824b8daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n",
      "(830, 264, 124)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get subjects that have all tasks and paras specified\n",
    "Functions for creating independent and response variables\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "def get_subs(allts, metadict, tasks, paras):\n",
    "    # Get subs for all paras\n",
    "    for i,para in enumerate(paras):\n",
    "        tmpset = set([int(sub[4:]) for sub in allts[para].keys()])\n",
    "        if i == 0:\n",
    "            paraset = tmpset\n",
    "        else:\n",
    "            paraset = paraset.intersection(tmpset)\n",
    "    # Get subs for all tasks\n",
    "    for i,task in enumerate(tasks):\n",
    "        tmpset = set([sub for sub in metadict[task].keys()])\n",
    "        if i == 0:\n",
    "            taskset = tmpset\n",
    "        else:\n",
    "            taskset = paraset.intersection(tmpset)\n",
    "    # Remove QC failures\n",
    "    allsubs = taskset.intersection(paraset)\n",
    "    for badsub in metadict['failedqc']:\n",
    "        try:\n",
    "            allsubs.remove(int(badsub[4:]))\n",
    "        except:\n",
    "            pass\n",
    "    return natsorted(list(allsubs))\n",
    "\n",
    "def get_X(allts, paras, subs):\n",
    "    X = []\n",
    "    for para in paras:\n",
    "        pX = [allts[para][f'sub-{sub}'] for sub in subs]\n",
    "        pX = np.stack(pX)\n",
    "        X.append(pX)\n",
    "    return X\n",
    "\n",
    "def get_y(metadict, tasks, subs):\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        if task == 'age' or task == 'wrat':\n",
    "            var = [metadict[task][sub] for sub in subs]\n",
    "            var = np.array(var)\n",
    "            y.append(var)\n",
    "        if task == 'sex':\n",
    "            maleness = [metadict[task][sub] == 'M' for sub in subs]\n",
    "            maleness = np.array(maleness)\n",
    "            sex = np.stack([maleness, 1-maleness], axis=1)\n",
    "            y.append(sex)\n",
    "    return y\n",
    "\n",
    "subs = get_subs(allts, metadict, ['wrat'], ['rest', 'nback', 'emoid'])\n",
    "print(len(subs))\n",
    "\n",
    "X = get_X(allts, ['rest', 'nback', 'emoid'], subs)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4187a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 264, 124)\n",
      "(830, 264, 231)\n"
     ]
    }
   ],
   "source": [
    "# TS to condensed FC\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_bandpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "tr = 3\n",
    "\n",
    "def filter_design_ts(X):\n",
    "    Xs = []\n",
    "    for i in range(X.shape[0]):\n",
    "        nX = butter_bandpass_filter(X[i], [0.01, 0.15], 1/tr)\n",
    "        Xs.append(nX)\n",
    "    return np.stack(Xs)\n",
    "\n",
    "def ts_to_flat_fc(X):\n",
    "    p = np.corrcoef(X)\n",
    "    a,b = np.triu_indices(p[0].shape[0], 1)\n",
    "    p = p[a,b]\n",
    "    return p\n",
    "\n",
    "# p = [np.stack([ts_to_flat_fc(ts) for ts in filter_design_ts(Xp)]) for Xp in X]\n",
    "Xf = [filter_design_ts(Xp) for Xp in X]\n",
    "Xs = [tsmod/np.linalg.norm(tsmod, axis=(-1), keepdims=True) for tsmod in Xf]\n",
    "print(Xs[0].shape)\n",
    "print(Xs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d7140810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "0 380 recon: [0.00671566538138334, 2.8938422445752883e-06] lr: [0.01]\n",
      "5 380 recon: [0.00589714166271204, 0.0008019399140955256] lr: [0.0075]\n",
      "10 380 recon: [0.005512465897948504, 0.0011852861145529663] lr: [0.0075]\n",
      "15 380 recon: [0.005258004446623459, 0.0011447641199750335] lr: [0.00421875]\n",
      "20 380 recon: [0.005105440741235429, 0.0012158229252871432] lr: [0.00421875]\n",
      "25 380 recon: [0.004986756161166922, 0.0012774127056460307] lr: [0.00421875]\n",
      "30 380 recon: [0.005085004332884545, 0.0016409427823867795] lr: [0.0031640625]\n",
      "34 380 recon: [0.004879976544251507, 0.001316699278656137] lr: [0.0017797851562500002]\n"
     ]
    }
   ],
   "source": [
    "Xt = []\n",
    "for Xp in Xs[2:3]:\n",
    "    idcs = torch.randperm(830)\n",
    "    Xp = torch.from_numpy(Xp).float().cuda()\n",
    "    Xt.append(Xp[idcs][:400])\n",
    "    \n",
    "print('Starting')\n",
    "\n",
    "lrc, lrw = make_dict(Xt, ranks=400*[1], nepochs=35, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284ba76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [198.72190856933594, 0.0] [0.1]\n",
      "50 [0.4534512758255005, 0.0] [0.1]\n",
      "100 [0.38452914357185364, 0.0] [0.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     Xp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Xp)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m     Xt\u001b[38;5;241m.\u001b[39mappend(Xp)\n\u001b[0;32m----> 6\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mfit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 199\u001b[0m, in \u001b[0;36mfit_weights\u001b[0;34m(low_rank_codes, Xs, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    198\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 199\u001b[0m \u001b[43msched\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1002\u001b[0m, in \u001b[0;36mReduceLROnPlateau.step\u001b[0;34m(self, metrics, epoch)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, metrics, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;66;03m# convert `metrics` to float, in case it's a zero-dim Tensor\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Xt = []\n",
    "for Xp in Xs[2:3]:\n",
    "    Xp = torch.from_numpy(Xp).float().cuda()\n",
    "    Xt.append(Xp)\n",
    "\n",
    "w = fit_weights(lrc, Xt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "313759f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004749314859509468\n",
      "torch.Size([830, 400])\n"
     ]
    }
   ],
   "source": [
    "def vstk2mat(vstk):\n",
    "    a,b = torch.triu_indices(264,264,offset=1)\n",
    "    return vstk[:,a,b]\n",
    "\n",
    "def fit_w(lrc, x):\n",
    "    book = lrc()\n",
    "    A = vstk2mat(book)\n",
    "    A = A.T.detach()\n",
    "    AA = A.T@A\n",
    "    ws = []\n",
    "    rs = []\n",
    "    \n",
    "    # One subject at a time\n",
    "    for sub in range(x.shape[0]):\n",
    "        B = vstk2mat(get_sub_fc(x[sub]).permute(2,0,1))\n",
    "        B = B.T\n",
    "        BA = A.T@B\n",
    "        I = 0.1*torch.eye(A.shape[1]).float().cuda()\n",
    "        w,_,_,_ = torch.linalg.lstsq((AA+I).detach(),BA.detach())\n",
    "        rmse = torch.mean((B-A@w)**2)**0.5\n",
    "        rmse = float(rmse)\n",
    "        rs.append(rmse)\n",
    "        ws.append(w.detach())\n",
    "        \n",
    "    print(np.mean(rmse))\n",
    "    return ws\n",
    "\n",
    "Xt = []\n",
    "for Xp in Xs[2:3]:\n",
    "    Xp = torch.from_numpy(Xp).float().cuda()\n",
    "    Xt.append(Xp)\n",
    "\n",
    "ws = fit_w(lrc, Xt[0])\n",
    "w = torch.mean(torch.stack(ws), axis=-1)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "022e9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1\n",
      "Done 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "task = 'emoid'\n",
    "basedir = '../../ImageNomer/data/anton/cohorts/test/decomp'\n",
    "wdir = f'{basedir}/{task}dd-weights'\n",
    "cdir = f'{basedir}/{task}dd-comps'\n",
    "    \n",
    "book = vstk2mat(lrc())\n",
    "b = torch.mean(torch.abs(w), dim=0).detach().cpu().numpy()\n",
    "c = torch.mean(torch.abs(book), dim=1).detach().cpu().numpy()\n",
    "idcs = np.argsort(b*c)[::-1]\n",
    "# print((b*c)[idcs])\n",
    "\n",
    "cc = book.detach().cpu().numpy()[idcs,:]\n",
    "ww = w.detach().cpu().numpy()[:,idcs]\n",
    "\n",
    "if not Path(wdir).exists():\n",
    "    os.mkdir(wdir)\n",
    "    \n",
    "if not Path(cdir).exists():\n",
    "    os.mkdir(cdir)\n",
    "\n",
    "for i,sub in enumerate(subs):\n",
    "    fname = f'{wdir}/{sub}_comp-{task}dd_weights.npy'\n",
    "    np.save(fname, ww[i])\n",
    "\n",
    "print('Done 1')\n",
    "    \n",
    "for i in range(book.shape[0]):\n",
    "    fname = f'{cdir}/{task}dd_comp-{i}.npy'\n",
    "    np.save(fname, cc[i])\n",
    "\n",
    "print('Done 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35fd1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "15.307805767059326\n"
     ]
    }
   ],
   "source": [
    "ntrain = 700\n",
    "rs = []\n",
    "\n",
    "for i in range(100):\n",
    "    idcs = torch.randperm(830)\n",
    "\n",
    "    y = get_y(metadict, ['wrat'], subs)[0]\n",
    "    y = y[idcs]\n",
    "    y = torch.from_numpy(y).float().cuda()\n",
    "    ytr = y[:ntrain]\n",
    "    yt = y[ntrain:]\n",
    "\n",
    "    mu = torch.mean(ytr)\n",
    "\n",
    "    ytr = ytr - mu\n",
    "    yt = yt - mu\n",
    "\n",
    "    x = w[idcs]\n",
    "#     x = torch.mean(x, axis=-1)\n",
    "    xtr = x[:ntrain]\n",
    "    xt = x[ntrain:]\n",
    "    \n",
    "    mux = torch.mean(xtr, axis=0)\n",
    "    \n",
    "#     xtr = xtr - mux\n",
    "#     xt = xt - mux\n",
    "\n",
    "    w2,_,_,_ = torch.linalg.lstsq(xtr.T@xtr+1e-2*torch.eye(xtr.shape[-1]).float().cuda(), xtr.T@ytr)\n",
    "    yhat = xt@w2\n",
    "#     print(torch.mean(yt**2)**0.5)\n",
    "    rmse = torch.mean((yhat-yt)**2)**0.5\n",
    "    rmse = float(rmse)\n",
    "    rs.append(rmse)\n",
    "    \n",
    "print('---')\n",
    "print(np.mean(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd1f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
