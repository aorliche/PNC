{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2a226c",
   "metadata": {},
   "source": [
    "# Partial correlation weights for linear age, sex, intelligence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9086dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'wrat', 'missingage', 'missingsex', 'missingwrat', 'failedqc']\n",
      "['emoid', 'nback', 'rest']\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9dae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n",
      "(830, 264, 124)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get subjects that have all tasks and paras specified\n",
    "Functions for creating independent and response variables\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_subs(allts, metadict, tasks, paras):\n",
    "    # Get subs for all paras\n",
    "    for i,para in enumerate(paras):\n",
    "        tmpset = set([int(sub[4:]) for sub in allts[para].keys()])\n",
    "        if i == 0:\n",
    "            paraset = tmpset\n",
    "        else:\n",
    "            paraset = paraset.intersection(tmpset)\n",
    "    # Get subs for all tasks\n",
    "    for i,task in enumerate(tasks):\n",
    "        tmpset = set([sub for sub in metadict[task].keys()])\n",
    "        if i == 0:\n",
    "            taskset = tmpset\n",
    "        else:\n",
    "            taskset = paraset.intersection(tmpset)\n",
    "    # Remove QC failures\n",
    "    allsubs = taskset.intersection(paraset)\n",
    "    for badsub in metadict['failedqc']:\n",
    "        try:\n",
    "            allsubs.remove(int(badsub[4:]))\n",
    "        except:\n",
    "            pass\n",
    "    return list(allsubs)\n",
    "\n",
    "def get_X(allts, paras, subs):\n",
    "    X = []\n",
    "    for para in paras:\n",
    "        pX = [allts[para][f'sub-{sub}'] for sub in subs]\n",
    "        pX = np.stack(pX)\n",
    "        X.append(pX)\n",
    "    return X\n",
    "\n",
    "def get_y(metadict, tasks, subs):\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        if task == 'age' or task == 'wrat':\n",
    "            var = [metadict[task][sub] for sub in subs]\n",
    "            var = np.array(var)\n",
    "            y.append(var)\n",
    "        if task == 'sex':\n",
    "            maleness = [metadict[task][sub] == 'M' for sub in subs]\n",
    "            maleness = np.array(maleness)\n",
    "            sex = np.stack([maleness, 1-maleness], axis=1)\n",
    "            y.append(sex)\n",
    "    return y\n",
    "\n",
    "subs = get_subs(allts, metadict, ['wrat'], ['rest', 'nback', 'emoid'])\n",
    "print(len(subs))\n",
    "\n",
    "X = get_X(allts, ['rest', 'nback', 'emoid'], subs)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4324cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 264, 124)\n"
     ]
    }
   ],
   "source": [
    "# TS to condensed FC\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_bandpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "tr = 3\n",
    "\n",
    "def filter_design_ts(X):\n",
    "    Xs = []\n",
    "    for i in range(X.shape[0]):\n",
    "        nX = butter_bandpass_filter(X[i], [0.01, 0.15], 1/tr)\n",
    "        Xs.append(nX)\n",
    "    return np.stack(Xs)\n",
    "\n",
    "def ts_to_flat_fc(X):\n",
    "    p = np.corrcoef(X)\n",
    "    a,b = np.triu_indices(p[0].shape[0], 1)\n",
    "    p = p[a,b]\n",
    "    return p\n",
    "\n",
    "X = [np.stack([ts for ts in filter_design_ts(Xp)]) for Xp in X]\n",
    "# Xfiltnorm = [tsmod/np.linalg.norm(tsmod, axis=(-1), keepdims=True) for tsmod in ts]\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103fbd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done partials\n",
      "Done partials\n",
      "Done partials\n",
      "(3, 830, 34716)\n"
     ]
    }
   ],
   "source": [
    "# Get all partial correlations\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "cm = ConnectivityMeasure(kind='partial correlation')\n",
    "\n",
    "a,b = np.triu_indices(264, 1)\n",
    "\n",
    "allp = []\n",
    "for taskidx in range(3):\n",
    "    partials = cm.fit_transform(X[taskidx].transpose(0,2,1))\n",
    "    partials = partials[:,a,b]\n",
    "    allp.append(partials)\n",
    "    print('Done partials')\n",
    "\n",
    "allp = np.stack(allp)\n",
    "print(allp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3f5866b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.0752, device='cuda:0')\n",
      "Done 0\n",
      "tensor(28.9338, device='cuda:0')\n",
      "Done 1\n",
      "tensor(28.3771, device='cuda:0')\n",
      "Done 2\n",
      "tensor(26.8056, device='cuda:0')\n",
      "Done 3\n",
      "tensor(29.4164, device='cuda:0')\n",
      "Done 4\n",
      "tensor(29.7039, device='cuda:0')\n",
      "Done 5\n",
      "tensor(26.5510, device='cuda:0')\n",
      "Done 6\n",
      "tensor(31.6080, device='cuda:0')\n",
      "Done 7\n",
      "tensor(28.3975, device='cuda:0')\n",
      "Done 8\n",
      "tensor(29.7382, device='cuda:0')\n",
      "Done 9\n",
      "tensor(30.5065, device='cuda:0')\n",
      "Done 10\n",
      "tensor(27.5603, device='cuda:0')\n",
      "Done 11\n",
      "tensor(29.1412, device='cuda:0')\n",
      "Done 12\n",
      "tensor(27.1840, device='cuda:0')\n",
      "Done 13\n",
      "tensor(28.2827, device='cuda:0')\n",
      "Done 14\n",
      "tensor(31.6508, device='cuda:0')\n",
      "Done 15\n",
      "tensor(28.2971, device='cuda:0')\n",
      "Done 16\n",
      "tensor(27.7216, device='cuda:0')\n",
      "Done 17\n",
      "tensor(29.5121, device='cuda:0')\n",
      "Done 18\n",
      "tensor(28.1083, device='cuda:0')\n",
      "Done 19\n",
      "---\n",
      "tensor(28.8286, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check prediction using partials\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "modidx = 0\n",
    "mod = 'rest'\n",
    "task = \"age\"\n",
    "sm=0\n",
    "\n",
    "for ii in range(20):\n",
    "\n",
    "    ntrain = 700\n",
    "    idcs = torch.randperm(allp.shape[1])\n",
    "\n",
    "    x = torch.from_numpy(allp[modidx]).float().cuda()\n",
    "    x = x[idcs]\n",
    "    xtr = x[:ntrain]\n",
    "    xt = x[ntrain:]\n",
    "\n",
    "    y = get_y(metadict, [task], subs)[0]\n",
    "    y = torch.from_numpy(y).float().cuda()\n",
    "    y = y[idcs]\n",
    "    ytr = y[:ntrain]\n",
    "    yt = y[ntrain:]\n",
    "    mu = torch.mean(ytr)\n",
    "    ytr = ytr - mu\n",
    "    yt = yt - mu\n",
    "\n",
    "    def toDict(w, acc):\n",
    "        dct = dict(w=w.detach().cpu().numpy(), \n",
    "                   trsubs=sorted([subs[i] for i in idcs[:ntrain]]),\n",
    "                   tsubs=sorted([subs[i] for i in idcs[ntrain:]]),\n",
    "                   desc=f\"Least squares partial corr {task} {mod} rmse: {float(acc)}\")\n",
    "        return dct\n",
    "\n",
    "    def save(dct, dr, idx):\n",
    "        base = f\"/home/anton/Documents/Tulane/Research/ImageNomer/data/anton/cohorts/test/weights/partial\"\n",
    "        with open(f\"{base}/{dr}/{mod}{idx}.pkl\", 'wb') as f:\n",
    "            pickle.dump(dct, f)\n",
    "\n",
    "    w, _, _, _ = torch.linalg.lstsq(xtr, ytr)\n",
    "    yhat = xt@w\n",
    "    acc = mseLoss(yhat, yt)**0.5\n",
    "\n",
    "    print(acc)\n",
    "    sm += acc/20\n",
    "    save(toDict(w,acc), f'{task}_mean_zero', ii)\n",
    "    print(f'Done {ii}')\n",
    "    \n",
    "print('---')\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "40b1a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7923076923076923\n",
      "[[0.89552239 0.10447761]\n",
      " [0.31746032 0.68253968]]\n",
      "Done 0\n",
      "0.7384615384615385\n",
      "[[0.7037037  0.2962963 ]\n",
      " [0.20408163 0.79591837]]\n",
      "Done 1\n",
      "0.7076923076923077\n",
      "[[0.72       0.28      ]\n",
      " [0.30909091 0.69090909]]\n",
      "Done 2\n",
      "0.7615384615384615\n",
      "[[0.81333333 0.18666667]\n",
      " [0.30909091 0.69090909]]\n",
      "Done 3\n",
      "0.7692307692307693\n",
      "[[0.80327869 0.19672131]\n",
      " [0.26086957 0.73913043]]\n",
      "Done 4\n",
      "0.8\n",
      "[[0.8125 0.1875]\n",
      " [0.22   0.78  ]]\n",
      "Done 5\n",
      "0.7769230769230769\n",
      "[[0.84057971 0.15942029]\n",
      " [0.29508197 0.70491803]]\n",
      "Done 6\n",
      "0.7692307692307693\n",
      "[[0.89393939 0.10606061]\n",
      " [0.359375   0.640625  ]]\n",
      "Done 7\n",
      "0.7615384615384615\n",
      "[[0.775 0.225]\n",
      " [0.26  0.74 ]]\n",
      "Done 8\n",
      "0.7461538461538462\n",
      "[[0.859375   0.140625  ]\n",
      " [0.36363636 0.63636364]]\n",
      "Done 9\n",
      "0.8076923076923077\n",
      "[[0.82352941 0.17647059]\n",
      " [0.20967742 0.79032258]]\n",
      "Done 10\n",
      "0.823076923076923\n",
      "[[0.84615385 0.15384615]\n",
      " [0.2        0.8       ]]\n",
      "Done 11\n",
      "0.6846153846153846\n",
      "[[0.80555556 0.19444444]\n",
      " [0.46551724 0.53448276]]\n",
      "Done 12\n",
      "0.8\n",
      "[[0.82894737 0.17105263]\n",
      " [0.24074074 0.75925926]]\n",
      "Done 13\n",
      "0.7923076923076923\n",
      "[[0.84       0.16      ]\n",
      " [0.27272727 0.72727273]]\n",
      "Done 14\n",
      "0.7692307692307693\n",
      "[[0.8        0.2       ]\n",
      " [0.27272727 0.72727273]]\n",
      "Done 15\n",
      "0.8538461538461538\n",
      "[[0.90909091 0.09090909]\n",
      " [0.203125   0.796875  ]]\n",
      "Done 16\n",
      "0.7307692307692307\n",
      "[[0.7826087  0.2173913 ]\n",
      " [0.32786885 0.67213115]]\n",
      "Done 17\n",
      "0.7692307692307693\n",
      "[[0.76623377 0.23376623]\n",
      " [0.22641509 0.77358491]]\n",
      "Done 18\n",
      "0.8307692307692308\n",
      "[[0.90625    0.09375   ]\n",
      " [0.24242424 0.75757576]]\n",
      "Done 19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "modidx = 0\n",
    "mod = 'rest'\n",
    "task = \"sex\"\n",
    "\n",
    "for ii in range(20):\n",
    "    ntrain = 700\n",
    "    idcs = np.arange(allp[modidx].shape[0])\n",
    "    np.random.shuffle(idcs)\n",
    "\n",
    "    x = allp[modidx]\n",
    "    x = x[idcs]\n",
    "    xtr = x[:ntrain]\n",
    "    xt = x[ntrain:]\n",
    "\n",
    "    y = get_y(metadict, [task], subs)[0][:,0]\n",
    "    y = y[idcs]\n",
    "    ytr = y[:ntrain]\n",
    "    yt = y[ntrain:]\n",
    "\n",
    "    def toDict(w, acc, conf):\n",
    "        dct = dict(w=w.detach().cpu().numpy(), \n",
    "                   trsubs=sorted([subs[i] for i in idcs[:ntrain]]),\n",
    "                   tsubs=sorted([subs[i] for i in idcs[ntrain:]]),\n",
    "                   desc=f\"Logistic regression partial corr {task} {mod} acc: {float(acc)}\")\n",
    "        return dct\n",
    "\n",
    "    def save(dct, dr, idx):\n",
    "        base = f\"/home/anton/Documents/Tulane/Research/ImageNomer/data/anton/cohorts/test/weights/partial\"\n",
    "        with open(f\"{base}/{dr}/{mod}{idx}.pkl\", 'wb') as f:\n",
    "            pickle.dump(dct, f)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, penalty='l2', C=1, solver='lbfgs').fit(xtr, ytr)\n",
    "    yhat = clf.predict(xt)\n",
    "    acc = np.sum(yhat == yt)/len(yt)\n",
    "    print(acc)\n",
    "\n",
    "    mat = confusion_matrix(yt, yhat, normalize='true', labels=[0,1])\n",
    "    print(mat)\n",
    "\n",
    "    save(toDict(w,acc,mat), f'{task}', ii)\n",
    "    print(f'Done {ii}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44cf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
