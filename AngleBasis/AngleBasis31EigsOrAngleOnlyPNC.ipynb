{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605076e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3849, 1, 264), (3849, 1, 264), (3849,), (3849,), (3849,), (3849, 34716)]\n",
      "0.5263704858404781\n",
      "0.5188360613146272\n",
      "14.398285268901013\n"
     ]
    }
   ],
   "source": [
    "# PNC\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "basis_file = '/home/anton/Documents/Tulane/Research/PNC_Good/AngleBasisNoJit1.pkl'\n",
    "demodir = '/home/anton/Documents/Tulane/Research/ImageNomer/data/anton/cohorts/PNC/'\n",
    "\n",
    "basis = pickle.load(open(basis_file, 'rb'))\n",
    "\n",
    "thetas = []\n",
    "jitter = []\n",
    "age = []\n",
    "sex = []\n",
    "race = []\n",
    "fc = []\n",
    "\n",
    "demo = pickle.load(open(f'{demodir}/demographics.pkl', 'rb'))\n",
    "\n",
    "for subtask in basis:\n",
    "    m = re.search('([^-]+)-(.+)', subtask)\n",
    "    sub = m.group(1)\n",
    "    task = m.group(2)\n",
    "    if sub not in demo['Race'] or demo['Race'][sub] not in ['AA', 'EA']:\n",
    "        continue\n",
    "    a = demo['age_at_cnb'][sub]\n",
    "    s = demo['Sex'][sub] == 'M'\n",
    "    r = demo['Race'][sub] == 'AA'\n",
    "    age.append(a)\n",
    "    sex.append(s)\n",
    "    race.append(r)\n",
    "    thetas.append(basis[subtask]['thetas'])\n",
    "    jitter.append(basis[subtask]['jitter'])\n",
    "    p = np.load(f'{demodir}/fc/{sub}_task-{task}_fc.npy')\n",
    "    fc.append(p)\n",
    "    \n",
    "thetas = np.stack(thetas)\n",
    "jitter = np.stack(jitter)\n",
    "age = np.array(age).astype('int')\n",
    "sex = np.array(sex).astype('int')\n",
    "race = np.array(race).astype('int')\n",
    "fc = np.stack(fc)\n",
    "\n",
    "print([a.shape for a in [thetas, jitter, sex, race, age, fc]])\n",
    "print(np.mean(1-sex))\n",
    "print(np.mean(1-race))\n",
    "print(np.mean(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ee9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done eigs\n",
      "Done 20\n",
      "Done 15\n",
      "Done 10\n",
      "Done 5\n",
      "Done 3\n",
      "Done 1\n"
     ]
    }
   ],
   "source": [
    "# Jitter Only\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mat2vec(mat):\n",
    "    a,b = np.triu_indices(264,1)\n",
    "    return mat[:,a,b]\n",
    "\n",
    "a,b = np.triu_indices(264,1)\n",
    "X = np.ones((fc.shape[0],264,264))\n",
    "X[:,a,b] = fc\n",
    "X[:,b,a] = fc\n",
    "w, v = np.linalg.eig(X)\n",
    "print('Done eigs')\n",
    "\n",
    "w[:,20:] = 0\n",
    "aps20 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps20 = mat2vec(aps20)\n",
    "print('Done 20')\n",
    "\n",
    "w[:,15:] = 0\n",
    "aps15 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps15 = mat2vec(aps15)\n",
    "print('Done 15')\n",
    "\n",
    "w[:,10:] = 0\n",
    "aps10 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps10 = mat2vec(aps10)\n",
    "print('Done 10')\n",
    "\n",
    "w[:,5:] = 0\n",
    "aps5 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps5 = mat2vec(aps5)\n",
    "print('Done 5')\n",
    "\n",
    "w[:,3:] = 0\n",
    "aps3 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps3 = mat2vec(aps3)\n",
    "print('Done 3')\n",
    "\n",
    "w[:,1:] = 0\n",
    "aps1 = np.real(np.einsum('nab,nb,ncb->nac',v,w,v))\n",
    "aps1 = mat2vec(aps1)\n",
    "print('Done 1')\n",
    "\n",
    "w = None\n",
    "v = None\n",
    "X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66eec197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0\n",
      "Done 500\n",
      "Done 1000\n",
      "Done 1500\n",
      "Done 2000\n",
      "Done 2500\n",
      "Done 3000\n",
      "Done 3500\n",
      "(349, 1, 34716)\n",
      "(3849, 34716)\n",
      "(3849, 34716)\n"
     ]
    }
   ],
   "source": [
    "# Angle only or regular AngleBasis\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    if isinstance(yhat, np.ndarray) or isinstance(yhat, int):\n",
    "        f = np.mean\n",
    "    else:\n",
    "        f = torch.mean\n",
    "    return f((y-yhat)**2)**0.5\n",
    "\n",
    "def tops(thetas, jitter):\n",
    "    t0 = np.expand_dims(thetas, 2)\n",
    "    t1 = np.expand_dims(thetas, 3)\n",
    "    j0 = np.expand_dims(jitter, 2)\n",
    "    j1 = np.expand_dims(jitter, 3)\n",
    "    ps = np.cos(t0-t1)*(j0*j1)\n",
    "    a,b = np.triu_indices(264, 1)\n",
    "    ps = ps[:,:,a,b]\n",
    "    return ps\n",
    "\n",
    "paps = []\n",
    "pres = []\n",
    "\n",
    "for i in range(0,fc.shape[0],500):\n",
    "    ps = tops(thetas[i:i+500], jitter[i:i+500])\n",
    "    aps = np.mean(ps, axis=1)\n",
    "    res = fc[i:i+500] - aps\n",
    "    paps.append(aps)\n",
    "    pres.append(res)\n",
    "    print(f'Done {i}')\n",
    "    \n",
    "aps = np.concatenate(paps)\n",
    "res = np.concatenate(pres)\n",
    "\n",
    "print(ps.shape)\n",
    "print(aps.shape)\n",
    "print(res.shape)\n",
    "\n",
    "aps1 = aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647347e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tonp(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def totorch(x):\n",
    "    return torch.from_numpy(x).float().cuda()\n",
    "\n",
    "def totorchidcs(x):\n",
    "    return torch.from_numpy(x).long().cuda() #F.one_hot(torch.from_numpy(x)).float().cuda()\n",
    "\n",
    "def rmse(yt, yhat):\n",
    "    return torch.mean((yt-yhat)**2)**0.5\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(34716,100).float().cuda()\n",
    "        self.l2 = nn.Linear(100,2).float().cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "def fit_mlp(xtr, ytr, verbose=False):\n",
    "    xtr = totorch(xtr)\n",
    "    ytr = totorchidcs(ytr)\n",
    "    \n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    mlp = MLP()\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "\n",
    "    nepochs = 1000\n",
    "    pperiod = 100\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        optim.zero_grad()\n",
    "        yhat = mlp(xtr)\n",
    "        loss = ce(yhat, ytr)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if verbose:\n",
    "            if epoch % pperiod == 0 or epoch == nepochs-1:\n",
    "                print(f'{epoch} {float(loss)}')\n",
    "\n",
    "    if verbose:\n",
    "        print('Complete')\n",
    "    \n",
    "    return mlp\n",
    "    \n",
    "def predict_help(model, xt):\n",
    "    xt = totorch(xt)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(xt)\n",
    "        return tonp(yhat)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a197bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420463385760189\n",
      "0.7366886521224421\n",
      "0.7154540842212074\n",
      "0.8994892609504481\n",
      "0.9452731270082868\n",
      "0.947945205479452\n",
      "0.9451243023845763\n",
      "0.6974395399966177\n",
      "0.7520987654320987\n",
      "0.7483713850837139\n",
      "0.7544529003889734\n",
      "0.7334415694233046\n",
      "0.7302350752579063\n",
      "0.7315542026044308\n",
      "0.7758836462032809\n",
      "0.8596313208185354\n",
      "0.9231117875866734\n",
      "0.9304447826822256\n",
      "0.9392186707255201\n",
      "0.8880297649247421\n",
      "---\n",
      "0.9486622695755116\n",
      "0.7357957043801793\n",
      "0.8791002875021139\n",
      "0.9140267207847117\n",
      "0.9248165060037206\n",
      "0.9473431422289871\n",
      "0.9503263994588195\n",
      "0.7053948926095045\n",
      "0.7157179096905124\n",
      "0.758836462032809\n",
      "0.7543784880771182\n",
      "0.7466260781329275\n",
      "0.7209267715203789\n",
      "0.7339556908506679\n",
      "0.8174260104853712\n",
      "0.8862912227295789\n",
      "0.9073296127177406\n",
      "0.9401792660240149\n",
      "0.9430610519194995\n",
      "0.8852697446304751\n",
      "---\n",
      "0.9439607644173853\n",
      "0.7140402502959581\n",
      "0.8544089294774226\n",
      "0.8990630813461864\n",
      "0.9264400473532893\n",
      "0.9424792829359039\n",
      "0.9463825469304922\n",
      "0.6568104177236598\n",
      "0.7136952477591747\n",
      "0.7392727887705056\n",
      "0.7899205141214272\n",
      "0.7287468290208016\n",
      "0.72259766615931\n",
      "0.6977845425334009\n",
      "0.8181024860476915\n",
      "0.8616201589717571\n",
      "0.9228885506511079\n",
      "0.925959749704042\n",
      "0.9401657365127686\n",
      "0.8904312531709792\n",
      "---\n",
      "0.9523287671232877\n",
      "0.7422763402672079\n",
      "0.8728361237950278\n",
      "0.8719296465415187\n",
      "0.9414916286149163\n",
      "0.9354845256215119\n",
      "0.8601319127346525\n",
      "0.7103669879925587\n",
      "0.7530796549974632\n",
      "0.7544326061221038\n",
      "0.7609132420091325\n",
      "0.7332386267546085\n",
      "0.7164417385421952\n",
      "0.7426010485371216\n",
      "0.8513512599357349\n",
      "0.8335193641129715\n",
      "0.924897683071199\n",
      "0.9185929308303737\n",
      "0.8597598511753762\n",
      "0.8437747336377472\n",
      "---\n",
      "0.9451243023845763\n",
      "0.7672856418061897\n",
      "0.8503771351259934\n",
      "0.8833553187891088\n",
      "0.9375883646203281\n",
      "0.9342736343649586\n",
      "0.9393742601048536\n",
      "0.7079993235244377\n",
      "0.7341518687637408\n",
      "0.7433113478775579\n",
      "0.7782039573820396\n",
      "0.7439742939286318\n",
      "0.7054693049213597\n",
      "0.7470793167596821\n",
      "0.8238931168611534\n",
      "0.8423811939793675\n",
      "0.9232538474547607\n",
      "0.9185794013191274\n",
      "0.9371757145273127\n",
      "0.8894165398274988\n",
      "---\n",
      "0.9504549298156604\n",
      "0.7558261457804838\n",
      "0.8724505327245053\n",
      "0.9062201927955352\n",
      "0.9421410451547436\n",
      "0.9484322678843228\n",
      "0.75064434297311\n",
      "0.6957145273127009\n",
      "0.7229764924742094\n",
      "0.7404025029595807\n",
      "0.7762421782513106\n",
      "0.7412075088787418\n",
      "0.715305259597497\n",
      "0.746781667512261\n",
      "0.8249890072721122\n",
      "0.8686216810417724\n",
      "0.9146017250126839\n",
      "0.9351192288178589\n",
      "0.7860240148824624\n",
      "0.7278335870116692\n",
      "---\n",
      "0.9258718078809404\n",
      "0.7029798748520211\n",
      "0.866010485371216\n",
      "0.8386876374090987\n",
      "0.934686284457974\n",
      "0.9242009132420091\n",
      "0.9379063081346186\n",
      "0.6770573313039067\n",
      "0.7296397767630645\n",
      "0.7526670049044478\n",
      "0.7592491121258244\n",
      "0.7182073397598512\n",
      "0.7032639945881954\n",
      "0.6953492305090478\n",
      "0.8232166412988332\n",
      "0.837869101978691\n",
      "0.9098866903433114\n",
      "0.9155555555555557\n",
      "0.9259394554371723\n",
      "0.861078978521901\n",
      "---\n",
      "0.9356807035345849\n",
      "0.7031557584982242\n",
      "0.8700693387451378\n",
      "0.9032504650769491\n",
      "0.6005411804498562\n",
      "0.9419448672416709\n",
      "0.9421342803991206\n",
      "0.6401623541349569\n",
      "0.7396042617960426\n",
      "0.7468628445797396\n",
      "0.7577541011330966\n",
      "0.7162861491628616\n",
      "0.7135599526467106\n",
      "0.683862675460849\n",
      "0.8320108236089971\n",
      "0.8628581092508034\n",
      "0.7122002367664468\n",
      "0.9249044478268222\n",
      "0.9304718417047184\n",
      "0.8442415017757483\n",
      "---\n",
      "0.9295856587180789\n",
      "0.7444884153559952\n",
      "0.8735599526467106\n",
      "0.9014848638592929\n",
      "0.9371013022154575\n",
      "0.9215085405039743\n",
      "0.9489125655335701\n",
      "0.6977168949771689\n",
      "0.7580991036698799\n",
      "0.7335633350245223\n",
      "0.7627735498055133\n",
      "0.7498122780314561\n",
      "0.7038322340605447\n",
      "0.7155014375105699\n",
      "0.8524809741248097\n",
      "0.8599019110434636\n",
      "0.9150481988838153\n",
      "0.9223473702012515\n",
      "0.9367360054118046\n",
      "0.8926162692372739\n",
      "---\n",
      "0.946111956705564\n",
      "0.7401995602908845\n",
      "0.8826653137155421\n",
      "0.9071401995602909\n",
      "0.9419989852866564\n",
      "0.9414713343480466\n",
      "0.9351192288178589\n",
      "0.6159986470488753\n",
      "0.7706883138846609\n",
      "0.7441704718417047\n",
      "0.7962658548959918\n",
      "0.7652765093860985\n",
      "0.7240926771520378\n",
      "0.708242854726873\n",
      "0.8549839337053948\n",
      "0.8722070015220701\n",
      "0.9287062404870624\n",
      "0.9197429392863183\n",
      "0.9330018603077963\n",
      "0.8790123456790123\n",
      "---\n",
      "0.9419827498731609 0.00840973414988138\n",
      "0.8601704718417047 0.04756189215407201\n",
      "0.7342736343649585 0.020305086692362286\n",
      "0.8536932183324877 0.04705245532054679\n",
      "0.892464738711314 0.02134648798859218\n",
      "0.9032078471165228 0.1010896338334921\n",
      "0.9385083713850838 0.00907040118454986\n",
      "0.9156056147471672 0.06047299546036556\n",
      "0.6804660916624388 0.03073018671969482\n",
      "0.7389751395230847 0.018136580313086573\n",
      "0.7461890749196686 0.00726757893614724\n",
      "0.7690153898190427 0.014385324027124738\n",
      "0.7376817182479283 0.01412524769406827\n",
      "0.7155724674446136 0.008728635377245096\n",
      "0.7202712667004904 0.02203279441160495\n",
      "0.8274337899543379 0.022046111533128444\n",
      "0.8584901065449012 0.015505280324695433\n",
      "0.8981924572974801 0.062345755476149164\n",
      "0.9251425672247588 0.007545375065263848\n",
      "0.913155420260443 0.04827718691827243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def cat(x, **kwargs):\n",
    "    return np.concatenate(x, **kwargs)\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    if isinstance(yhat, np.ndarray) or isinstance(yhat, int):\n",
    "        f = np.mean\n",
    "    else:\n",
    "        f = torch.mean\n",
    "    return f((y-yhat)**2)**0.5\n",
    "\n",
    "def predict(xtr, xt, ytr, yt, lst):\n",
    "    reg = fit_mlp(xtr, ytr)\n",
    "    p = predict_help(reg, xt)\n",
    "    acc = roc_auc_score(yt, p[:,1])\n",
    "    print(acc)\n",
    "    lst.append(acc)\n",
    "    return p\n",
    "    \n",
    "def get_res(fctr, fct, abtr, abt):\n",
    "    return fctr-abtr, fct-abt\n",
    "\n",
    "def combine(yt, p0, p1, lst):\n",
    "    p = (p0+p1)/2\n",
    "    acc = roc_auc_score(yt, p[:,1])\n",
    "    print(acc)\n",
    "    lst.append(acc)\n",
    "\n",
    "rfc = []\n",
    "\n",
    "rab1 = []\n",
    "rab3 = []\n",
    "rab5 = []\n",
    "rab10 = []\n",
    "rab15 = []\n",
    "rab20 = []\n",
    "\n",
    "rres1 = []\n",
    "rres3 = []\n",
    "rres5 = []\n",
    "rres10 = []\n",
    "rres15 = []\n",
    "rres20 = []\n",
    "\n",
    "rens1 = []\n",
    "rens3 = []\n",
    "rens5 = []\n",
    "rens10 = []\n",
    "rens15 = []\n",
    "rens20 = []\n",
    "\n",
    "rbest = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    x0tr, x0t, x1tr, x1t, x2tr, x2t, x3tr, x3t, x4tr, x4t, x5tr, x5t, x6tr, x6t, ytr, yt = train_test_split(\n",
    "        fc, aps1, aps3, aps5, aps10, aps15, aps20, sex, stratify=sex, train_size=0.8)\n",
    "    \n",
    "    x1atr, x1at = get_res(x0tr, x0t, x1tr, x1t)\n",
    "    x2atr, x2at = get_res(x0tr, x0t, x2tr, x2t)\n",
    "    x3atr, x3at = get_res(x0tr, x0t, x3tr, x3t)\n",
    "    x4atr, x4at = get_res(x0tr, x0t, x4tr, x4t)\n",
    "    x5atr, x5at = get_res(x0tr, x0t, x5tr, x5t)\n",
    "    x6atr, x6at = get_res(x0tr, x0t, x6tr, x6t)\n",
    "\n",
    "    predict(x0tr, x0t, ytr, yt, rfc)\n",
    "    \n",
    "    p1 = predict(x1tr, x1t, ytr, yt, rab1)\n",
    "    p2 = predict(x2tr, x2t, ytr, yt, rab3)\n",
    "    p3 = predict(x3tr, x3t, ytr, yt, rab5)\n",
    "    p4 = predict(x4tr, x4t, ytr, yt, rab10)\n",
    "    p5 = predict(x5tr, x5t, ytr, yt, rab15)\n",
    "    p6 = predict(x6tr, x6t, ytr, yt, rab20)\n",
    "    \n",
    "    p1a = predict(x1atr, x1at, ytr, yt, rres1)\n",
    "    p2a = predict(x2atr, x2at, ytr, yt, rres3)\n",
    "    p3a = predict(x3atr, x3at, ytr, yt, rres5)\n",
    "    p4a = predict(x4atr, x4at, ytr, yt, rres10)\n",
    "    p5a = predict(x5atr, x5at, ytr, yt, rres15)\n",
    "    p6a = predict(x6atr, x6at, ytr, yt, rres20)\n",
    "    \n",
    "    combine(yt, p1, p1a, rens1)\n",
    "    combine(yt, p2, p2a, rens3)\n",
    "    combine(yt, p3, p3a, rens5)\n",
    "    combine(yt, p4, p4a, rens10)\n",
    "    combine(yt, p5, p5a, rens15)\n",
    "    combine(yt, p6, p6a, rens20)\n",
    "\n",
    "    combine(yt, p6, p1a, rbest)\n",
    "    \n",
    "    print('---')\n",
    "    \n",
    "print(np.mean(rfc), np.std(rfc))\n",
    "print(np.mean(rbest), np.std(rbest))\n",
    "\n",
    "print(np.mean(rab1), np.std(rab1))\n",
    "print(np.mean(rab3), np.std(rab3))\n",
    "print(np.mean(rab5), np.std(rab5))\n",
    "print(np.mean(rab10), np.std(rab10))\n",
    "print(np.mean(rab15), np.std(rab15))\n",
    "print(np.mean(rab20), np.std(rab20))\n",
    "\n",
    "print(np.mean(rres1), np.std(rres1))\n",
    "print(np.mean(rres3), np.std(rres3))\n",
    "print(np.mean(rres5), np.std(rres5))\n",
    "print(np.mean(rres10), np.std(rres10))\n",
    "print(np.mean(rres15), np.std(rres15))\n",
    "print(np.mean(rres20), np.std(rres20))\n",
    "\n",
    "print(np.mean(rens1), np.std(rens1))\n",
    "print(np.mean(rens3), np.std(rens3))\n",
    "print(np.mean(rens5), np.std(rens5))\n",
    "print(np.mean(rens10), np.std(rens10))\n",
    "print(np.mean(rens15), np.std(rens15))\n",
    "print(np.mean(rens20), np.std(rens20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5f595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
