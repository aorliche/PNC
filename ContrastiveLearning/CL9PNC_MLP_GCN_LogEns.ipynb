{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ced9523-e383-4487-9131-73358235d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1329,), (1329,), (1329,), (1329, 34716), (1329, 34716), (1329, 34716)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "pncdir = '/home/anton/Documents/Tulane/Research/ImageNomer/data/anton/cohorts/PNC/'\n",
    "pncdemo = pickle.load(open(f'{pncdir}/demographics.pkl', 'rb'))\n",
    "pnctopdir = '/home/anton/Documents/Tulane/Research/Work/ContrastiveLearning/PNC/'\n",
    "\n",
    "task = 'emoid'\n",
    "\n",
    "fc = []\n",
    "aps20 = []\n",
    "aps15 = []\n",
    "aps10 = []\n",
    "aps5 = []\n",
    "aps3 = []\n",
    "aps1 = []\n",
    "age = []\n",
    "sex = []\n",
    "race = []\n",
    "\n",
    "for sub in pncdemo['age_at_cnb']:\n",
    "    try:\n",
    "        a = pncdemo['age_at_cnb'][sub]\n",
    "        s = pncdemo['Sex'][sub]\n",
    "        r = pncdemo['Race'][sub]\n",
    "        if r not in ['AA', 'EA']:\n",
    "            continue\n",
    "        s = s == 'M'\n",
    "        r = r == 'AA'\n",
    "        fc.append(np.load(f'{pncdir}/fc/{sub}_task-{task}_fc.npy'))\n",
    "        aps20.append(np.load(f'{pnctopdir}/Top20/{sub}_task-{task}top20_fc.npy'))\n",
    "        aps3.append(np.load(f'{pnctopdir}/Top3/{sub}_task-{task}top3_fc.npy'))\n",
    "        aps1.append(np.load(f'{pnctopdir}/Top1/{sub}_task-{task}top1_fc.npy'))\n",
    "        age.append(a)\n",
    "        sex.append(s)\n",
    "        race.append(r)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "age = np.stack(age)\n",
    "sex = np.stack(sex).astype('int')\n",
    "race = np.stack(race).astype('int')\n",
    "fc = np.stack(fc)\n",
    "aps20 = np.stack(aps20)\n",
    "aps1 = np.stack(aps1)\n",
    "\n",
    "print([x.shape for x in [age, sex, race, fc, aps20, aps1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c62e6f-1d3a-4ce1-9a55-6b669c2b10f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14.614675521850586 14.611854553222656\n",
      "500 1.1868321895599365 2.067201614379883\n",
      "1000 0.21831853687763214 0.9813175201416016\n",
      "1500 0.10926540940999985 0.46686476469039917\n",
      "2000 0.08424967527389526 0.27859917283058167\n",
      "2500 0.07564762979745865 0.19285067915916443\n",
      "3000 0.07312721014022827 0.15111427009105682\n",
      "3500 0.0742514505982399 0.12591402232646942\n",
      "4000 0.07624126970767975 0.11299039423465729\n",
      "4500 0.07730118930339813 0.1056116595864296\n",
      "4999 0.05518325790762901 0.08887757360935211\n",
      "2.8701558113098145\n",
      "2.642207384109497\n",
      "0 14.798666954040527 14.612897872924805\n",
      "500 1.6970840692520142 1.8030000925064087\n",
      "1000 0.24957388639450073 0.7646662592887878\n",
      "1500 0.09078259021043777 0.4263554811477661\n",
      "2000 0.06076277047395706 0.2745044529438019\n",
      "2500 0.05025229975581169 0.22119274735450745\n",
      "3000 0.04338376969099045 0.1780514419078827\n",
      "3500 0.036099329590797424 0.1568181961774826\n",
      "4000 0.028447171673178673 0.14860761165618896\n",
      "4500 0.023549148812890053 0.1449964940547943\n",
      "4999 0.07064847648143768 0.10766331106424332\n",
      "2.475322961807251\n",
      "2.3414950370788574\n",
      "0 14.7495698928833 14.828686714172363\n",
      "500 1.806168794631958 1.7289166450500488\n",
      "1000 0.2936865985393524 0.6839830875396729\n",
      "1500 0.09888145327568054 0.380700945854187\n",
      "2000 0.06341724842786789 0.2596715986728668\n",
      "2500 0.05226625129580498 0.1959989368915558\n",
      "3000 0.04656866565346718 0.15821494162082672\n",
      "3500 0.0406506322324276 0.1408529281616211\n",
      "4000 0.03359794616699219 0.12896066904067993\n",
      "4500 0.02805996872484684 0.11694180965423584\n",
      "4999 0.06346298009157181 0.13827942311763763\n",
      "2.631027936935425\n",
      "2.442101240158081\n",
      "0 14.820886611938477 14.759186744689941\n",
      "500 1.3509210348129272 2.021707057952881\n",
      "1000 0.23463395237922668 0.9670768976211548\n",
      "1500 0.10689061880111694 0.4750406742095947\n",
      "2000 0.07862392067909241 0.2788791358470917\n",
      "2500 0.06842131912708282 0.19384437799453735\n",
      "3000 0.06171172857284546 0.14811119437217712\n",
      "3500 0.053788669407367706 0.12409570068120956\n",
      "4000 0.0478181317448616 0.11261584609746933\n",
      "4500 0.04334677755832672 0.10023678839206696\n",
      "4999 0.08083616197109222 0.10562556982040405\n",
      "2.751168966293335\n",
      "2.5865554809570312\n",
      "0 14.68197250366211 14.532181739807129\n",
      "500 1.4303721189498901 1.4220893383026123\n",
      "1000 0.23728163540363312 0.6741724014282227\n",
      "1500 0.10635730624198914 0.4124833345413208\n",
      "2000 0.07967636734247208 0.309620201587677\n",
      "2500 0.07121791690587997 0.24701368808746338\n",
      "3000 0.06969115883111954 0.21726588904857635\n",
      "3500 0.07200590521097183 0.20014122128486633\n",
      "4000 0.074199378490448 0.19631962478160858\n",
      "4500 0.07503663748502731 0.19333763420581818\n",
      "4999 0.04665234312415123 0.1406957358121872\n",
      "2.7046959400177\n",
      "2.540858030319214\n",
      "0 14.834080696105957 14.777442932128906\n",
      "500 1.463771104812622 1.7470076084136963\n",
      "1000 0.2711530327796936 0.7396300435066223\n",
      "1500 0.1136043444275856 0.42562180757522583\n",
      "2000 0.08006702363491058 0.28209489583969116\n",
      "2500 0.06852646172046661 0.2271139919757843\n",
      "3000 0.06279866397380829 0.17983995378017426\n",
      "3500 0.05815358832478523 0.15201058983802795\n",
      "4000 0.053607720881700516 0.14056292176246643\n",
      "4500 0.05042910575866699 0.1256246715784073\n",
      "4999 0.06847754865884781 0.14557524025440216\n",
      "2.674906015396118\n",
      "2.428694725036621\n",
      "0 14.6849365234375 14.607361793518066\n",
      "500 1.2407811880111694 1.7742670774459839\n",
      "1000 0.22035817801952362 0.7536123991012573\n",
      "1500 0.11096932739019394 0.42444708943367004\n",
      "2000 0.08472791314125061 0.281097412109375\n",
      "2500 0.07497775554656982 0.21849825978279114\n",
      "3000 0.06937214732170105 0.18168728053569794\n",
      "3500 0.06367895752191544 0.16171333193778992\n",
      "4000 0.05823590233922005 0.1503104269504547\n",
      "4500 0.05462801083922386 0.1432165652513504\n",
      "4999 0.08049844950437546 0.11913987249135971\n",
      "2.694300651550293\n",
      "2.54146671295166\n",
      "0 14.895834922790527 14.965563774108887\n",
      "500 1.3832467794418335 2.0186057090759277\n",
      "1000 0.229636549949646 0.9540566802024841\n",
      "1500 0.10672475397586823 0.4756966233253479\n",
      "2000 0.07891488075256348 0.29417884349823\n",
      "2500 0.0698598325252533 0.2059187889099121\n",
      "3000 0.06779804825782776 0.17393475770950317\n",
      "3500 0.07084567844867706 0.13651134073734283\n",
      "4000 0.07562343031167984 0.11597319692373276\n",
      "4500 0.07892272621393204 0.10444021970033646\n",
      "4999 0.037567902356386185 0.11362036317586899\n",
      "2.466109275817871\n",
      "2.2968156337738037\n",
      "0 14.737870216369629 14.689462661743164\n",
      "500 1.182342767715454 1.6833326816558838\n",
      "1000 0.22893618047237396 0.7651143074035645\n",
      "1500 0.11845789104700089 0.4303767681121826\n",
      "2000 0.09219503402709961 0.2917405068874359\n",
      "2500 0.08302441239356995 0.2111518383026123\n",
      "3000 0.08042549341917038 0.18011553585529327\n",
      "3500 0.08152525126934052 0.16313982009887695\n",
      "4000 0.08309109508991241 0.1443120688199997\n",
      "4500 0.08371663093566895 0.1337103694677353\n",
      "4999 0.06212163716554642 0.16269607841968536\n",
      "2.467209577560425\n",
      "2.3035521507263184\n",
      "0 14.783455848693848 14.990077018737793\n",
      "500 1.44798743724823 2.0844852924346924\n",
      "1000 0.2655821442604065 0.9160325527191162\n",
      "1500 0.11376043409109116 0.45127469301223755\n",
      "2000 0.08161518722772598 0.2630291283130646\n",
      "2500 0.07044292986392975 0.18748779594898224\n",
      "3000 0.0649195984005928 0.1488863080739975\n",
      "3500 0.06054197996854782 0.12649598717689514\n",
      "4000 0.05608594790101051 0.11015763133764267\n",
      "4500 0.05265467241406441 0.10439536720514297\n",
      "4999 0.07063014060258865 0.0861012265086174\n",
      "2.4343485832214355\n",
      "2.3222298622131348\n",
      "---\n",
      "2.6169245719909666 0.14069260186052243\n",
      "2.444597625732422 0.12060175185706364\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(34716,30).float().cuda()\n",
    "        self.fc2 = nn.Linear(30,out).float().cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze()\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, out):\n",
    "        super(GCN, self).__init__()\n",
    "        self.fc1 = nn.Linear(34716,30).float().cuda()\n",
    "        self.fc2 = nn.Linear(30,out).float().cuda()\n",
    "\n",
    "    def forward(self, x, E):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.squeeze()\n",
    "        x = E@x\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze()\n",
    "        return x\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    return np.mean((yhat-y)**2)**0.5\n",
    "\n",
    "def rmse2(yhat, y):\n",
    "    return torch.mean((yhat-y)**2)**0.5\n",
    "\n",
    "accs1 = []\n",
    "accs2 = []\n",
    "accs3 = []\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    xtr, xt, ytr, yt, vtr, vt, wtr, wt = train_test_split(fc, age, aps20, aps1, train_size=0.8)\n",
    "\n",
    "    # rega = Ridge(alpha=1).fit(xtr, ytr)\n",
    "    # regb = Ridge(alpha=1).fit(xtr-wtr, ytr)\n",
    "    # # rega = LogisticRegression(max_iter=1000).fit(xtr, ytr)\n",
    "    # # regb = LogisticRegression(max_iter=1000).fit(xtr-wtr, ytr)\n",
    "\n",
    "    # yhata = rega.predict(xt)\n",
    "    # yhatb = regb.predict(xt-wt)\n",
    "    # # yhata = rega.predict_proba(xt)\n",
    "    # # yhatb = regb.predict_proba(xt-wt)\n",
    "\n",
    "    # yhat = (yhata+yhatb)/2\n",
    "    # acc = rmse(yhat, yt)\n",
    "    # # yhat = np.argmax(yhat, axis=1)\n",
    "    # # acc = np.mean(yhat == yt)\n",
    "    # accs1.append(acc)\n",
    "    # print(acc)\n",
    "\n",
    "    mlp = MLP(1)\n",
    "    gcn = GCN(1)\n",
    "\n",
    "    opa = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    opb = torch.optim.Adam(gcn.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    Ntr = len(xtr)\n",
    "    Nt = len(xt)\n",
    "    \n",
    "    Etr = torch.ones(Ntr, Ntr)/Ntr + torch.eye(Ntr)\n",
    "    Etr = Etr.float().cuda()\n",
    "    \n",
    "    Et = torch.ones(Ntr+Nt, Ntr+Nt)/(Ntr+Nt) + torch.eye(Ntr+Nt)\n",
    "    Et = Et.float().cuda()\n",
    "\n",
    "    nepochs = 5000\n",
    "    pperiod = 500\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    xtr = torch.from_numpy(xtr).float().cuda()\n",
    "    ytr = torch.from_numpy(ytr).long().cuda()\n",
    "    xt = torch.from_numpy(xt).float().cuda()\n",
    "    yt = torch.from_numpy(yt).long().cuda()\n",
    "\n",
    "    for e in range(nepochs):\n",
    "        opa.zero_grad()\n",
    "        opb.zero_grad()\n",
    "        yhata = mlp(xtr)\n",
    "        yhatb = gcn(xtr, Etr)\n",
    "        lossa = rmse2(yhata, ytr)\n",
    "        lossb = rmse2(yhatb, ytr)\n",
    "        # lossa = ce(yhata, ytr)\n",
    "        # lossb = ce(yhatb, ytr)\n",
    "        lossa.backward()\n",
    "        lossb.backward()\n",
    "        opa.step()\n",
    "        opb.step()\n",
    "        if e % pperiod == 0 or e == nepochs-1:\n",
    "            print(f'{e} {float(lossa)} {float(lossb)}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhata = mlp(xt)\n",
    "        yhatb = gcn(torch.cat([xtr, xt], axis=0), Et)[Ntr:]\n",
    "        acca = float(rmse2(yhata, yt))\n",
    "        accb = float(rmse2(yhatb, yt))\n",
    "        # yhata = torch.argmax(yhata, dim=1).detach().cpu().numpy()\n",
    "        # yhatb = torch.argmax(yhatb, dim=1).detach().cpu().numpy()\n",
    "        # acca = np.mean(yhata == yt.detach().cpu().numpy())\n",
    "        # accb = np.mean(yhatb == yt.detach().cpu().numpy())\n",
    "        print(acca)\n",
    "        print(accb)\n",
    "        accs2.append(acca)\n",
    "        accs3.append(accb)\n",
    "\n",
    "print('---')\n",
    "# print(np.mean(accs1), np.std(accs1))\n",
    "print(np.mean(accs2), np.std(accs2))\n",
    "print(np.mean(accs3), np.std(accs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cfde1-61cb-48ea-9ac1-e7c0dcceafa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
