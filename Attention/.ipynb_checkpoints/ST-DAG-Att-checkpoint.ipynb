{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a3d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "156\n",
      "{'meta': {'AgeInMonths': 146, 'Gender': 'F', 'Ethnicity': 'AFRICAN', 'AgeGroupID': 2, 'AgeGroupEdge1': 144, 'AgeGroupEdge2': 180}, 'rest': '30', 'nback': '31', 'emoid': '31', 'ID': 600262185931}\n"
     ]
    }
   ],
   "source": [
    "# Bad subjects\n",
    "# Bad IDs: 605515760919, 601983541597\n",
    "\n",
    "# Load split\n",
    "\n",
    "import pickle\n",
    "\n",
    "badIDs = [605515760919, 601983541597]\n",
    "\n",
    "with open('../../Splits/RegressionAllTasks/split1.bin', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "    train = []\n",
    "    trainDirty = d['train']\n",
    "    test = []\n",
    "    testDirty = d['test']\n",
    "    \n",
    "    # Remove bad subjects\n",
    "    for subj in trainDirty:\n",
    "        if subj['ID'] not in badIDs:\n",
    "            train.append(subj)\n",
    "            \n",
    "    for subj in testDirty:\n",
    "        if subj['ID'] not in badIDs:\n",
    "            test.append(subj)\n",
    "    \n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab4aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "def loadTimeseries(_id, _dir):\n",
    "    ts = None\n",
    "    with open('{:s}/{:d}.bin'.format(_dir, _id), 'rb') as f:\n",
    "        ts = pickle.load(f)\n",
    "    return ts\n",
    "\n",
    "train_rest_ts = [loadTimeseries(int(subj['nback']), '../../nback_fmri_power264/timeseries') for subj in train]\n",
    "# train_nback_ts = [loadTimeseries(int(subj['nback']), '../../nback_fmri_power264/timeseries') for subj in train]\n",
    "# train_emoid_ts = [loadTimeseries(int(subj['emoid']), '../../emoid_fmri_power264/timeseries') for subj in train]\n",
    "\n",
    "test_rest_ts = [loadTimeseries(int(subj['nback']), '../../nback_fmri_power264/timeseries') for subj in test]\n",
    "# test_nback_ts = [loadTimeseries(int(subj['nback']), '../../nback_fmri_power264/timeseries') for subj in test]\n",
    "# test_emoid_ts = [loadTimeseries(int(subj['emoid']), '../../emoid_fmri_power264/timeseries') for subj in test]\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5932c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def normalizeSubjects(subjects):\n",
    "    for i in range(len(subjects)):\n",
    "        subj = subjects[i]\n",
    "        subj -= np.mean(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        subj /= np.std(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        if np.sum(np.isnan(subj)) > 0:\n",
    "            print(i)\n",
    "        if np.sum(np.isinf(subj)) > 0:\n",
    "            print(i)\n",
    "\n",
    "# normalizeSubjects(train_rest_ts)\n",
    "# normalizeSubjects(train_nback_ts)\n",
    "# normalizeSubjects(train_emoid_ts)\n",
    "\n",
    "normalizeSubjects(test_rest_ts)\n",
    "# normalizeSubjects(test_nback_ts)\n",
    "# normalizeSubjects(test_emoid_ts)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a693de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Create labels (ages)\n",
    "\n",
    "y_train = np.vstack([subj['meta']['AgeInMonths'] for subj in train])\n",
    "y_test = np.vstack([subj['meta']['AgeInMonths'] for subj in test])\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183f6605",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "# train_rest_ts_torch = [torch.from_numpy(subj).float().cuda() for subj in train_rest_ts]\n",
    "test_rest_ts_torch = [torch.from_numpy(subj).float().cuda() for subj in test_rest_ts]\n",
    "\n",
    "# print(train_rest_ts_torch[0].device)\n",
    "# print(train_rest_ts_torch[0].shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd79bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Functional connectivity\n",
    "\n",
    "from math import floor as f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "# Assumes normalized to mean=0 and std=1\n",
    "def pearsonr(ts):\n",
    "    nt = ts.shape[1]\n",
    "    return ts@ts.T/nt\n",
    "\n",
    "# Threshold at some 0 < pct < 1 of edges\n",
    "def threshold(fc, pct, keepNeg=False):\n",
    "    # Set main diagonal to zero\n",
    "    N = fc.shape[0]\n",
    "    fc -= np.eye(N)\n",
    "    # Do not count edges on main diagonal\n",
    "    # This means we will never get odd number of edges\n",
    "    # If, for instance, we use 100% of edges\n",
    "    nedge = N*N-N \n",
    "    nedgeThr = f(nedge*pct)\n",
    "    if nedgeThr % 2 == 1:\n",
    "        nedgeThr += 1\n",
    "    vec = -np.abs(fc.reshape(-1)) if keepNeg else -fc.reshape(-1)\n",
    "    indcs = np.argsort(vec)[0:nedgeThr]\n",
    "    binfc = np.zeros(N*N)\n",
    "    binfc[indcs] = 1\n",
    "    return binfc.reshape(fc.shape)\n",
    "\n",
    "# Find the degree matrix of a thresholded binary pearson matrix\n",
    "def degree(A):\n",
    "    d = np.sum(A,axis=0)\n",
    "    d[np.where(d==0)] = 1e-5 # Avoid singularity\n",
    "    return np.diag(d)\n",
    "\n",
    "# Find the normalized Laplacian\n",
    "def Laplacian(A):\n",
    "    D = degree(A)\n",
    "    invSqrtD = np.linalg.inv(scipy.linalg.sqrtm(D))\n",
    "    return np.eye(A.shape[0])-invSqrtD@A@invSqrtD\n",
    "\n",
    "# Return chebyshev polynomial of order K\n",
    "def Chebyshev(L, k):\n",
    "    if k < 0 or k > 10:\n",
    "        raise ValueError('Bad value for order of Chebyshev polynomial')\n",
    "    if k == 0:\n",
    "        # Not sure if this should be 1 or I or if this recursion doesn't work for matrices\n",
    "        return np.eye(L.shape[0]) \n",
    "    if k == 1:\n",
    "        return L\n",
    "    return 2*L@Chebyshev(L,k-1)-Chebyshev(L,k-2)\n",
    "\n",
    "# The 4 orders of Chebyshev polynomials for spatial convolution\n",
    "def AllChebyshev(L):\n",
    "    global num\n",
    "    print('{:d} '.format(num), end='')\n",
    "    num += 1\n",
    "    orders = [Chebyshev(L,0), Chebyshev(L,1), Chebyshev(L,2), Chebyshev(L,3)]\n",
    "    orders = np.stack(orders)\n",
    "    return torch.from_numpy(orders).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec52093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Make pearson tensors for FC_conv network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_rest_p_torch = [torch.from_numpy(pearsonr(ts)).float().cuda() for ts in train_rest_ts]\n",
    "test_rest_p_torch = [torch.from_numpy(pearsonr(ts)).float().cuda() for ts in test_rest_ts]\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705d8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 \n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Make Chebyshev approximations to convolutional filters\n",
    "\n",
    "# num = 0\n",
    "# print('Completed... ', end='')\n",
    "# train_rest_cheb_torch = [AllChebyshev(Laplacian(threshold(pearsonr(ts), 0.2))) for ts in train_rest_ts]\n",
    "# print()\n",
    "\n",
    "num = 0\n",
    "print('Completed... ', end='')\n",
    "test_rest_cheb_torch = [AllChebyshev(Laplacian(threshold(pearsonr(ts), 0.2))) for ts in test_rest_ts]\n",
    "print()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf010ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AllChebyshev', 'Chebyshev', 'F', 'In', 'Laplacian', 'N', 'Out', 'ST_DAG_Att', 'ST_graph_conv', 'X', 'Xc', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_i2', '_i20', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', '_i27', '_i28', '_i29', '_i3', '_i30', '_i31', '_i32', '_i33', '_i34', '_i35', '_i36', '_i37', '_i4', '_i5', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh', 'a', 'badIDs', 'cuda', 'd', 'degree', 'exit', 'f', 'gc', 'get_ipython', 'i', 'loadTimeseries', 'nc', 'nn', 'normalizeSubjects', 'np', 'nroi', 'nrred', 'nt', 'num', 'p', 'pearsonr', 'pickle', 'plt', 'pred', 'quit', 'r', 'scipy', 'st_dag_att', 'subj', 't', 'test', 'testDirty', 'test_rest_cheb_torch', 'test_rest_ts', 'test_rest_ts_torch', 'threshold', 'torch', 'train', 'trainDirty', 'train_rest_ts', 'y', 'y0', 'y1', 'y2', 'y_test', 'y_train']\n",
      "cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.device' object has no attribute 'empty_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-b214c613bd1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.device' object has no attribute 'empty_cache'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0) \n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "print('Total: {:f} Reserved: {:f} Allocated: {:f} Free: {:f}'.format(t,r,a,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed0607a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_rest_ts_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-37731be635b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# LeakyReLU used in all layers with leak rate=0.33\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_rest_ts_torch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mnroi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_rest_ts_torch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_rest_ts_torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Try ST-graph-conv first\n",
    "\n",
    "# 8 filters with kernel size 6 in temporal dimension\n",
    "# 8 spectral filters with Chebyshev polynomials of order 4\n",
    "# temporal and spatial pooling with stride 2\n",
    "# 3 output layers with 256, 256, and 1 hidden node; 2 dropout layers with rate=0.2\n",
    "# LeakyReLU used in all layers with leak rate=0.33\n",
    "\n",
    "nt = train_rest_ts_torch[0].shape[1]\n",
    "nroi = train_rest_ts_torch[0].shape[0]\n",
    "\n",
    "class ST_graph_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ST_graph_conv, self).__init__()\n",
    "        self.tc1 = nn.Conv2d(1,8,(1,6),(1,2)).cuda() # Output size A=f((nt-5)/2)\n",
    "        self.tc2 = nn.Conv2d(8,8,(1,6),(1,2)).cuda() # Output size f((A-5)/2)\n",
    "        self.sc1 = nn.Parameter(torch.zeros(4,8,8, requires_grad=True).cuda())\n",
    "        self.sc2 = nn.Parameter(torch.zeros(4,8,8, requires_grad=True).cuda())\n",
    "        self.sc1b = nn.Parameter(torch.zeros(8, requires_grad=True).cuda())\n",
    "        self.sc2b = nn.Parameter(torch.zeros(8, requires_grad=True).cuda())\n",
    "        self.sac1 = nn.Linear(nroi*2*8,10).cuda()\n",
    "        self.sac2 = nn.Linear(nroi*2*8,10).cuda()\n",
    "        self.fc1 = nn.Linear(20,256).cuda()\n",
    "        self.fc2 = nn.Linear(256,256).cuda()\n",
    "        self.fc3 = nn.Linear(256,1).cuda()\n",
    "        self.lrelu = nn.LeakyReLU(0.33)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ts = x[0]\n",
    "        tk = x[1] # nroi x nroi x 4\n",
    "        nb = ts.shape[0]\n",
    "        # First temporal convolution\n",
    "        x = self.lrelu(self.tc1(ts))\n",
    "        # First spatial convolution\n",
    "        x = torch.einsum('abcd,aecf->abcf', x, tk)\n",
    "        x = torch.einsum('abcf,dbg->agcf', x, self.sc1)\n",
    "        x = x + self.sc1b.view(8,1,1)\n",
    "        # First aggregate\n",
    "        ymean = torch.mean(x,dim=3)\n",
    "        ystd = torch.std(x,dim=3)\n",
    "        y = torch.cat([ymean.view(nb,nroi*8), ystd.view(nb,nroi*8)],dim=1)\n",
    "        y = self.lrelu(self.sac1(y))\n",
    "        # Second temporal convolution\n",
    "        x = self.lrelu(self.tc2(x))\n",
    "        # Second spatial convolution\n",
    "        x = torch.einsum('abcd,aecf->abcf', x, tk)\n",
    "        x = torch.einsum('abcf,dbg->agcf', x, self.sc2)\n",
    "        x = x + self.sc2b.view(8,1,1)\n",
    "        # Second aggregate\n",
    "        zmean = torch.mean(x,dim=3)\n",
    "        zstd = torch.std(x,dim=3)\n",
    "        z = torch.cat([zmean.view(nb,nroi*8), zstd.view(nb,nroi*8)],dim=1)\n",
    "        z = self.lrelu(self.sac2(z))\n",
    "        # Output\n",
    "        x = torch.cat([y,z], dim=1)\n",
    "        x = self.dp(self.lrelu(self.fc1(x)))\n",
    "        x = self.dp(self.lrelu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "st_graph_conv = ST_graph_conv()\n",
    "optim = torch.optim.Adam(st_graph_conv.parameters(), lr=1e-3)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12149237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Now try FC-conv network\n",
    "\n",
    "nroi = train_rest_p_torch[0].shape[0]\n",
    "nc = 8\n",
    "nrred = int(nroi/4)\n",
    "\n",
    "class FC_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_conv, self).__init__()\n",
    "        self.ec1 = nn.Conv2d(1,nc,(1,nroi)).cuda()\n",
    "        self.nc1 = nn.Conv2d(nc,nc,(nroi,1)).cuda()\n",
    "        self.sat1 = nn.Linear(nroi,nrred).cuda()\n",
    "        self.sat2 = nn.Linear(nrred,nroi).cuda()\n",
    "        self.fc1 = nn.Linear(nc,256).cuda()\n",
    "        self.fc2 = nn.Linear(256,256).cuda()\n",
    "        self.fc3 = nn.Linear(256,1).cuda()\n",
    "        self.lrelu = nn.LeakyReLU(0.33)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        nb = x.shape[0]\n",
    "        x = self.lrelu(self.ec1(x))\n",
    "        y = torch.mean(x, 1)\n",
    "        y = y.reshape([nb,nroi])\n",
    "        y = self.lrelu(self.sat1(y))\n",
    "        y = torch.sigmoid(self.sat2(y))\n",
    "        x = torch.einsum('abcd,ac->abcd',x,y)\n",
    "        x = self.lrelu(self.nc1(x))\n",
    "        x = x.reshape([nb,nc])\n",
    "        x = self.dp(self.lrelu(self.fc1(x)))\n",
    "        x = self.dp(self.lrelu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "fc_conv = FC_conv()\n",
    "optim = torch.optim.Adam(fc_conv.parameters(), lr=1e-3)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b64799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Try the complete ST-DAG-Att model\n",
    "# No spatial attention pooling...\n",
    "\n",
    "from math import floor as f\n",
    "\n",
    "nt = test_rest_ts_torch[0].shape[1]\n",
    "nroi = test_rest_ts_torch[0].shape[0]\n",
    "nc = 8\n",
    "nrred = int(nroi/4)\n",
    "\n",
    "class ST_DAG_Att(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ST_DAG_Att, self).__init__()\n",
    "        # First FC conv\n",
    "        self.ec1 = nn.Conv2d(1,nc,(1,nroi)).cuda()\n",
    "        self.nc1 = nn.Conv2d(nc,nc,(nroi,1)).cuda()\n",
    "        self.sat1 = nn.Linear(nroi,nrred).cuda()\n",
    "        self.sat2 = nn.Linear(nrred,nroi).cuda()\n",
    "        # First spatial conv\n",
    "        self.tc1 = nn.Conv2d(1,8,(1,6),(1,2)).cuda() # Output size A=f((nt-5)/2)\n",
    "        self.sc1 = nn.Parameter(torch.zeros(4,8,8, requires_grad=True).cuda())\n",
    "        self.sc1b = nn.Parameter(torch.zeros(8, requires_grad=True).cuda())\n",
    "        # Second FC conv\n",
    "        self.ec2 = nn.Conv2d(nc,nc,(1,nroi)).cuda()\n",
    "        self.nc2 = nn.Conv2d(nc,nc,(nroi,1)).cuda()\n",
    "        self.sat3 = nn.Linear(nroi,nrred).cuda()\n",
    "        self.sat4 = nn.Linear(nrred,nroi).cuda()\n",
    "        # First ST aggregation\n",
    "        self.sac1 = nn.Linear(nroi*2*8,10).cuda()\n",
    "        # Second spatial conv\n",
    "        self.tc2 = nn.Conv2d(8,8,(1,6),(1,2)).cuda() # Output size A=f((nt-5)/2)\n",
    "        self.sc2 = nn.Parameter(torch.zeros(4,8,8, requires_grad=True).cuda())\n",
    "        self.sc2b = nn.Parameter(torch.zeros(8, requires_grad=True).cuda())\n",
    "        # Third FC conv\n",
    "        self.ec3 = nn.Conv2d(nc,nc,(1,nroi)).cuda()\n",
    "        self.nc3 = nn.Conv2d(nc,nc,(nroi,1)).cuda()\n",
    "        self.sat5 = nn.Linear(nroi,nrred).cuda()\n",
    "        self.sat6 = nn.Linear(nrred,nroi).cuda()\n",
    "        # Second ST aggregation\n",
    "        self.sac2 = nn.Linear(nroi*2*8,10).cuda()\n",
    "        # Output\n",
    "        self.fc1 = nn.Linear(nc*3+2*10,256).cuda()\n",
    "        self.fc2 = nn.Linear(256,256).cuda()\n",
    "        self.fc3 = nn.Linear(256,1).cuda()\n",
    "        self.lrelu = nn.LeakyReLU(0.33)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for ts, tk in x:\n",
    "            # First FC conv\n",
    "            p = pearsonr(ts)\n",
    "            ts = ts.unsqueeze(0).unsqueeze(0)\n",
    "            z = self.lrelu(self.ec1(p.unsqueeze(0).unsqueeze(0)))\n",
    "            y = torch.mean(z, 1)\n",
    "            y = y.reshape([nroi])\n",
    "            y = self.lrelu(self.sat1(y))\n",
    "            y = torch.sigmoid(self.sat2(y))\n",
    "            # Attention\n",
    "            ts = torch.einsum('abcd,c->abcd',ts,y)\n",
    "            z = torch.einsum('abcd,c->abcd',z,y)\n",
    "            ft1 = self.lrelu(self.nc1(z)).reshape([nc])\n",
    "            # First temporal conv\n",
    "            ts = self.lrelu(self.tc1(ts))\n",
    "            # First spatial convolution\n",
    "            ts = torch.einsum('abcd,ecf->abcf', ts, tk)\n",
    "            ts = torch.einsum('abcf,dbg->agcf', ts, self.sc1)\n",
    "            ts = ts + self.sc1b.view(8,1,1)\n",
    "            # Second FC conv\n",
    "            p = []\n",
    "            for i in range(ts.shape[1]):\n",
    "                p.append(pearsonr(ts[0,i,:,:]))\n",
    "            p = torch.stack(p).unsqueeze(0)\n",
    "            z = self.lrelu(self.ec2(p))\n",
    "            y = torch.mean(z, 1)\n",
    "            y = y.reshape([nroi])\n",
    "            y = self.lrelu(self.sat3(y))\n",
    "            y = torch.sigmoid(self.sat4(y))\n",
    "            # Attention\n",
    "            ts = torch.einsum('abcd,c->abcd',ts,y)\n",
    "            z = torch.einsum('abcd,c->abcd',z,y)\n",
    "            ft2 = self.lrelu(self.nc1(z)).reshape([nc])\n",
    "            # First ST aggregation\n",
    "            tsmean = torch.mean(ts,dim=2)\n",
    "            tsstd = torch.std(ts,dim=2)\n",
    "            ft3 = torch.cat([tsmean.view(nroi*8), tsstd.view(nroi*8)])\n",
    "            ft3 = self.lrelu(self.sac1(ft3))\n",
    "            # Second temporal conv\n",
    "            ts = self.lrelu(self.tc2(ts))\n",
    "            # Second spatial convolution\n",
    "            ts = torch.einsum('abcd,ecf->abcf', ts, tk)\n",
    "            ts = torch.einsum('abcf,dbg->agcf', ts, self.sc2)\n",
    "            ts = ts + self.sc2b.view(8,1,1)\n",
    "            # Third FC conv\n",
    "            p = []\n",
    "            for i in range(ts.shape[1]):\n",
    "                p.append(pearsonr(ts[0,i,:,:]))\n",
    "            p = torch.stack(p).unsqueeze(0)\n",
    "            z = self.lrelu(self.ec3(p))\n",
    "            y = torch.mean(z, 1)\n",
    "            y = y.reshape([nroi])\n",
    "            y = self.lrelu(self.sat5(y))\n",
    "            y = torch.sigmoid(self.sat6(y))\n",
    "            # Attention\n",
    "            ts = torch.einsum('abcd,c->abcd',ts,y)\n",
    "            z = torch.einsum('abcd,c->abcd',z,y)\n",
    "            ft4 = self.lrelu(self.nc1(z)).reshape([nc])\n",
    "            # Second ST aggregation\n",
    "            tsmean = torch.mean(ts,dim=2)\n",
    "            tsstd = torch.std(ts,dim=2)\n",
    "            ft5 = torch.cat([tsmean.view(nroi*8), tsstd.view(nroi*8)])\n",
    "            ft5 = self.lrelu(self.sac2(ft5))\n",
    "            # Output\n",
    "            x = torch.cat([ft1,ft2,ft3,ft4,ft5])\n",
    "            x = self.dp(self.lrelu(self.fc1(x)))\n",
    "            x = self.dp(self.lrelu(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "            res.append(x)\n",
    "        return torch.cat(res)\n",
    "    \n",
    "st_dag_att = ST_DAG_Att()\n",
    "# optim = torch.optim.Adam(st_dag_att.parameters(), lr=1e-3)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2e4cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Save/load model\n",
    "\n",
    "# torch.save(st_graph_conv.state_dict(), \"ST_graph_conv1.model\")\n",
    "\n",
    "# st_graph_conv = ST_graph_conv()\n",
    "# st_graph_conv.load_state_dict(torch.load(\"ST_graph_conv1.model\"))\n",
    "\n",
    "# torch.save(st_dag_att.state_dict(), \"ST_DAG_Att.model\")\n",
    "\n",
    "st_dag_att = ST_DAG_Att()\n",
    "st_dag_att.load_state_dict(torch.load(\"ST_DAG_Att.model\"))\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f81a806",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=353635.687500\n",
      "epoch 10 loss=326580.406250\n",
      "epoch 20 loss=142852.656250\n",
      "epoch 30 loss=35637.359375\n",
      "epoch 40 loss=37562.968750\n",
      "epoch 50 loss=19902.718750\n",
      "epoch 60 loss=20463.449219\n",
      "epoch 70 loss=16544.687500\n",
      "epoch 80 loss=19823.103516\n",
      "epoch 90 loss=19732.570312\n",
      "epoch 100 loss=14954.939453\n",
      "epoch 110 loss=17670.988281\n",
      "epoch 120 loss=19532.609375\n",
      "epoch 130 loss=17290.931641\n",
      "epoch 140 loss=14663.348633\n",
      "epoch 150 loss=18954.097656\n",
      "epoch 160 loss=14648.523438\n",
      "epoch 170 loss=15829.254883\n",
      "epoch 180 loss=12940.171875\n",
      "epoch 190 loss=14435.650391\n",
      "epoch 200 loss=10665.939453\n",
      "epoch 210 loss=11795.488281\n",
      "epoch 220 loss=15875.127930\n",
      "epoch 230 loss=18481.894531\n",
      "epoch 240 loss=14796.125000\n",
      "epoch 250 loss=11469.555664\n",
      "epoch 260 loss=10808.142578\n",
      "epoch 270 loss=14470.950195\n",
      "epoch 280 loss=11426.467773\n",
      "epoch 290 loss=9563.355469\n",
      "epoch 300 loss=12772.384766\n",
      "epoch 310 loss=11627.162109\n",
      "epoch 320 loss=8450.625000\n",
      "epoch 330 loss=9017.936523\n",
      "epoch 340 loss=10511.111328\n",
      "epoch 350 loss=8804.508789\n",
      "epoch 360 loss=8832.225586\n",
      "epoch 370 loss=10531.825195\n",
      "epoch 380 loss=7486.900879\n",
      "epoch 390 loss=6696.096191\n",
      "epoch 400 loss=7692.820312\n",
      "epoch 410 loss=10725.136719\n",
      "epoch 420 loss=6595.887695\n",
      "epoch 430 loss=5396.368164\n",
      "epoch 440 loss=5934.202637\n",
      "epoch 450 loss=5955.926758\n",
      "epoch 460 loss=5498.791504\n",
      "epoch 470 loss=6015.377441\n",
      "epoch 480 loss=8080.204590\n",
      "epoch 490 loss=5559.011230\n",
      "epoch 500 loss=3612.696045\n",
      "epoch 510 loss=3284.059082\n",
      "epoch 520 loss=3807.904785\n",
      "epoch 530 loss=4301.310547\n",
      "epoch 540 loss=3735.003174\n",
      "epoch 550 loss=3478.633301\n",
      "epoch 560 loss=5433.386230\n",
      "epoch 570 loss=4649.567383\n",
      "epoch 580 loss=3098.130859\n",
      "epoch 590 loss=2466.509033\n",
      "epoch 600 loss=3585.542480\n",
      "epoch 610 loss=3931.667969\n",
      "epoch 620 loss=4489.108398\n",
      "epoch 630 loss=3818.496094\n",
      "epoch 640 loss=4627.412109\n",
      "epoch 650 loss=3903.788330\n",
      "epoch 660 loss=3174.378418\n",
      "epoch 670 loss=3190.957764\n",
      "epoch 680 loss=2533.902832\n",
      "epoch 690 loss=3803.859375\n",
      "epoch 700 loss=2861.234375\n",
      "epoch 710 loss=3484.963623\n",
      "epoch 720 loss=3668.858643\n",
      "epoch 730 loss=3818.805908\n",
      "epoch 740 loss=2286.097168\n",
      "epoch 750 loss=3265.255615\n",
      "epoch 760 loss=1737.492798\n",
      "epoch 770 loss=2287.363770\n",
      "epoch 780 loss=2655.775879\n",
      "epoch 790 loss=2430.668457\n",
      "epoch 800 loss=2425.481689\n",
      "epoch 810 loss=2201.301270\n",
      "epoch 820 loss=3096.582031\n",
      "epoch 830 loss=2453.073486\n",
      "epoch 840 loss=2257.151855\n",
      "epoch 850 loss=1907.149048\n",
      "epoch 860 loss=2494.348389\n",
      "epoch 870 loss=1917.892944\n",
      "epoch 880 loss=2118.837402\n",
      "epoch 890 loss=2242.755371\n",
      "epoch 900 loss=2607.420410\n",
      "epoch 910 loss=2625.087891\n",
      "epoch 920 loss=1850.843994\n",
      "epoch 930 loss=2451.103271\n",
      "epoch 940 loss=3096.384277\n",
      "epoch 950 loss=2936.425781\n",
      "epoch 960 loss=2041.635742\n",
      "epoch 970 loss=1635.076782\n",
      "epoch 980 loss=2063.835449\n",
      "epoch 990 loss=3209.314453\n",
      "epoch 1000 loss=3038.746094\n",
      "epoch 1010 loss=2428.519287\n",
      "epoch 1020 loss=2665.218262\n",
      "epoch 1030 loss=1910.805054\n",
      "epoch 1040 loss=2483.015137\n",
      "epoch 1050 loss=2815.798828\n",
      "epoch 1060 loss=2171.253906\n",
      "epoch 1070 loss=2123.556396\n",
      "epoch 1080 loss=2224.170166\n",
      "epoch 1090 loss=2282.184814\n",
      "epoch 1100 loss=1613.872314\n",
      "epoch 1110 loss=2319.509033\n",
      "epoch 1120 loss=2270.963379\n",
      "epoch 1130 loss=1721.359375\n",
      "epoch 1140 loss=2102.945068\n",
      "epoch 1150 loss=1805.039062\n",
      "epoch 1160 loss=1842.116211\n",
      "epoch 1170 loss=1543.792969\n",
      "epoch 1180 loss=2163.498047\n",
      "epoch 1190 loss=1635.031860\n",
      "epoch 1200 loss=1897.810791\n",
      "epoch 1210 loss=2090.699707\n",
      "epoch 1220 loss=1921.359131\n",
      "epoch 1230 loss=1875.684204\n",
      "epoch 1240 loss=2367.490723\n",
      "epoch 1250 loss=1235.371460\n",
      "epoch 1260 loss=1657.896240\n",
      "epoch 1270 loss=1466.525757\n",
      "epoch 1280 loss=1620.573242\n",
      "epoch 1290 loss=1286.765381\n",
      "epoch 1300 loss=1321.569092\n",
      "epoch 1310 loss=2522.520020\n",
      "epoch 1320 loss=3733.440186\n",
      "epoch 1330 loss=2085.520020\n",
      "epoch 1340 loss=2133.092773\n",
      "epoch 1350 loss=2063.182373\n",
      "epoch 1360 loss=2669.445312\n",
      "epoch 1370 loss=2149.664551\n",
      "epoch 1380 loss=2024.495361\n",
      "epoch 1390 loss=1544.760010\n",
      "epoch 1400 loss=2846.435547\n",
      "epoch 1410 loss=2756.437988\n",
      "epoch 1420 loss=2568.676758\n",
      "epoch 1430 loss=1915.416992\n",
      "epoch 1440 loss=1543.572754\n",
      "epoch 1450 loss=1934.421509\n",
      "epoch 1460 loss=1579.914673\n",
      "epoch 1470 loss=1479.507446\n",
      "epoch 1480 loss=1349.184082\n",
      "epoch 1490 loss=1512.176025\n",
      "epoch 1500 loss=1493.422363\n",
      "epoch 1510 loss=1803.608032\n",
      "epoch 1520 loss=2107.612549\n",
      "epoch 1530 loss=1900.378174\n",
      "epoch 1540 loss=4026.385254\n",
      "epoch 1550 loss=2246.023926\n",
      "epoch 1560 loss=2854.135254\n",
      "epoch 1570 loss=2032.106689\n",
      "epoch 1580 loss=2045.333984\n",
      "epoch 1590 loss=1735.831055\n",
      "epoch 1600 loss=2289.367676\n",
      "epoch 1610 loss=2133.443848\n",
      "epoch 1620 loss=1450.271484\n",
      "epoch 1630 loss=1343.226929\n",
      "epoch 1640 loss=1921.799194\n",
      "epoch 1650 loss=2237.098633\n",
      "epoch 1660 loss=2140.585205\n",
      "epoch 1670 loss=2060.143066\n",
      "epoch 1680 loss=1513.931763\n",
      "epoch 1690 loss=1872.599365\n",
      "epoch 1700 loss=1507.970825\n",
      "epoch 1710 loss=1163.116455\n",
      "epoch 1720 loss=1901.995483\n",
      "epoch 1730 loss=1395.691162\n",
      "epoch 1740 loss=1862.798584\n",
      "epoch 1750 loss=1491.436401\n",
      "epoch 1760 loss=1629.475220\n",
      "epoch 1770 loss=1507.844727\n",
      "epoch 1780 loss=981.230103\n",
      "epoch 1790 loss=1375.222046\n",
      "epoch 1800 loss=1061.855225\n",
      "epoch 1810 loss=1590.439209\n",
      "epoch 1820 loss=1565.865234\n",
      "epoch 1830 loss=1092.407471\n",
      "epoch 1840 loss=1888.209229\n",
      "epoch 1850 loss=1601.610596\n",
      "epoch 1860 loss=1813.796143\n",
      "epoch 1870 loss=1220.515625\n",
      "epoch 1880 loss=1459.851440\n",
      "epoch 1890 loss=1587.336548\n",
      "epoch 1900 loss=1119.575073\n",
      "epoch 1910 loss=1040.632568\n",
      "epoch 1920 loss=1369.278931\n",
      "epoch 1930 loss=1212.518555\n",
      "epoch 1940 loss=1027.392456\n",
      "epoch 1950 loss=1063.965820\n",
      "epoch 1960 loss=1628.383423\n",
      "epoch 1970 loss=1313.641235\n",
      "epoch 1980 loss=1334.802734\n",
      "epoch 1990 loss=1585.954590\n",
      "epoch 2000 loss=1543.511841\n",
      "epoch 2010 loss=908.504211\n",
      "epoch 2020 loss=1753.819946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6355edfd75dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mrunning\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpPeriod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnEpoch\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N = len(train_rest_ts_torch)\n",
    "# N = len(train_rest_p_torch)\n",
    "running = 0\n",
    "nEpoch = 5000\n",
    "pPeriod = 10\n",
    "nBatch = 10\n",
    "\n",
    "for epoch in range(nEpoch):\n",
    "    batch = []\n",
    "    batchCheb = []\n",
    "    truth = torch.zeros(nBatch)\n",
    "    for i in range(nBatch):\n",
    "        idx = random.randint(0,N-1)\n",
    "#         subj = train_rest_p_torch[idx]\n",
    "        subj = train_rest_ts_torch[idx]\n",
    "        cheb = train_rest_cheb_torch[idx]\n",
    "#         batch.append(subj.unsqueeze(0))\n",
    "#         batchCheb.append(cheb.unsqueeze(0))\n",
    "        batch.append([subj, cheb])\n",
    "        truth[i] = y_train[idx,0]\n",
    "    optim.zero_grad()\n",
    "#     pred = st_graph_conv([torch.cat(batch), torch.cat(batchCheb)]).view(nBatch)\n",
    "#     pred = fc_conv(torch.cat(batch)).view(nBatch)\n",
    "    pred = st_dag_att(batch).view(nBatch)\n",
    "    loss = torch.sum((truth.cuda()-pred)**2)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    running += loss.cpu()\n",
    "    if epoch % pPeriod == 0 or epoch == nEpoch-1:\n",
    "        if epoch != 0:\n",
    "            running = running/pPeriod\n",
    "        print('epoch {:d} loss={:f}'.format(epoch, running))\n",
    "        running = 0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb57e297",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 18.19 MiB free; 2.88 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7f840a6013d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#     optim.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst_dag_att\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;31m#     p = fc_conv(X[i].unsqueeze(0).unsqueeze(0))[0,0].cpu().detach().numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#     if p < 0 or p > 350:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aorlichenko\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-9df62a093861>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mec3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 18.19 MiB free; 2.88 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = y_test\n",
    "X = test_rest_ts_torch\n",
    "Xc = test_rest_cheb_torch\n",
    "N = len(y)\n",
    "\n",
    "# pred = np.zeros(N)\n",
    "# y1 = np.zeros(N) # rmse\n",
    "# y2 = np.zeros(N) # mae\n",
    "\n",
    "pred = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y0 = []\n",
    "\n",
    "for i in range(N):\n",
    "    try:\n",
    "        p = st_dag_att([[X[i], Xc[i]]])\n",
    "    except Exception as e:\n",
    "        print('Got {:d} before failure'.format(i+1))\n",
    "#     p = fc_conv(X[i].unsqueeze(0).unsqueeze(0))[0,0].cpu().detach().numpy()\n",
    "#     if p < 0 or p > 350:\n",
    "#         continue\n",
    "    y0.append(y[i,0])\n",
    "    pred.append(p)\n",
    "    y1.append((y[i,0]-p)**2)\n",
    "    y2.append(abs(y[i,0]-p))\n",
    "    \n",
    "y0 = np.array(y0)\n",
    "pred = np.array(pred)\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "\n",
    "# batch = []\n",
    "# batchCheb = []\n",
    "# for i in range(N):\n",
    "#     batch.append(X[i].unsqueeze(0).unsqueeze(0))\n",
    "#     batchCheb.append(Xc[i].unsqueeze(0))\n",
    "\n",
    "# batch = torch.cat(batch)\n",
    "# batchCheb = torch.cat(batchCheb)\n",
    "# pred = st_graph_conv([batch, batchCheb]).view(N).cpu().detach().numpy()\n",
    "# y1 = (y.reshape(N)-pred)**2 # rmse\n",
    "# y2 = abs(y.reshape(N)-pred) # mae\n",
    "    \n",
    "print(np.corrcoef(y0.flatten(), pred))\n",
    "print((y1.sum()/len(y1))**0.5/12)\n",
    "print(y2.sum()/len(y2)/12)\n",
    "# print(y1)\n",
    "# print(y2)\n",
    "\n",
    "idcs = np.argsort(y0,axis=0)\n",
    "plt.plot(y0[idcs], label='truth')\n",
    "plt.plot(pred[idcs], label='prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y0.flatten(), pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380800f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
