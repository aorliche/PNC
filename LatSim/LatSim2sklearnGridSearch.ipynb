{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3ed1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'wrat', 'missingage', 'missingsex', 'missingwrat', 'failedqc']\n",
      "['emoid', 'nback', 'rest']\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Using newly preprocessed subjects\n",
    "\n",
    "import pickle\n",
    "\n",
    "metadictname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_agesexwrat.pkl'\n",
    "alltsname = '/home/anton/Documents/Tulane/Research/PNC_Good/PNC_PowerTS_float2.pkl'\n",
    "\n",
    "with open(metadictname, 'rb') as f:\n",
    "    metadict = pickle.load(f)\n",
    "\n",
    "with open(alltsname, 'rb') as f:\n",
    "    allts = pickle.load(f)\n",
    "    \n",
    "print(list(metadict.keys()))\n",
    "print(list(allts.keys()))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86de9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n",
      "(830, 264, 124)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get subjects that have all tasks and paras specified\n",
    "Functions for creating independent and response variables\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "def get_subs(allts, metadict, tasks, paras):\n",
    "    # Get subs for all paras\n",
    "    for i,para in enumerate(paras):\n",
    "        tmpset = set([int(sub[4:]) for sub in allts[para].keys()])\n",
    "        if i == 0:\n",
    "            paraset = tmpset\n",
    "        else:\n",
    "            paraset = paraset.intersection(tmpset)\n",
    "    # Get subs for all tasks\n",
    "    for i,task in enumerate(tasks):\n",
    "        tmpset = set([sub for sub in metadict[task].keys()])\n",
    "        if i == 0:\n",
    "            taskset = tmpset\n",
    "        else:\n",
    "            taskset = paraset.intersection(tmpset)\n",
    "    # Remove QC failures\n",
    "    allsubs = taskset.intersection(paraset)\n",
    "    for badsub in metadict['failedqc']:\n",
    "        try:\n",
    "            allsubs.remove(int(badsub[4:]))\n",
    "        except:\n",
    "            pass\n",
    "    return natsorted(list(allsubs))\n",
    "\n",
    "def get_X(allts, paras, subs):\n",
    "    X = []\n",
    "    for para in paras:\n",
    "        pX = [allts[para][f'sub-{sub}'] for sub in subs]\n",
    "        pX = np.stack(pX)\n",
    "        X.append(pX)\n",
    "    return X\n",
    "\n",
    "def get_y(metadict, tasks, subs):\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        if task == 'age' or task == 'wrat':\n",
    "            var = [metadict[task][sub] for sub in subs]\n",
    "            var = np.array(var)\n",
    "            y.append(var)\n",
    "        if task == 'sex':\n",
    "            maleness = [metadict[task][sub] == 'M' for sub in subs]\n",
    "            maleness = np.array(maleness)\n",
    "            sex = np.stack([maleness, 1-maleness], axis=1)\n",
    "            y.append(sex)\n",
    "    return y\n",
    "\n",
    "subs = get_subs(allts, metadict, ['wrat'], ['rest', 'nback', 'emoid'])\n",
    "print(len(subs))\n",
    "\n",
    "X = get_X(allts, ['rest', 'nback', 'emoid'], subs)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3d5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 34716)\n"
     ]
    }
   ],
   "source": [
    "# TS to condensed FC\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_bandpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "tr = 3\n",
    "\n",
    "def filter_design_ts(X):\n",
    "    Xs = []\n",
    "    for i in range(X.shape[0]):\n",
    "        nX = butter_bandpass_filter(X[i], [0.01, 0.15], 1/tr)\n",
    "        Xs.append(nX)\n",
    "    return np.stack(Xs)\n",
    "\n",
    "def ts_to_flat_fc(X):\n",
    "    p = np.corrcoef(X)\n",
    "    a,b = np.triu_indices(p[0].shape[0], 1)\n",
    "    p = p[a,b]\n",
    "    return p\n",
    "\n",
    "p = [np.stack([ts_to_flat_fc(ts) for ts in filter_design_ts(Xp)]) for Xp in X]\n",
    "# Xfiltnorm = [tsmod/np.linalg.norm(tsmod, axis=(-1), keepdims=True) for tsmod in ts]\n",
    "print(p[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f325756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done partials\n",
      "Done partials\n",
      "Done partials\n",
      "(3, 830, 34716)\n"
     ]
    }
   ],
   "source": [
    "# Get all partial correlations\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "cm = ConnectivityMeasure(kind='partial correlation')\n",
    "\n",
    "a,b = np.triu_indices(264, 1)\n",
    "\n",
    "allp = []\n",
    "for taskidx in range(3):\n",
    "    Xf = filter_design_ts(X[taskidx])\n",
    "    partials = cm.fit_transform(Xf.transpose(0,2,1))\n",
    "    partials = partials[:,a,b]\n",
    "    allp.append(partials)\n",
    "    print('Done partials')\n",
    "\n",
    "allp = np.stack(allp)\n",
    "print(allp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1d0b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import sys\n",
    "\n",
    "if '../../LatentSimilarity' not in sys.path:\n",
    "    sys.path.append('../../LatentSimilarity/')\n",
    "\n",
    "from latsim import LatSim, train_sim_mse, train_sim_ce\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def to_torch(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return torch.from_numpy(x).float().cuda()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "'''\n",
    "One class for regression, one (sub)class for classification\n",
    "'''\n",
    "class LatSimReg(BaseEstimator):\n",
    "    def __init__(self, **params):\n",
    "        self.set_params(**params)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_params():\n",
    "        return dict(ld=2, stop=1, lr=1e-4, nepochs=100)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_distributions():\n",
    "        return dict(\n",
    "            ld=[1,2,10],\n",
    "            stop=[0,1,10*10,100*100],\n",
    "            lr=[1e-5,1e-4,1e-3],\n",
    "            nepochs=[100,1000,2000],\n",
    "        )\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return dict(ld=self.ld, stop=self.stop, lr=self.lr, nepochs=self.nepochs)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        dft = LatSimReg.get_default_params()\n",
    "        for key in dft:\n",
    "            if key in params:\n",
    "                setattr(self, key, params[key])\n",
    "            else:\n",
    "                setattr(self, key, dft[key])\n",
    "        return self\n",
    "\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "        self.x = 1*x\n",
    "        self.y = 1*y\n",
    "        params = LatSimReg.get_default_params()\n",
    "        for arg in kwargs:\n",
    "            if arg in params:\n",
    "                params[arg] = kwargs[arg]\n",
    "        self.sim = LatSim(x.shape[1], params['ld'])\n",
    "        del params['ld']\n",
    "        train_sim_mse(self.sim, self.x, self.y, **params)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = torch.from_numpy(x).float().cuda()\n",
    "        with torch.no_grad():\n",
    "            yhat = self.sim(self.x, self.y, x)\n",
    "        return yhat.detach().cpu().numpy()\n",
    "    \n",
    "class LatSimClf(LatSimReg):\n",
    "    @staticmethod\n",
    "    def get_default_params():\n",
    "        return dict(ld=2, stop=1, lr=1e-4, nepochs=100)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_distributions():\n",
    "        return dict(\n",
    "            ld=[1,2,10],\n",
    "            stop=[0,0.1,0.2,0.3],\n",
    "            lr=[1e-5,1e-4,1e-3],\n",
    "            nepochs=[100,1000,2000],\n",
    "        )\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        dft = LatSimClf.get_default_params()\n",
    "        for key in dft:\n",
    "            if key in params:\n",
    "                setattr(self, key, params[key])\n",
    "            else:\n",
    "                setattr(self, key, dft[key])\n",
    "        return self\n",
    "    \n",
    "    def fit(self, x, y, **kwargs):\n",
    "        y = to_torch(y).long()\n",
    "        y = F.one_hot(y).float()\n",
    "        return super().fit(x, y, **kwargs)\n",
    "\n",
    "    def predict(self, x):\n",
    "        yhat = super().predict(x)\n",
    "        return np.argmax(yhat, axis=1)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "427cf2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7696969696969697\n",
      "0.7787878787878788\n",
      "0.7727272727272727\n",
      "0.7303030303030303\n",
      "0.7454545454545455\n",
      "0.7636363636363637\n",
      "0.7424242424242424\n",
      "0.7424242424242424\n",
      "0.7393939393939394\n",
      "0.7484848484848485\n",
      "0.7636363636363637\n",
      "0.706060606060606\n",
      "0.7393939393939394\n",
      "0.7727272727272727\n",
      "0.7515151515151515\n",
      "0.7727272727272727\n",
      "0.7090909090909091\n",
      "0.7666666666666667\n",
      "0.7666666666666667\n",
      "0.7606060606060606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7521212121212122"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = []\n",
    "\n",
    "for i in range(20):\n",
    "    idcs = np.random.permutation(830)\n",
    "    ntrain = 500\n",
    "    task ='sex'\n",
    "\n",
    "    x = p[2]\n",
    "    x = x[idcs]\n",
    "    xtr = x[:ntrain]\n",
    "    xt = x[ntrain:]\n",
    "\n",
    "    mux = np.mean(xtr, axis=0, keepdims=True)\n",
    "    sigx = np.std(xtr, axis=0, keepdims=True)\n",
    "    xtr = xtr - mux\n",
    "    xt = xt - mux\n",
    "\n",
    "    y = get_y(metadict, [task], subs)[0]\n",
    "    y = y[idcs]\n",
    "    y = np.argmax(y, axis=1)\n",
    "    ytr = y[:ntrain]\n",
    "    yt = y[ntrain:]\n",
    "\n",
    "#     mu = np.mean(ytr)\n",
    "#     ytr = ytr - mu\n",
    "#     yt = yt - mu\n",
    "\n",
    "    reg = LatSimClf().fit(xtr, ytr, ld=2, nepochs=1000, lr=1e-4, stop=0)\n",
    "    yhat = reg.predict(xt)\n",
    "    acc = np.sum(yhat == yt)/len(yt)\n",
    "    acc = float(acc)\n",
    "    print(acc)\n",
    "    ar.append(acc)\n",
    "    # print(np.sum(yhat == yt)/len(yt))\n",
    "#     rmse = np.mean((yhat-yt)**2)**0.5\n",
    "#     rmse = float(rmse)\n",
    "#     print(rmse)\n",
    "#     ar.append(rmse)\n",
    "\n",
    "np.mean(np.array(ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0ae6c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop': 0.1, 'nepochs': 1000, 'lr': 0.0001, 'ld': 10}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for WRAT\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "x = p[2]\n",
    "y = get_y(metadict, ['sex'], subs)[0]\n",
    "\n",
    "x = x - np.mean(x, axis=0, keepdims=True)\n",
    "y = np.argmax(y, axis=1)\n",
    "# y = y - np.mean(y)\n",
    "\n",
    "reg = LatSimClf()\n",
    "params = LatSimClf.get_default_distributions()\n",
    "n_iter = 100\n",
    "reg = RandomizedSearchCV(reg, params, scoring='accuracy')\n",
    "search = reg.fit(x, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3339b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
