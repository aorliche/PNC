{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74905155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load meta dict\n",
    "\n",
    "with open('../../PNC/AllSubjectsMeta.bin', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "# Load rest subject ids and splits\n",
    "\n",
    "with open('../../Work/Abstract/PaperBin/AllThreeSplit.bin', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "    subids = splits['allThreeYesWrat']\n",
    "    groups = splits['groups']\n",
    "    \n",
    "print(len(subids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240d0f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "subidsNp = np.array(subids)\n",
    "\n",
    "# Load timeseries\n",
    "\n",
    "def loadSeries(prefix, para, idx):\n",
    "    with open('{:}/{:}_fmri_power264/timeseries/{:}.bin'.format(prefix, para, idx), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "rest_ts = [loadSeries('../../PNC', 'rest', meta[subid]['rest']) for subid in subidsNp]\n",
    "nback_ts = [loadSeries('../../PNC', 'nback', meta[subid]['nback']) for subid in subidsNp]\n",
    "emoid_ts = [loadSeries('../../PNC', 'emoid', meta[subid]['emoid']) for subid in subidsNp]\n",
    "\n",
    "print('Loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c764a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalizeSubjects(subjects):\n",
    "    for i in range(len(subjects)):\n",
    "        subj = subjects[i]\n",
    "        subj -= np.mean(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        subj /= np.std(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        if np.sum(np.isnan(subj)) > 0:\n",
    "            print(i)\n",
    "        if np.sum(np.isinf(subj)) > 0:\n",
    "            print(i)\n",
    "\n",
    "normalizeSubjects(rest_ts)\n",
    "normalizeSubjects(nback_ts)\n",
    "normalizeSubjects(emoid_ts)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3077143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 264, 264)\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Calculate pearson matrices\n",
    "\n",
    "rest_p = np.stack([np.corrcoef(sub) for sub in rest_ts])\n",
    "nback_p = np.stack([np.corrcoef(sub) for sub in nback_ts])\n",
    "emoid_p = np.stack([np.corrcoef(sub) for sub in emoid_ts])\n",
    "\n",
    "print(rest_p.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa0df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 322\n",
      "[[223   1   0]\n",
      " [190   0   1]\n",
      " [197   0   1]\n",
      " [145   1   0]\n",
      " [148   0   1]\n",
      " [142   0   1]\n",
      " [123   1   0]\n",
      " [176   1   0]\n",
      " [129   0   1]\n",
      " [173   1   0]]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors (right now just ages, maleness, and femaless)\n",
    "\n",
    "males = 0\n",
    "females = 0\n",
    "\n",
    "X_all = []\n",
    "for subid in subidsNp:\n",
    "    subj = meta[subid]\n",
    "    maleness = 1 if subj['meta']['Gender'] == 'M' else 0\n",
    "    femaleness = 1 if maleness == 0 else 0\n",
    "    feat = np.array([subj['meta']['AgeInMonths'], maleness, femaleness])\n",
    "    X_all.append(feat)\n",
    "    if maleness == 1:\n",
    "        males += 1\n",
    "    if femaleness == 1:\n",
    "        females += 1\n",
    "X_all = np.vstack(X_all)\n",
    "\n",
    "print(f'{males} {females}')\n",
    "print(X_all[10:20])\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5082d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convertTorch(p):\n",
    "    t = torch.from_numpy(p).float()\n",
    "    u = []\n",
    "    for i in range(t.shape[0]):\n",
    "        u.append(t[i][torch.triu_indices(264,264,offset=1).unbind()])\n",
    "    return torch.stack(u).cuda()\n",
    "\n",
    "rest_p_t = convertTorch(rest_p)\n",
    "nback_p_t = convertTorch(nback_p)\n",
    "emoid_p_t = convertTorch(emoid_p)\n",
    "\n",
    "print(rest_p_t.shape)\n",
    "print(nback_p_t.shape)\n",
    "print(emoid_p_t.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae48065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "wratDict = dict()\n",
    "\n",
    "with open('../../PNC/wrat.csv', 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        wratDict[line[0]] = {'raw': line[2], 'std': line[3]}\n",
    "\n",
    "wrat = []\n",
    "\n",
    "for key in subids:\n",
    "    wrat.append(float(wratDict[str(key)]['std']))\n",
    "    \n",
    "wrat = np.array(wrat)\n",
    "wrat_t = torch.from_numpy(wrat).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4964115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def makePoly(ps):\n",
    "    pps = []\n",
    "    for i in range(ps.shape[0]):\n",
    "        p = ps[i].flatten()\n",
    "        pp = nPoly*[None]\n",
    "        for j in range(nPoly):\n",
    "            pp[j] = p**(j+1)\n",
    "        pps.append(torch.stack(pp))\n",
    "    return torch.stack(pps)\n",
    "\n",
    "def arith(n):\n",
    "    return int(n*(n+1)/2)\n",
    "\n",
    "def mask(e):\n",
    "    return e - torch.diag(torch.diag(e))\n",
    "\n",
    "class MiniPgi(nn.Module):\n",
    "    def __init__(self, w, nPara, nPoly, nTgts, dp=0.5, relu=0.1):\n",
    "        super(MiniPgi, self).__init__()\n",
    "        self.masks = []\n",
    "        for i in range(nTgts):\n",
    "            self.masks.append(nn.Parameter(\n",
    "                0.01*torch.ones(nPara,nPoly,arith(263),w).float().cuda()\n",
    "                +0.001*torch.randn(nPara,nPoly,arith(263),w).float().cuda()\n",
    "            ))\n",
    "        self.dp = nn.Dropout(p=dp)\n",
    "        self.relu = []\n",
    "        for i in range(nTgts):\n",
    "            rel = relu if type(relu) == float or type(relu) == int else relu[i]\n",
    "            self.relu.append(nn.LeakyReLU(negative_slope=rel))\n",
    "        \n",
    "    def forward(self, x, age=None, gender=None, wrat=None):\n",
    "        x = self.dp(x)\n",
    "        lbls = [age, gender, wrat]\n",
    "        res = []\n",
    "        es = []\n",
    "        for i,lbl in enumerate(lbls):\n",
    "            y = torch.einsum('abcd,bcde->ae', x, self.masks[i])\n",
    "            e = y@y.T\n",
    "            es.append(e)\n",
    "            idcs = torch.logical_not(torch.any(lbl, dim=1))\n",
    "            e[:,idcs] = 0\n",
    "            e = self.relu[i](mask(e))\n",
    "            s = torch.sum(e, dim=1)\n",
    "            e = e/s.unsqueeze(1)\n",
    "            res.append(e@lbl)\n",
    "        return res, es\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "dd97dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=(34356.73046875, 0.6892601251602173, 10770.7490234375)\n",
      "epoch 100 loss=(3342.408447265625, 14.127725601196289, 860.6365966796875)\n",
      "epoch 200 loss=(2475.087890625, 8.74641227722168, 322.0522766113281)\n",
      "epoch 300 loss=(2220.810546875, 1.575121283531189, 281.592529296875)\n",
      "epoch 400 loss=(1994.09375, 1.1781543493270874, 259.7020263671875)\n",
      "epoch 500 loss=(1812.14794921875, 1.3416664600372314, 251.16204833984375)\n",
      "epoch 600 loss=(1667.5797119140625, 1.3973873853683472, 250.02137756347656)\n",
      "epoch 700 loss=(1555.588623046875, 1.3950055837631226, 251.5443878173828)\n",
      "epoch 800 loss=(1465.3126220703125, 1.3294774293899536, 252.66419982910156)\n",
      "epoch 900 loss=(1397.0595703125, 1.224228024482727, 252.66751098632812)\n",
      "epoch 1000 loss=(1340.791748046875, 1.082519292831421, 251.71006774902344)\n",
      "epoch 1100 loss=(1301.86376953125, 0.9479101896286011, 250.9268341064453)\n",
      "epoch 1200 loss=(1270.6766357421875, 0.8478869795799255, 249.64492797851562)\n",
      "epoch 1300 loss=(1242.7872314453125, 0.7698935866355896, 248.4291229248047)\n",
      "epoch 1400 loss=(1220.2532958984375, 0.7265567779541016, 246.5027618408203)\n",
      "epoch 1500 loss=(1194.8890380859375, 0.7027819752693176, 244.18417358398438)\n",
      "epoch 1600 loss=(1172.902099609375, 0.6978161334991455, 241.61590576171875)\n",
      "epoch 1700 loss=(1153.14306640625, 0.6950280666351318, 239.49574279785156)\n",
      "epoch 1800 loss=(1131.9700927734375, 0.6898051500320435, 237.21397399902344)\n",
      "epoch 1900 loss=(1112.149658203125, 0.6899881362915039, 235.45936584472656)\n",
      "epoch 2000 loss=(1093.2362060546875, 0.6893700957298279, 233.56588745117188)\n",
      "epoch 2100 loss=(1069.5201416015625, 0.693648099899292, 231.72555541992188)\n",
      "epoch 2200 loss=(1045.45947265625, 0.6905786395072937, 229.99684143066406)\n",
      "epoch 2300 loss=(1027.929443359375, 0.693490207195282, 228.16888427734375)\n",
      "epoch 2400 loss=(1003.6456298828125, 0.6983914971351624, 226.50155639648438)\n",
      "epoch 2500 loss=(984.0187377929688, 0.6988011002540588, 224.73040771484375)\n",
      "epoch 2600 loss=(962.39794921875, 0.6991533041000366, 222.61660766601562)\n",
      "epoch 2700 loss=(940.9894409179688, 0.6980139017105103, 221.393798828125)\n",
      "epoch 2800 loss=(918.80078125, 0.7014632225036621, 219.35504150390625)\n",
      "epoch 2900 loss=(893.2684326171875, 0.7034723162651062, 216.54611206054688)\n",
      "epoch 3000 loss=(872.4339599609375, 0.7113349437713623, 214.95384216308594)\n",
      "epoch 3100 loss=(845.2549438476562, 0.7083979249000549, 213.7178955078125)\n",
      "epoch 3200 loss=(820.4927978515625, 0.7130489349365234, 210.87733459472656)\n",
      "epoch 3300 loss=(798.2586059570312, 0.7090743780136108, 209.1826934814453)\n",
      "epoch 3400 loss=(778.9368896484375, 0.7018534541130066, 205.63511657714844)\n",
      "epoch 3500 loss=(749.88671875, 0.6975763440132141, 203.79220581054688)\n",
      "epoch 3600 loss=(726.31884765625, 0.7018253803253174, 199.71971130371094)\n",
      "epoch 3700 loss=(698.921630859375, 0.6933175325393677, 197.90133666992188)\n",
      "epoch 3800 loss=(671.0015258789062, 0.6921055912971497, 195.33677673339844)\n",
      "epoch 3900 loss=(651.3604125976562, 0.6852918267250061, 192.36978149414062)\n",
      "epoch 4000 loss=(631.0543823242188, 0.6866742372512817, 188.75057983398438)\n",
      "epoch 4100 loss=(606.5712890625, 0.6750928163528442, 185.5552520751953)\n",
      "epoch 4200 loss=(583.701904296875, 0.6720271706581116, 182.0377197265625)\n",
      "epoch 4300 loss=(561.4959106445312, 0.6685709953308105, 179.35462951660156)\n",
      "epoch 4400 loss=(543.4317626953125, 0.6654409170150757, 175.97364807128906)\n",
      "epoch 4500 loss=(517.47900390625, 0.6554924845695496, 171.17445373535156)\n",
      "epoch 4600 loss=(494.44512939453125, 0.6462594270706177, 168.04371643066406)\n",
      "epoch 4700 loss=(477.6277160644531, 0.6379508972167969, 163.76634216308594)\n",
      "epoch 4800 loss=(456.0203552246094, 0.63151615858078, 160.4366455078125)\n",
      "epoch 4900 loss=(442.744873046875, 0.626015305519104, 156.4853973388672)\n",
      "epoch 5000 loss=(416.1201477050781, 0.6242032647132874, 154.37481689453125)\n",
      "epoch 5100 loss=(392.0251159667969, 0.6139678955078125, 150.04624938964844)\n",
      "epoch 5200 loss=(373.0898132324219, 0.6116666793823242, 145.53485107421875)\n",
      "epoch 5300 loss=(359.93255615234375, 0.6056966185569763, 142.62298583984375)\n",
      "epoch 5400 loss=(338.02349853515625, 0.5947864651679993, 138.1521453857422)\n",
      "epoch 5500 loss=(333.2153015136719, 0.5903456807136536, 135.6021270751953)\n",
      "epoch 5600 loss=(310.5022888183594, 0.5888961553573608, 132.04185485839844)\n",
      "epoch 5700 loss=(300.9728698730469, 0.5820573568344116, 127.7862319946289)\n",
      "epoch 5800 loss=(280.4061279296875, 0.5810003876686096, 125.87875366210938)\n",
      "epoch 5900 loss=(265.4512634277344, 0.5757265686988831, 122.28458404541016)\n",
      "epoch 6000 loss=(260.7071533203125, 0.5762943029403687, 116.92473602294922)\n",
      "epoch 6100 loss=(239.3552703857422, 0.5687521696090698, 113.73028564453125)\n",
      "epoch 6200 loss=(222.40304565429688, 0.5617996454238892, 109.49276733398438)\n",
      "epoch 6300 loss=(212.56796264648438, 0.562946081161499, 107.67210388183594)\n",
      "epoch 6400 loss=(207.17298889160156, 0.555206298828125, 104.9646224975586)\n",
      "epoch 6500 loss=(190.45523071289062, 0.5541849136352539, 99.6500015258789)\n",
      "epoch 6600 loss=(175.62271118164062, 0.5536578893661499, 98.44575500488281)\n",
      "epoch 6700 loss=(167.3450469970703, 0.5471714735031128, 91.72420501708984)\n",
      "epoch 6800 loss=(155.7364501953125, 0.5436115860939026, 89.66419219970703)\n",
      "epoch 6900 loss=(150.46546936035156, 0.5386070609092712, 86.0893325805664)\n",
      "epoch 7000 loss=(148.28494262695312, 0.5365508794784546, 81.6212387084961)\n",
      "epoch 7100 loss=(136.65870666503906, 0.5308146476745605, 77.80567169189453)\n",
      "epoch 7200 loss=(126.58401489257812, 0.5303767323493958, 75.1504135131836)\n",
      "epoch 7300 loss=(120.6404037475586, 0.5219835638999939, 70.31989288330078)\n",
      "epoch 7400 loss=(110.35386657714844, 0.5163636207580566, 68.97745513916016)\n",
      "epoch 7500 loss=(106.54752349853516, 0.5158519744873047, 64.05059814453125)\n",
      "epoch 7600 loss=(98.45809936523438, 0.507097601890564, 61.65570831298828)\n",
      "epoch 7700 loss=(89.58356475830078, 0.4994458258152008, 58.91965103149414)\n",
      "epoch 7800 loss=(86.6932601928711, 0.5047929286956787, 54.317386627197266)\n",
      "epoch 7900 loss=(87.67420959472656, 0.49567726254463196, 53.41202163696289)\n",
      "epoch 8000 loss=(76.42204284667969, 0.49060162901878357, 51.37492752075195)\n",
      "epoch 8100 loss=(76.26317596435547, 0.4859272837638855, 47.31841278076172)\n",
      "epoch 8200 loss=(76.57443237304688, 0.4807159900665283, 43.29151916503906)\n",
      "epoch 8300 loss=(67.43470764160156, 0.47603464126586914, 40.724029541015625)\n",
      "epoch 8400 loss=(63.075382232666016, 0.47624146938323975, 38.09523010253906)\n",
      "epoch 8500 loss=(60.66933822631836, 0.4690499007701874, 37.890384674072266)\n",
      "epoch 8600 loss=(55.74735641479492, 0.4676743149757385, 35.04645538330078)\n",
      "epoch 8700 loss=(52.93837356567383, 0.45821982622146606, 32.53367614746094)\n",
      "epoch 8800 loss=(58.424705505371094, 0.463397741317749, 29.12631607055664)\n",
      "epoch 8900 loss=(56.31820297241211, 0.451308012008667, 29.344892501831055)\n",
      "epoch 9000 loss=(48.50944137573242, 0.4430775046348572, 29.209165573120117)\n",
      "epoch 9100 loss=(48.90620040893555, 0.4501686096191406, 28.234437942504883)\n",
      "epoch 9200 loss=(49.20996856689453, 0.4352932274341583, 25.42302894592285)\n",
      "epoch 9300 loss=(47.95220947265625, 0.4391739070415497, 26.53061294555664)\n",
      "epoch 9400 loss=(43.233673095703125, 0.43001502752304077, 23.287677764892578)\n",
      "epoch 9500 loss=(44.44447708129883, 0.43173515796661377, 20.20608901977539)\n",
      "epoch 9600 loss=(42.97584533691406, 0.4282834827899933, 20.490535736083984)\n",
      "epoch 9700 loss=(42.23598861694336, 0.4161265194416046, 20.985671997070312)\n",
      "epoch 9800 loss=(41.76111602783203, 0.41535237431526184, 19.467098236083984)\n",
      "epoch 9900 loss=(40.04045486450195, 0.4137261211872101, 18.222938537597656)\n",
      "epoch 10000 loss=(37.337799072265625, 0.41136401891708374, 18.346208572387695)\n",
      "epoch 10100 loss=(38.77101135253906, 0.4064229726791382, 16.034536361694336)\n",
      "epoch 10200 loss=(39.13004684448242, 0.39780035614967346, 17.777484893798828)\n",
      "epoch 10300 loss=(38.09059143066406, 0.3935561180114746, 15.855360984802246)\n",
      "epoch 10400 loss=(38.53850555419922, 0.3970187306404114, 15.01883316040039)\n",
      "epoch 10500 loss=(36.64900207519531, 0.38274094462394714, 15.330479621887207)\n",
      "epoch 10600 loss=(41.58101272583008, 0.3843619227409363, 13.511567115783691)\n",
      "epoch 10700 loss=(35.720890045166016, 0.38266152143478394, 14.933987617492676)\n",
      "epoch 10800 loss=(40.64004898071289, 0.375510573387146, 13.318110466003418)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10900 loss=(35.68893051147461, 0.37001410126686096, 13.75341510772705)\n",
      "epoch 11000 loss=(35.63298034667969, 0.37096330523490906, 13.438542366027832)\n",
      "epoch 11100 loss=(31.697607040405273, 0.36949092149734497, 13.004411697387695)\n",
      "epoch 11200 loss=(35.359962463378906, 0.3609299063682556, 9.950973510742188)\n",
      "epoch 11300 loss=(35.89720153808594, 0.35302087664604187, 12.759252548217773)\n",
      "epoch 11400 loss=(30.70734214782715, 0.36534422636032104, 11.684103012084961)\n",
      "epoch 11500 loss=(29.967037200927734, 0.35343751311302185, 12.897727012634277)\n",
      "epoch 11600 loss=(33.656211853027344, 0.3462008833885193, 11.038800239562988)\n",
      "epoch 11700 loss=(31.3634090423584, 0.34333157539367676, 13.42618465423584)\n",
      "epoch 11800 loss=(33.630619049072266, 0.3386926054954529, 10.68406867980957)\n",
      "epoch 11900 loss=(35.073951721191406, 0.3361932635307312, 10.922505378723145)\n",
      "epoch 12000 loss=(31.404380798339844, 0.33757761120796204, 10.96273136138916)\n",
      "epoch 12100 loss=(33.018470764160156, 0.33082863688468933, 10.658102035522461)\n",
      "epoch 12200 loss=(30.415924072265625, 0.32548972964286804, 11.314674377441406)\n",
      "epoch 12300 loss=(31.397499084472656, 0.3221368193626404, 10.99925708770752)\n",
      "epoch 12400 loss=(33.61945343017578, 0.31956157088279724, 10.807758331298828)\n",
      "epoch 12500 loss=(32.60157775878906, 0.3108113408088684, 12.345353126525879)\n",
      "epoch 12600 loss=(26.13086700439453, 0.30821385979652405, 10.733712196350098)\n",
      "epoch 12700 loss=(31.886646270751953, 0.3099091649055481, 11.281601905822754)\n",
      "epoch 12800 loss=(30.841896057128906, 0.30205464363098145, 10.990717887878418)\n",
      "epoch 12900 loss=(31.298795700073242, 0.3043350875377655, 11.069594383239746)\n",
      "epoch 13000 loss=(31.777482986450195, 0.30263635516166687, 10.861594200134277)\n",
      "epoch 13100 loss=(33.6872673034668, 0.29726722836494446, 9.844718933105469)\n",
      "epoch 13200 loss=(29.356101989746094, 0.28700608015060425, 11.033346176147461)\n",
      "epoch 13300 loss=(36.896114349365234, 0.2893446087837219, 10.386967658996582)\n",
      "epoch 13400 loss=(32.79692077636719, 0.27603453397750854, 10.292816162109375)\n",
      "epoch 13500 loss=(32.7404670715332, 0.28149810433387756, 10.879443168640137)\n",
      "epoch 13600 loss=(29.33580207824707, 0.27163374423980713, 10.277286529541016)\n",
      "epoch 13700 loss=(33.477779388427734, 0.2711203992366791, 9.677199363708496)\n",
      "epoch 13800 loss=(30.983440399169922, 0.269305944442749, 11.48775863647461)\n",
      "epoch 13900 loss=(30.896671295166016, 0.27062371373176575, 10.004918098449707)\n",
      "epoch 14000 loss=(34.27869415283203, 0.26730695366859436, 10.421932220458984)\n",
      "epoch 14100 loss=(29.771242141723633, 0.26618653535842896, 11.116063117980957)\n",
      "epoch 14200 loss=(31.217226028442383, 0.2537511885166168, 9.067131042480469)\n",
      "epoch 14300 loss=(36.00181198120117, 0.2515358328819275, 10.327093124389648)\n",
      "epoch 14400 loss=(31.70793342590332, 0.25574225187301636, 10.124604225158691)\n",
      "epoch 14500 loss=(31.701366424560547, 0.24399720132350922, 8.613557815551758)\n",
      "epoch 14600 loss=(27.75684928894043, 0.2521260380744934, 10.163326263427734)\n",
      "epoch 14700 loss=(28.93253517150879, 0.24073810875415802, 10.905108451843262)\n",
      "epoch 14800 loss=(31.42396354675293, 0.2370028793811798, 9.805865287780762)\n",
      "epoch 14900 loss=(30.546323776245117, 0.24028687179088593, 10.00284481048584)\n",
      "epoch 14999 loss=(31.8291072845459, 0.24262654781341553, 10.008886337280273)\n",
      "Finished training\n",
      "0 (25.322284698486328, 0.8166667222976685, 14.055497169494629)\n",
      "epoch 0 loss=(34024.375, 0.6894002556800842, 10707.265625)\n",
      "epoch 100 loss=(3228.05517578125, 0.6982386112213135, 783.5966186523438)\n",
      "epoch 200 loss=(2061.458740234375, 0.6759705543518066, 346.1680603027344)\n",
      "epoch 300 loss=(1881.321533203125, 0.6829935312271118, 321.19122314453125)\n",
      "epoch 400 loss=(1720.30810546875, 0.6941953301429749, 299.22705078125)\n",
      "epoch 500 loss=(1585.61279296875, 0.6903281211853027, 281.1703796386719)\n",
      "epoch 600 loss=(1479.5654296875, 0.6797857284545898, 266.3667297363281)\n",
      "epoch 700 loss=(1403.499755859375, 0.6776605844497681, 255.51295471191406)\n",
      "epoch 800 loss=(1350.2415771484375, 0.6823633909225464, 248.4268035888672)\n",
      "epoch 900 loss=(1308.6435546875, 0.6825396418571472, 243.11639404296875)\n",
      "epoch 1000 loss=(1274.699462890625, 0.6856842041015625, 238.86895751953125)\n",
      "epoch 1100 loss=(1247.3759765625, 0.6918144822120667, 235.09762573242188)\n",
      "epoch 1200 loss=(1222.970458984375, 0.6930971145629883, 232.3777313232422)\n",
      "epoch 1300 loss=(1201.7755126953125, 0.6962242722511292, 229.65994262695312)\n",
      "epoch 1400 loss=(1178.820068359375, 0.691423773765564, 227.13291931152344)\n",
      "epoch 1500 loss=(1156.0631103515625, 0.6841904520988464, 224.947265625)\n",
      "epoch 1600 loss=(1136.2027587890625, 0.6906115412712097, 222.6947021484375)\n",
      "epoch 1700 loss=(1116.8250732421875, 0.6857384443283081, 219.8047637939453)\n",
      "epoch 1800 loss=(1092.883056640625, 0.67827969789505, 218.25990295410156)\n",
      "epoch 1900 loss=(1066.9908447265625, 0.6736385226249695, 216.34925842285156)\n",
      "epoch 2000 loss=(1046.71240234375, 0.6665307283401489, 214.35501098632812)\n",
      "epoch 2100 loss=(1026.0289306640625, 0.6620034575462341, 212.10240173339844)\n",
      "epoch 2200 loss=(1001.1210327148438, 0.660849392414093, 209.98272705078125)\n",
      "epoch 2300 loss=(977.8326416015625, 0.6580289602279663, 207.32489013671875)\n",
      "epoch 2400 loss=(952.5716552734375, 0.6544336080551147, 205.6160888671875)\n",
      "epoch 2500 loss=(928.7243041992188, 0.6494447588920593, 203.6558074951172)\n",
      "epoch 2600 loss=(903.0925903320312, 0.6502986550331116, 200.93077087402344)\n",
      "epoch 2700 loss=(880.6635131835938, 0.641801118850708, 198.66659545898438)\n",
      "epoch 2800 loss=(851.442138671875, 0.6410430669784546, 196.2085418701172)\n",
      "epoch 2900 loss=(826.935546875, 0.6452490091323853, 194.50852966308594)\n",
      "epoch 3000 loss=(802.5089721679688, 0.6366735696792603, 191.52972412109375)\n",
      "epoch 3100 loss=(774.9500732421875, 0.6379977464675903, 189.21734619140625)\n",
      "epoch 3200 loss=(747.249755859375, 0.6329466104507446, 186.9080047607422)\n",
      "epoch 3300 loss=(722.6480712890625, 0.6333186626434326, 183.9430694580078)\n",
      "epoch 3400 loss=(698.5203247070312, 0.6289340853691101, 181.44569396972656)\n",
      "epoch 3500 loss=(674.5949096679688, 0.6254894137382507, 179.08653259277344)\n",
      "epoch 3600 loss=(648.250732421875, 0.6269025206565857, 176.756103515625)\n",
      "epoch 3700 loss=(629.550048828125, 0.6250942349433899, 173.80120849609375)\n",
      "epoch 3800 loss=(600.0196533203125, 0.6266381740570068, 171.22756958007812)\n",
      "epoch 3900 loss=(588.4801635742188, 0.6168085932731628, 168.8712158203125)\n",
      "epoch 4000 loss=(555.9546508789062, 0.6115889549255371, 165.36634826660156)\n",
      "epoch 4100 loss=(540.3675537109375, 0.6143566370010376, 162.48916625976562)\n",
      "epoch 4200 loss=(515.0230712890625, 0.6026917099952698, 160.0283203125)\n",
      "epoch 4300 loss=(495.7639465332031, 0.6017853617668152, 156.9938201904297)\n",
      "epoch 4400 loss=(476.0138854980469, 0.6000353693962097, 154.14694213867188)\n",
      "epoch 4500 loss=(455.60894775390625, 0.5976016521453857, 149.94863891601562)\n",
      "epoch 4600 loss=(448.9288330078125, 0.5920745730400085, 147.99819946289062)\n",
      "epoch 4700 loss=(422.61614990234375, 0.5921568274497986, 144.94017028808594)\n",
      "epoch 4800 loss=(398.7898254394531, 0.589285671710968, 142.4706268310547)\n",
      "epoch 4900 loss=(380.9775390625, 0.5850023627281189, 138.4481964111328)\n",
      "epoch 5000 loss=(379.0824890136719, 0.581280529499054, 133.80215454101562)\n",
      "epoch 5100 loss=(355.10662841796875, 0.5790609121322632, 131.08238220214844)\n",
      "epoch 5200 loss=(339.9693298339844, 0.5734688639640808, 128.2959442138672)\n",
      "epoch 5300 loss=(330.80633544921875, 0.5674301981925964, 125.17870330810547)\n",
      "epoch 5400 loss=(306.9341125488281, 0.5622249245643616, 120.83509063720703)\n",
      "epoch 5500 loss=(299.7808532714844, 0.5629028677940369, 117.27818298339844)\n",
      "epoch 5600 loss=(282.1418151855469, 0.563625156879425, 114.0710220336914)\n",
      "epoch 5700 loss=(264.8040771484375, 0.5589187741279602, 110.4399642944336)\n",
      "epoch 5800 loss=(252.1210479736328, 0.5582385063171387, 107.64533233642578)\n",
      "epoch 5900 loss=(244.69351196289062, 0.5498887300491333, 102.56690979003906)\n",
      "epoch 6000 loss=(224.68240356445312, 0.5486966967582703, 97.9903564453125)\n",
      "epoch 6100 loss=(221.30810546875, 0.5483548641204834, 95.54634857177734)\n",
      "epoch 6200 loss=(208.11769104003906, 0.5406345129013062, 91.07778930664062)\n",
      "epoch 6300 loss=(192.6153564453125, 0.5333437919616699, 89.02779388427734)\n",
      "epoch 6400 loss=(180.1453399658203, 0.5340474247932434, 83.73299407958984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6500 loss=(174.34033203125, 0.5195513367652893, 81.17807006835938)\n",
      "epoch 6600 loss=(159.37266540527344, 0.5184041261672974, 78.739013671875)\n",
      "epoch 6700 loss=(146.3401641845703, 0.5181519389152527, 74.3351058959961)\n",
      "epoch 6800 loss=(145.09130859375, 0.5151187181472778, 69.44391632080078)\n",
      "epoch 6900 loss=(136.03231811523438, 0.5034183859825134, 66.7623291015625)\n",
      "epoch 7000 loss=(129.41505432128906, 0.5018207430839539, 64.51425170898438)\n",
      "epoch 7100 loss=(130.34690856933594, 0.4933115541934967, 60.551151275634766)\n",
      "epoch 7200 loss=(111.4366683959961, 0.4961828291416168, 59.76736831665039)\n",
      "epoch 7300 loss=(104.73654174804688, 0.4904756546020508, 53.868080139160156)\n",
      "epoch 7400 loss=(103.27442169189453, 0.4824196994304657, 50.25917053222656)\n",
      "epoch 7500 loss=(94.75049591064453, 0.4787808358669281, 48.947021484375)\n",
      "epoch 7600 loss=(89.6080322265625, 0.4660957157611847, 47.52576446533203)\n",
      "epoch 7700 loss=(84.30463409423828, 0.4625184237957001, 44.24424362182617)\n",
      "epoch 7800 loss=(78.42491149902344, 0.4575875997543335, 42.360774993896484)\n",
      "epoch 7900 loss=(74.6495132446289, 0.44946005940437317, 37.7636604309082)\n",
      "epoch 8000 loss=(77.32369995117188, 0.4414465129375458, 36.86936569213867)\n",
      "epoch 8100 loss=(69.8547592163086, 0.4491390883922577, 33.8553352355957)\n",
      "epoch 8200 loss=(67.76231384277344, 0.437224417924881, 30.27745819091797)\n",
      "epoch 8300 loss=(63.019142150878906, 0.43509674072265625, 31.575841903686523)\n",
      "epoch 8400 loss=(58.032047271728516, 0.4166182577610016, 28.193893432617188)\n",
      "epoch 8500 loss=(53.467918395996094, 0.42249158024787903, 27.881980895996094)\n",
      "epoch 8600 loss=(52.433467864990234, 0.40971770882606506, 23.85996437072754)\n",
      "epoch 8700 loss=(57.000240325927734, 0.4078490436077118, 24.199077606201172)\n",
      "epoch 8800 loss=(51.79873275756836, 0.40017661452293396, 22.750587463378906)\n",
      "epoch 8900 loss=(44.998687744140625, 0.39442676305770874, 23.74431800842285)\n",
      "epoch 9000 loss=(46.22711944580078, 0.3933398723602295, 19.00883674621582)\n",
      "epoch 9100 loss=(50.11214828491211, 0.38506680727005005, 20.557405471801758)\n",
      "epoch 9200 loss=(44.50289535522461, 0.37738850712776184, 19.707229614257812)\n",
      "epoch 9300 loss=(45.58018493652344, 0.37801557779312134, 18.770544052124023)\n",
      "epoch 9400 loss=(42.52542495727539, 0.37323689460754395, 16.846662521362305)\n",
      "epoch 9500 loss=(41.46732711791992, 0.3672584593296051, 16.63905143737793)\n",
      "epoch 9600 loss=(39.317352294921875, 0.35574570298194885, 15.068246841430664)\n",
      "epoch 9700 loss=(36.820518493652344, 0.3586365580558777, 14.965690612792969)\n",
      "epoch 9800 loss=(35.225746154785156, 0.35696840286254883, 14.91270637512207)\n",
      "epoch 9900 loss=(37.08064270019531, 0.34674644470214844, 15.605534553527832)\n",
      "epoch 10000 loss=(35.47430419921875, 0.3412035405635834, 13.033191680908203)\n",
      "epoch 10100 loss=(37.73291778564453, 0.34219807386398315, 15.43028450012207)\n",
      "epoch 10200 loss=(36.81758499145508, 0.34491202235221863, 13.559556007385254)\n",
      "epoch 10300 loss=(34.314388275146484, 0.32607707381248474, 12.784026145935059)\n",
      "epoch 10400 loss=(33.1788215637207, 0.3332119584083557, 13.220884323120117)\n",
      "epoch 10500 loss=(36.43221664428711, 0.31865888833999634, 13.556513786315918)\n",
      "epoch 10600 loss=(34.68052291870117, 0.3262217044830322, 11.086950302124023)\n",
      "epoch 10700 loss=(34.230628967285156, 0.3197609484195709, 12.975829124450684)\n",
      "epoch 10800 loss=(35.69828414916992, 0.31973904371261597, 11.760251998901367)\n",
      "epoch 10900 loss=(33.45793914794922, 0.3140956461429596, 13.077796936035156)\n",
      "epoch 11000 loss=(34.25173568725586, 0.30801287293434143, 12.787036895751953)\n",
      "epoch 11100 loss=(34.94559860229492, 0.3066950738430023, 11.94617748260498)\n",
      "epoch 11200 loss=(32.99026870727539, 0.30635443329811096, 10.231513023376465)\n",
      "epoch 11300 loss=(35.174922943115234, 0.2956508696079254, 11.854009628295898)\n",
      "epoch 11400 loss=(29.671384811401367, 0.29147499799728394, 11.423328399658203)\n",
      "epoch 11500 loss=(28.499582290649414, 0.297080397605896, 10.270291328430176)\n",
      "epoch 11600 loss=(33.06222915649414, 0.28763526678085327, 11.773630142211914)\n",
      "epoch 11700 loss=(31.20886993408203, 0.2784115672111511, 11.772347450256348)\n",
      "epoch 11800 loss=(32.48353576660156, 0.2739490866661072, 10.434916496276855)\n",
      "epoch 11900 loss=(31.19423484802246, 0.2747383415699005, 10.487049102783203)\n",
      "epoch 12000 loss=(33.6822624206543, 0.27725017070770264, 10.19161319732666)\n",
      "epoch 12100 loss=(33.071903228759766, 0.2790243327617645, 12.187231063842773)\n",
      "epoch 12200 loss=(34.94881057739258, 0.2678638994693756, 9.97183895111084)\n",
      "epoch 12300 loss=(30.246158599853516, 0.26503464579582214, 10.452564239501953)\n",
      "epoch 12400 loss=(31.169933319091797, 0.2635355293750763, 9.93407917022705)\n",
      "epoch 12500 loss=(31.472488403320312, 0.2624829411506653, 10.479393005371094)\n",
      "epoch 12600 loss=(33.4278564453125, 0.26233574748039246, 10.473151206970215)\n",
      "epoch 12700 loss=(33.860233306884766, 0.24845941364765167, 9.905413627624512)\n",
      "epoch 12800 loss=(30.787395477294922, 0.2532980442047119, 8.771002769470215)\n",
      "epoch 12900 loss=(31.720966339111328, 0.2424546629190445, 9.370080947875977)\n",
      "epoch 13000 loss=(33.06919860839844, 0.24444562196731567, 9.54224681854248)\n",
      "epoch 13100 loss=(31.61647605895996, 0.24719570577144623, 10.419054985046387)\n",
      "epoch 13200 loss=(30.08987808227539, 0.24332420527935028, 9.53913402557373)\n",
      "epoch 13300 loss=(31.186437606811523, 0.2434619665145874, 10.356202125549316)\n",
      "epoch 13400 loss=(29.624324798583984, 0.23275811970233917, 10.490137100219727)\n",
      "epoch 13500 loss=(31.550291061401367, 0.23839977383613586, 9.028730392456055)\n",
      "epoch 13600 loss=(34.325164794921875, 0.23382136225700378, 9.823326110839844)\n",
      "epoch 13700 loss=(29.724441528320312, 0.2303864061832428, 8.96102523803711)\n",
      "epoch 13800 loss=(34.17531204223633, 0.22342394292354584, 10.02783203125)\n",
      "epoch 13900 loss=(32.093448638916016, 0.2198142260313034, 10.394939422607422)\n",
      "epoch 14000 loss=(33.531455993652344, 0.21798889338970184, 9.025995254516602)\n",
      "epoch 14100 loss=(32.788421630859375, 0.21871857345104218, 10.383359909057617)\n",
      "epoch 14200 loss=(29.161115646362305, 0.21083354949951172, 9.547730445861816)\n",
      "epoch 14300 loss=(29.01601219177246, 0.20762643218040466, 10.207857131958008)\n",
      "epoch 14400 loss=(30.815420150756836, 0.20771823823451996, 10.13360595703125)\n",
      "epoch 14500 loss=(31.763751983642578, 0.20548738539218903, 9.444709777832031)\n",
      "Early stopping\n",
      "Finished training\n",
      "1 (26.60723114013672, 0.8333333730697632, 15.865699768066406)\n",
      "epoch 0 loss=(34492.5390625, 0.6875897645950317, 10751.3212890625)\n",
      "epoch 100 loss=(8959.6328125, 1.5031609535217285, 3807.41943359375)\n",
      "epoch 200 loss=(2327.638916015625, 2.245939016342163, 687.579833984375)\n",
      "epoch 300 loss=(1597.933837890625, 2.2372429370880127, 278.1036071777344)\n",
      "epoch 400 loss=(1498.1053466796875, 2.0163090229034424, 258.2366638183594)\n",
      "epoch 500 loss=(1433.70263671875, 1.7249302864074707, 253.5905303955078)\n",
      "epoch 600 loss=(1382.7772216796875, 1.3574403524398804, 250.21282958984375)\n",
      "epoch 700 loss=(1339.62158203125, 0.9947885274887085, 245.64500427246094)\n",
      "epoch 800 loss=(1305.7408447265625, 0.7580971717834473, 241.09674072265625)\n",
      "epoch 900 loss=(1276.9498291015625, 0.6909573674201965, 237.10877990722656)\n",
      "epoch 1000 loss=(1252.950927734375, 0.6890290975570679, 233.37086486816406)\n",
      "epoch 1100 loss=(1229.499755859375, 0.6845037341117859, 229.746337890625)\n",
      "epoch 1200 loss=(1209.921142578125, 0.6801851987838745, 226.35406494140625)\n",
      "epoch 1300 loss=(1187.6649169921875, 0.6757946610450745, 223.23046875)\n",
      "epoch 1400 loss=(1167.920654296875, 0.6818860173225403, 220.63966369628906)\n",
      "epoch 1500 loss=(1146.8616943359375, 0.6739770770072937, 218.0557861328125)\n",
      "epoch 1600 loss=(1126.795166015625, 0.6744227409362793, 215.83029174804688)\n",
      "epoch 1700 loss=(1107.9541015625, 0.6759341359138489, 212.8893280029297)\n",
      "epoch 1800 loss=(1085.905029296875, 0.6784882545471191, 211.37142944335938)\n",
      "epoch 1900 loss=(1064.809326171875, 0.6760225296020508, 208.6730499267578)\n",
      "epoch 2000 loss=(1044.8013916015625, 0.6741551756858826, 206.46311950683594)\n",
      "epoch 2100 loss=(1026.0428466796875, 0.6794207096099854, 204.4700927734375)\n",
      "epoch 2200 loss=(1002.9722290039062, 0.6746804714202881, 202.27166748046875)\n",
      "epoch 2300 loss=(980.4900512695312, 0.6789659857749939, 200.6019744873047)\n",
      "epoch 2400 loss=(960.2294311523438, 0.6755728721618652, 198.01817321777344)\n",
      "epoch 2500 loss=(936.6106567382812, 0.6756171584129333, 195.37181091308594)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2600 loss=(916.4573364257812, 0.6750541925430298, 193.2520751953125)\n",
      "epoch 2700 loss=(895.3868408203125, 0.6693183779716492, 191.05072021484375)\n",
      "epoch 2800 loss=(873.1363525390625, 0.6679407954216003, 189.50328063964844)\n",
      "epoch 2900 loss=(848.7347412109375, 0.6583871245384216, 187.4261932373047)\n",
      "epoch 3000 loss=(833.0697021484375, 0.6594201922416687, 184.71444702148438)\n",
      "epoch 3100 loss=(810.1107177734375, 0.6611530780792236, 182.6865997314453)\n",
      "epoch 3200 loss=(785.1355590820312, 0.6544987559318542, 180.43600463867188)\n",
      "epoch 3300 loss=(768.359130859375, 0.6464167833328247, 178.451416015625)\n",
      "epoch 3400 loss=(748.9642944335938, 0.648076057434082, 175.41566467285156)\n",
      "epoch 3500 loss=(726.0905151367188, 0.6431788206100464, 173.4097137451172)\n",
      "epoch 3600 loss=(708.5702514648438, 0.6406373381614685, 171.40252685546875)\n",
      "epoch 3700 loss=(685.1307373046875, 0.6420516967773438, 169.09251403808594)\n",
      "epoch 3800 loss=(666.6702880859375, 0.6412068605422974, 166.4605255126953)\n",
      "epoch 3900 loss=(645.8851928710938, 0.6464927196502686, 164.48580932617188)\n",
      "epoch 4000 loss=(627.9813232421875, 0.63141268491745, 161.36561584472656)\n",
      "epoch 4100 loss=(608.0079956054688, 0.6408204436302185, 158.618896484375)\n",
      "epoch 4200 loss=(588.9059448242188, 0.638527512550354, 156.17559814453125)\n",
      "epoch 4300 loss=(569.7680053710938, 0.6380361914634705, 154.98504638671875)\n",
      "epoch 4400 loss=(549.5696411132812, 0.6407482028007507, 150.7107696533203)\n",
      "epoch 4500 loss=(538.859619140625, 0.6393222808837891, 148.13218688964844)\n",
      "epoch 4600 loss=(516.66943359375, 0.6380971670150757, 144.7792510986328)\n",
      "epoch 4700 loss=(504.428466796875, 0.6365995407104492, 142.21841430664062)\n",
      "epoch 4800 loss=(490.8427734375, 0.6331084370613098, 140.4146728515625)\n",
      "epoch 4900 loss=(476.2281188964844, 0.6359316110610962, 137.640380859375)\n",
      "epoch 5000 loss=(457.5634460449219, 0.6370077133178711, 133.28494262695312)\n",
      "epoch 5100 loss=(436.6566162109375, 0.638664960861206, 130.37054443359375)\n",
      "epoch 5200 loss=(430.13519287109375, 0.6430433988571167, 128.01065063476562)\n",
      "epoch 5300 loss=(409.8413391113281, 0.6434546113014221, 125.5467300415039)\n",
      "epoch 5400 loss=(398.91339111328125, 0.6349281072616577, 121.91326904296875)\n",
      "epoch 5500 loss=(393.5481262207031, 0.6372756361961365, 119.72119903564453)\n",
      "epoch 5600 loss=(368.7944641113281, 0.6486997008323669, 118.06436920166016)\n",
      "epoch 5700 loss=(359.7108459472656, 0.6410592794418335, 114.22299194335938)\n",
      "epoch 5800 loss=(337.6241149902344, 0.6378428339958191, 111.51403045654297)\n",
      "epoch 5900 loss=(332.3149719238281, 0.6456717848777771, 107.27818298339844)\n",
      "epoch 6000 loss=(321.9294738769531, 0.6380712389945984, 103.85415649414062)\n",
      "epoch 6100 loss=(305.2086181640625, 0.6444865465164185, 101.39983367919922)\n",
      "epoch 6200 loss=(296.20599365234375, 0.6380496025085449, 100.50814819335938)\n",
      "epoch 6300 loss=(283.0831604003906, 0.6382732391357422, 96.84969329833984)\n",
      "epoch 6400 loss=(271.28857421875, 0.6360966563224792, 92.74342346191406)\n",
      "epoch 6500 loss=(263.74395751953125, 0.6380288004875183, 89.51908111572266)\n",
      "epoch 6600 loss=(257.5118408203125, 0.6322882175445557, 87.52676391601562)\n",
      "epoch 6700 loss=(250.3784637451172, 0.632878839969635, 85.47390747070312)\n",
      "epoch 6800 loss=(237.52281188964844, 0.6267781853675842, 81.74385833740234)\n",
      "epoch 6900 loss=(220.50439453125, 0.6289370656013489, 78.37206268310547)\n",
      "epoch 7000 loss=(207.7717742919922, 0.6305412650108337, 76.35012817382812)\n",
      "epoch 7100 loss=(203.85572814941406, 0.614285945892334, 73.98656463623047)\n",
      "epoch 7200 loss=(187.97654724121094, 0.6101260781288147, 70.32635498046875)\n",
      "epoch 7300 loss=(190.23153686523438, 0.6080043911933899, 68.2009506225586)\n",
      "epoch 7400 loss=(178.48594665527344, 0.6023399233818054, 67.0242919921875)\n",
      "epoch 7500 loss=(170.18211364746094, 0.6018629670143127, 61.912010192871094)\n",
      "epoch 7600 loss=(160.76861572265625, 0.5936056971549988, 62.60399627685547)\n",
      "epoch 7700 loss=(152.48707580566406, 0.5874106884002686, 56.93219757080078)\n",
      "epoch 7800 loss=(150.89329528808594, 0.5826398730278015, 57.34117126464844)\n",
      "epoch 7900 loss=(142.6240692138672, 0.5727139711380005, 54.094322204589844)\n",
      "epoch 8000 loss=(136.1863250732422, 0.5718390941619873, 50.105552673339844)\n",
      "epoch 8100 loss=(132.80047607421875, 0.5603373646736145, 48.93250274658203)\n",
      "epoch 8200 loss=(124.71879577636719, 0.5623591542243958, 48.84052658081055)\n",
      "epoch 8300 loss=(122.780517578125, 0.5404130816459656, 45.753204345703125)\n",
      "epoch 8400 loss=(109.3356704711914, 0.5362670421600342, 41.83515167236328)\n",
      "epoch 8500 loss=(110.86572265625, 0.5242303609848022, 41.277095794677734)\n",
      "epoch 8600 loss=(105.13438415527344, 0.5287285447120667, 40.053062438964844)\n",
      "epoch 8700 loss=(94.59407043457031, 0.5150224566459656, 37.46200180053711)\n",
      "epoch 8800 loss=(95.09050750732422, 0.5048121809959412, 35.815155029296875)\n",
      "epoch 8900 loss=(87.71592712402344, 0.5052604675292969, 36.72479248046875)\n",
      "epoch 9000 loss=(86.17707824707031, 0.49458378553390503, 32.621952056884766)\n",
      "epoch 9100 loss=(79.28971862792969, 0.48464858531951904, 30.184295654296875)\n",
      "epoch 9200 loss=(73.87980651855469, 0.48515430092811584, 30.28899383544922)\n",
      "epoch 9300 loss=(72.61680603027344, 0.4858574867248535, 28.66111183166504)\n",
      "epoch 9400 loss=(67.98674774169922, 0.4730309844017029, 26.824560165405273)\n",
      "epoch 9500 loss=(66.66510772705078, 0.4627957344055176, 26.063661575317383)\n",
      "epoch 9600 loss=(62.05529022216797, 0.459808886051178, 24.64353370666504)\n",
      "epoch 9700 loss=(68.66717529296875, 0.45537546277046204, 22.14738655090332)\n",
      "epoch 9800 loss=(63.88135528564453, 0.4441964328289032, 22.386333465576172)\n",
      "epoch 9900 loss=(62.89011764526367, 0.4375918507575989, 21.536054611206055)\n",
      "epoch 10000 loss=(60.605140686035156, 0.43465277552604675, 21.64220428466797)\n",
      "epoch 10100 loss=(52.518165588378906, 0.4304419755935669, 21.18755531311035)\n",
      "epoch 10200 loss=(53.03437423706055, 0.4393835663795471, 19.956335067749023)\n",
      "epoch 10300 loss=(55.5109748840332, 0.42008328437805176, 18.110933303833008)\n",
      "epoch 10400 loss=(50.7385139465332, 0.4143567681312561, 16.365032196044922)\n",
      "epoch 10500 loss=(53.51346206665039, 0.39552584290504456, 17.9648494720459)\n",
      "epoch 10600 loss=(48.982784271240234, 0.4197129011154175, 17.494287490844727)\n",
      "epoch 10700 loss=(45.821475982666016, 0.4009920656681061, 17.01030731201172)\n",
      "epoch 10800 loss=(43.21531295776367, 0.408841073513031, 16.57352638244629)\n",
      "epoch 10900 loss=(42.918670654296875, 0.39937999844551086, 14.855914115905762)\n",
      "epoch 11000 loss=(44.508460998535156, 0.38279274106025696, 15.544921875)\n",
      "epoch 11100 loss=(41.46697235107422, 0.38551440834999084, 14.0569429397583)\n",
      "epoch 11200 loss=(43.39374923706055, 0.389519602060318, 15.01667308807373)\n",
      "epoch 11300 loss=(46.12299728393555, 0.37678438425064087, 13.67486572265625)\n",
      "epoch 11400 loss=(41.749935150146484, 0.3855619728565216, 13.616789817810059)\n",
      "epoch 11500 loss=(39.787845611572266, 0.3680022358894348, 13.178555488586426)\n",
      "epoch 11600 loss=(39.19757843017578, 0.37592852115631104, 13.64476203918457)\n",
      "epoch 11700 loss=(41.89141082763672, 0.36752092838287354, 12.71784782409668)\n",
      "epoch 11800 loss=(39.894371032714844, 0.36700940132141113, 13.494539260864258)\n",
      "epoch 11900 loss=(36.109535217285156, 0.35834869742393494, 11.775053024291992)\n",
      "epoch 12000 loss=(36.304527282714844, 0.36052384972572327, 11.820086479187012)\n",
      "epoch 12100 loss=(40.93318176269531, 0.34338757395744324, 10.366957664489746)\n",
      "epoch 12200 loss=(38.555458068847656, 0.3464736044406891, 10.929669380187988)\n",
      "epoch 12300 loss=(36.70267105102539, 0.33709871768951416, 10.535626411437988)\n",
      "epoch 12400 loss=(37.15370178222656, 0.33037111163139343, 11.597739219665527)\n",
      "epoch 12500 loss=(34.27911376953125, 0.32358160614967346, 12.200616836547852)\n",
      "epoch 12600 loss=(32.24901580810547, 0.3226998448371887, 11.534165382385254)\n",
      "epoch 12700 loss=(37.910465240478516, 0.31689292192459106, 11.651566505432129)\n",
      "epoch 12800 loss=(38.54283142089844, 0.3137946128845215, 11.19059944152832)\n",
      "epoch 12900 loss=(43.67586135864258, 0.3212500214576721, 10.2008056640625)\n",
      "epoch 13000 loss=(36.58077621459961, 0.30922120809555054, 12.20667839050293)\n",
      "epoch 13100 loss=(33.01220703125, 0.3082089126110077, 10.642935752868652)\n",
      "epoch 13200 loss=(32.78627395629883, 0.3065530061721802, 9.827731132507324)\n",
      "epoch 13300 loss=(31.175128936767578, 0.30314067006111145, 10.787065505981445)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13400 loss=(31.18669319152832, 0.3113628327846527, 9.810646057128906)\n",
      "epoch 13500 loss=(36.73444747924805, 0.296626478433609, 10.60896110534668)\n",
      "epoch 13600 loss=(39.978965759277344, 0.28623971343040466, 9.328393936157227)\n",
      "epoch 13700 loss=(35.55405044555664, 0.28400346636772156, 10.609560012817383)\n",
      "epoch 13800 loss=(34.430564880371094, 0.2842578589916229, 11.024502754211426)\n",
      "epoch 13900 loss=(33.597694396972656, 0.29503753781318665, 10.606837272644043)\n",
      "epoch 14000 loss=(35.225162506103516, 0.2912326455116272, 9.812630653381348)\n",
      "epoch 14100 loss=(32.0417366027832, 0.28150832653045654, 11.248828887939453)\n",
      "epoch 14200 loss=(32.56589889526367, 0.27873674035072327, 10.93210220336914)\n",
      "epoch 14300 loss=(34.08009719848633, 0.27570998668670654, 9.997740745544434)\n",
      "epoch 14400 loss=(35.402557373046875, 0.2606024146080017, 10.17635440826416)\n",
      "epoch 14500 loss=(32.94259262084961, 0.2654147446155548, 10.28441333770752)\n",
      "epoch 14600 loss=(35.5040283203125, 0.28000861406326294, 10.761906623840332)\n",
      "epoch 14700 loss=(31.84014129638672, 0.2638740837574005, 9.888028144836426)\n",
      "epoch 14800 loss=(31.87210464477539, 0.26175472140312195, 10.330527305603027)\n",
      "epoch 14900 loss=(33.956947326660156, 0.2665663957595825, 10.058635711669922)\n",
      "epoch 14999 loss=(34.9051513671875, 0.2615147531032562, 8.985343933105469)\n",
      "Finished training\n",
      "2 (19.459274291992188, 0.6833333969116211, 15.808429718017578)\n",
      "epoch 0 loss=(34415.7265625, 0.7026633620262146, 10843.2685546875)\n",
      "epoch 100 loss=(5744.9619140625, 6.077956676483154, 1760.224609375)\n",
      "epoch 200 loss=(1628.48291015625, 4.4558024406433105, 330.6490173339844)\n",
      "epoch 300 loss=(1458.806884765625, 3.3715600967407227, 271.5757751464844)\n",
      "epoch 400 loss=(1384.2879638671875, 2.491027355194092, 252.96725463867188)\n",
      "epoch 500 loss=(1332.409423828125, 1.7192778587341309, 243.59078979492188)\n",
      "epoch 600 loss=(1296.54443359375, 1.1727091073989868, 237.47422790527344)\n",
      "epoch 700 loss=(1269.5235595703125, 0.9079796671867371, 233.45050048828125)\n",
      "epoch 800 loss=(1247.1844482421875, 0.8695715665817261, 229.99131774902344)\n",
      "epoch 900 loss=(1226.8323974609375, 0.8766233921051025, 226.7418670654297)\n",
      "epoch 1000 loss=(1205.8157958984375, 0.899369478225708, 223.23928833007812)\n",
      "epoch 1100 loss=(1186.317626953125, 0.914243757724762, 220.34890747070312)\n",
      "epoch 1200 loss=(1163.8839111328125, 0.9266163110733032, 217.01478576660156)\n",
      "epoch 1300 loss=(1143.4859619140625, 0.9365798234939575, 213.455322265625)\n",
      "epoch 1400 loss=(1123.9906005859375, 0.9376223683357239, 210.46009826660156)\n",
      "epoch 1500 loss=(1097.9014892578125, 0.9323752522468567, 207.45037841796875)\n",
      "epoch 1600 loss=(1076.2550048828125, 0.9376299381256104, 204.2502899169922)\n",
      "epoch 1700 loss=(1050.5858154296875, 0.9309137463569641, 200.3756103515625)\n",
      "epoch 1800 loss=(1025.92626953125, 0.9321454167366028, 197.22042846679688)\n",
      "epoch 1900 loss=(1003.562255859375, 0.9033613801002502, 195.1798553466797)\n",
      "epoch 2000 loss=(981.9564819335938, 0.8986528515815735, 191.45912170410156)\n",
      "epoch 2100 loss=(957.5288696289062, 0.8858452439308167, 188.82672119140625)\n",
      "epoch 2200 loss=(931.9154052734375, 0.8704717755317688, 184.8840789794922)\n",
      "epoch 2300 loss=(907.1974487304688, 0.8608388304710388, 182.1092529296875)\n",
      "epoch 2400 loss=(882.2135620117188, 0.8324071764945984, 179.63845825195312)\n",
      "epoch 2500 loss=(860.9336547851562, 0.846985936164856, 175.81507873535156)\n",
      "epoch 2600 loss=(836.3843383789062, 0.8226801753044128, 173.56173706054688)\n",
      "epoch 2700 loss=(813.2920532226562, 0.8002985715866089, 170.57147216796875)\n",
      "epoch 2800 loss=(787.5093994140625, 0.7967208623886108, 167.83355712890625)\n",
      "epoch 2900 loss=(764.8502197265625, 0.7870290875434875, 164.18177795410156)\n",
      "epoch 3000 loss=(745.2752685546875, 0.7736062407493591, 161.05117797851562)\n",
      "epoch 3100 loss=(719.47705078125, 0.7619707584381104, 159.51486206054688)\n",
      "epoch 3200 loss=(699.7362060546875, 0.7543952465057373, 157.19288635253906)\n",
      "epoch 3300 loss=(675.93115234375, 0.7364019155502319, 152.4242706298828)\n",
      "epoch 3400 loss=(648.5728149414062, 0.7262983918190002, 149.5953369140625)\n",
      "epoch 3500 loss=(638.021484375, 0.7078931331634521, 147.12501525878906)\n",
      "epoch 3600 loss=(616.393310546875, 0.6980041265487671, 143.28182983398438)\n",
      "epoch 3700 loss=(590.065673828125, 0.6790314316749573, 141.7996368408203)\n",
      "epoch 3800 loss=(570.9312744140625, 0.6778876781463623, 138.03453063964844)\n",
      "epoch 3900 loss=(551.0520629882812, 0.6588276624679565, 135.46383666992188)\n",
      "epoch 4000 loss=(523.3024291992188, 0.6514395475387573, 130.9902801513672)\n",
      "epoch 4100 loss=(509.4461669921875, 0.6358951330184937, 128.8802947998047)\n",
      "epoch 4200 loss=(492.684814453125, 0.6295005083084106, 124.28157043457031)\n",
      "epoch 4300 loss=(476.90460205078125, 0.6198554039001465, 123.49819946289062)\n",
      "epoch 4400 loss=(458.8307189941406, 0.6182191371917725, 120.69276428222656)\n",
      "epoch 4500 loss=(444.6417541503906, 0.6135751605033875, 119.303466796875)\n",
      "epoch 4600 loss=(430.19769287109375, 0.6021395921707153, 113.67716979980469)\n",
      "epoch 4700 loss=(404.7740478515625, 0.595667839050293, 110.7442398071289)\n",
      "epoch 4800 loss=(388.0579528808594, 0.5879997611045837, 108.12642669677734)\n",
      "epoch 4900 loss=(375.7277526855469, 0.5921589732170105, 104.18540954589844)\n",
      "epoch 5000 loss=(365.9670715332031, 0.5778301954269409, 101.00249481201172)\n",
      "epoch 5100 loss=(348.2874450683594, 0.5768588185310364, 99.1409912109375)\n",
      "epoch 5200 loss=(334.00115966796875, 0.5717025995254517, 93.12007141113281)\n",
      "epoch 5300 loss=(324.73895263671875, 0.5689248442649841, 93.16700744628906)\n",
      "epoch 5400 loss=(301.2376708984375, 0.5592143535614014, 89.4747085571289)\n",
      "epoch 5500 loss=(298.80120849609375, 0.5598929524421692, 86.44923400878906)\n",
      "epoch 5600 loss=(276.05426025390625, 0.5608974695205688, 85.86241912841797)\n",
      "epoch 5700 loss=(258.2489318847656, 0.5532397031784058, 81.11154174804688)\n",
      "epoch 5800 loss=(254.04464721679688, 0.5439546704292297, 76.65103149414062)\n",
      "epoch 5900 loss=(240.8266143798828, 0.545762300491333, 75.28812408447266)\n",
      "epoch 6000 loss=(232.32052612304688, 0.5387381315231323, 69.99317169189453)\n",
      "epoch 6100 loss=(226.4849395751953, 0.5405152440071106, 71.16368103027344)\n",
      "epoch 6200 loss=(207.70323181152344, 0.5297969579696655, 67.16980743408203)\n",
      "epoch 6300 loss=(212.5961151123047, 0.527010440826416, 62.745845794677734)\n",
      "epoch 6400 loss=(192.74249267578125, 0.5163976550102234, 61.533058166503906)\n",
      "epoch 6500 loss=(179.85276794433594, 0.5229002237319946, 58.967323303222656)\n",
      "epoch 6600 loss=(169.4547882080078, 0.5127127170562744, 57.479373931884766)\n",
      "epoch 6700 loss=(155.49937438964844, 0.5073236227035522, 53.43476486206055)\n",
      "epoch 6800 loss=(157.8961181640625, 0.49899789690971375, 53.422996520996094)\n",
      "epoch 6900 loss=(147.149169921875, 0.4915294945240021, 48.451560974121094)\n",
      "epoch 7000 loss=(134.1551971435547, 0.4852208197116852, 46.7003288269043)\n",
      "epoch 7100 loss=(135.29075622558594, 0.47406938672065735, 44.37083435058594)\n",
      "epoch 7200 loss=(118.8801040649414, 0.47639143466949463, 42.83480453491211)\n",
      "epoch 7300 loss=(114.28380584716797, 0.47232627868652344, 40.053043365478516)\n",
      "epoch 7400 loss=(105.01007843017578, 0.45599713921546936, 39.99063491821289)\n",
      "epoch 7500 loss=(102.2654800415039, 0.4598830044269562, 38.541988372802734)\n",
      "epoch 7600 loss=(102.86742401123047, 0.4442223310470581, 34.38760757446289)\n",
      "epoch 7700 loss=(97.6861572265625, 0.4353320598602295, 33.614341735839844)\n",
      "epoch 7800 loss=(85.56434631347656, 0.4316675066947937, 32.69768142700195)\n",
      "epoch 7900 loss=(81.9991226196289, 0.43267616629600525, 30.58854866027832)\n",
      "epoch 8000 loss=(79.63880157470703, 0.41590386629104614, 28.726917266845703)\n",
      "epoch 8100 loss=(72.9359359741211, 0.41321486234664917, 28.427221298217773)\n",
      "epoch 8200 loss=(73.66585540771484, 0.40016892552375793, 26.449350357055664)\n",
      "epoch 8300 loss=(67.60118103027344, 0.3964250087738037, 26.209768295288086)\n",
      "epoch 8400 loss=(71.80476379394531, 0.38661354780197144, 22.143171310424805)\n",
      "epoch 8500 loss=(63.522335052490234, 0.384111225605011, 21.73423957824707)\n",
      "epoch 8600 loss=(63.848243713378906, 0.3754696547985077, 22.49036979675293)\n",
      "epoch 8700 loss=(58.724281311035156, 0.3777378499507904, 21.062532424926758)\n",
      "epoch 8800 loss=(57.65067672729492, 0.36193785071372986, 21.59974479675293)\n",
      "epoch 8900 loss=(55.88421630859375, 0.36441588401794434, 18.232667922973633)\n",
      "epoch 9000 loss=(53.67005920410156, 0.3740755617618561, 18.781808853149414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9100 loss=(52.459285736083984, 0.3473733067512512, 17.13123893737793)\n",
      "epoch 9200 loss=(49.74173355102539, 0.341569721698761, 17.984127044677734)\n",
      "epoch 9300 loss=(51.241939544677734, 0.3404122292995453, 17.154590606689453)\n",
      "epoch 9400 loss=(45.33974838256836, 0.3313780426979065, 15.501279830932617)\n",
      "epoch 9500 loss=(45.94810104370117, 0.32785752415657043, 15.42088794708252)\n",
      "epoch 9600 loss=(45.03618240356445, 0.32154762744903564, 16.535114288330078)\n",
      "epoch 9700 loss=(39.319026947021484, 0.31972265243530273, 15.08818244934082)\n",
      "epoch 9800 loss=(39.65130615234375, 0.3171282112598419, 15.409366607666016)\n",
      "epoch 9900 loss=(40.8817138671875, 0.3042983114719391, 15.374292373657227)\n",
      "epoch 10000 loss=(43.09428405761719, 0.3037133812904358, 14.740116119384766)\n",
      "epoch 10100 loss=(43.621681213378906, 0.30262893438339233, 13.054230690002441)\n",
      "epoch 10200 loss=(39.65077590942383, 0.31282564997673035, 13.123016357421875)\n",
      "epoch 10300 loss=(40.84294509887695, 0.29497331380844116, 11.90457820892334)\n",
      "epoch 10400 loss=(38.33229446411133, 0.29035377502441406, 13.123542785644531)\n",
      "epoch 10500 loss=(40.420772552490234, 0.2827836275100708, 11.64928913116455)\n",
      "epoch 10600 loss=(37.263797760009766, 0.28164902329444885, 11.77442741394043)\n",
      "epoch 10700 loss=(38.82733917236328, 0.2781514823436737, 10.583413124084473)\n",
      "epoch 10800 loss=(37.49496841430664, 0.2720760405063629, 11.510744094848633)\n",
      "epoch 10900 loss=(41.67747116088867, 0.2688242495059967, 12.641478538513184)\n",
      "epoch 11000 loss=(34.91298294067383, 0.27804237604141235, 9.960522651672363)\n",
      "epoch 11100 loss=(40.31659698486328, 0.2612093687057495, 10.933189392089844)\n",
      "epoch 11200 loss=(40.1412353515625, 0.2638251483440399, 10.01766586303711)\n",
      "epoch 11300 loss=(38.2004280090332, 0.268117219209671, 11.243398666381836)\n",
      "epoch 11400 loss=(39.20787811279297, 0.253610223531723, 9.464723587036133)\n",
      "epoch 11500 loss=(35.608482360839844, 0.24424949288368225, 11.530851364135742)\n",
      "epoch 11600 loss=(37.83413314819336, 0.24524296820163727, 10.653299331665039)\n",
      "epoch 11700 loss=(34.18768310546875, 0.24620510637760162, 10.908731460571289)\n",
      "epoch 11800 loss=(33.48007583618164, 0.23927611112594604, 9.816370010375977)\n",
      "epoch 11900 loss=(34.815914154052734, 0.23671607673168182, 10.283350944519043)\n",
      "epoch 12000 loss=(37.03219985961914, 0.23667214810848236, 10.630298614501953)\n",
      "epoch 12100 loss=(39.23673629760742, 0.23501263558864594, 10.985710144042969)\n",
      "epoch 12200 loss=(35.22041702270508, 0.23170822858810425, 10.604328155517578)\n",
      "epoch 12300 loss=(39.945159912109375, 0.22474068403244019, 12.891535758972168)\n",
      "epoch 12400 loss=(34.44221115112305, 0.2226027250289917, 9.552458763122559)\n",
      "epoch 12500 loss=(37.55485153198242, 0.22319309413433075, 11.603630065917969)\n",
      "epoch 12600 loss=(41.527950286865234, 0.2230626344680786, 9.867244720458984)\n",
      "epoch 12700 loss=(36.297298431396484, 0.21590669453144073, 9.619369506835938)\n",
      "epoch 12800 loss=(35.42551803588867, 0.21696223318576813, 10.613326072692871)\n",
      "epoch 12900 loss=(35.0404052734375, 0.21368473768234253, 10.577302932739258)\n",
      "epoch 13000 loss=(32.75831604003906, 0.20734451711177826, 9.493805885314941)\n",
      "epoch 13100 loss=(38.54259490966797, 0.2071676105260849, 9.81946086883545)\n",
      "epoch 13200 loss=(37.47679901123047, 0.21742431819438934, 10.323371887207031)\n",
      "Early stopping\n",
      "Finished training\n",
      "3 (25.55489730834961, 0.8813559412956238, 15.037235260009766)\n",
      "epoch 0 loss=(34754.25390625, 0.6904222369194031, 10726.7587890625)\n",
      "epoch 100 loss=(4160.48486328125, 4.326782703399658, 951.8534545898438)\n",
      "epoch 200 loss=(1922.3792724609375, 3.7900564670562744, 274.7757873535156)\n",
      "epoch 300 loss=(1757.2864990234375, 4.4743499755859375, 260.890380859375)\n",
      "epoch 400 loss=(1617.481201171875, 5.040371894836426, 253.63243103027344)\n",
      "epoch 500 loss=(1504.4818115234375, 5.410943508148193, 249.38565063476562)\n",
      "epoch 600 loss=(1422.87255859375, 5.52827262878418, 246.3736572265625)\n",
      "epoch 700 loss=(1360.375244140625, 5.439060211181641, 243.9686279296875)\n",
      "epoch 800 loss=(1310.353515625, 5.177940368652344, 241.71539306640625)\n",
      "epoch 900 loss=(1272.4053955078125, 4.745594024658203, 239.53900146484375)\n",
      "epoch 1000 loss=(1243.4700927734375, 4.1653008460998535, 237.4207763671875)\n",
      "epoch 1100 loss=(1218.4588623046875, 3.4532206058502197, 235.23411560058594)\n",
      "epoch 1200 loss=(1196.6329345703125, 2.701751470565796, 233.33018493652344)\n",
      "epoch 1300 loss=(1176.7091064453125, 1.9222915172576904, 230.15304565429688)\n",
      "epoch 1400 loss=(1159.5999755859375, 1.2041679620742798, 227.9990997314453)\n",
      "epoch 1500 loss=(1143.530029296875, 0.7609997987747192, 225.4300994873047)\n",
      "epoch 1600 loss=(1124.3399658203125, 0.6651142835617065, 222.42181396484375)\n",
      "epoch 1700 loss=(1106.5068359375, 0.650275468826294, 219.87362670898438)\n",
      "epoch 1800 loss=(1087.9129638671875, 0.6456657648086548, 216.8690643310547)\n",
      "epoch 1900 loss=(1070.1226806640625, 0.6435407400131226, 214.77066040039062)\n",
      "epoch 2000 loss=(1049.7183837890625, 0.6406680345535278, 211.36337280273438)\n",
      "epoch 2100 loss=(1031.537353515625, 0.6373811364173889, 208.4642791748047)\n",
      "epoch 2200 loss=(1010.9111938476562, 0.6406934857368469, 205.5517578125)\n",
      "epoch 2300 loss=(993.917236328125, 0.6278154850006104, 203.52047729492188)\n",
      "epoch 2400 loss=(971.89453125, 0.6314262747764587, 200.3730926513672)\n",
      "epoch 2500 loss=(952.2451782226562, 0.6316068768501282, 197.8697967529297)\n",
      "epoch 2600 loss=(931.739013671875, 0.6271253228187561, 194.55316162109375)\n",
      "epoch 2700 loss=(907.4050903320312, 0.6185939311981201, 191.7617950439453)\n",
      "epoch 2800 loss=(892.0399780273438, 0.6159663796424866, 188.9561767578125)\n",
      "epoch 2900 loss=(861.3386840820312, 0.6146566271781921, 185.80206298828125)\n",
      "epoch 3000 loss=(843.2333984375, 0.603967547416687, 182.4296875)\n",
      "epoch 3100 loss=(819.9892578125, 0.6011250019073486, 180.35647583007812)\n",
      "epoch 3200 loss=(796.8627319335938, 0.6001498103141785, 177.37876892089844)\n",
      "epoch 3300 loss=(774.5455322265625, 0.589581310749054, 174.1311492919922)\n",
      "epoch 3400 loss=(756.0454711914062, 0.5860707759857178, 172.43264770507812)\n",
      "epoch 3500 loss=(730.830078125, 0.5838266611099243, 168.62704467773438)\n",
      "epoch 3600 loss=(703.636474609375, 0.5786075592041016, 164.6900634765625)\n",
      "epoch 3700 loss=(686.0770874023438, 0.570661187171936, 162.52955627441406)\n",
      "epoch 3800 loss=(662.6951293945312, 0.5702402591705322, 160.0655059814453)\n",
      "epoch 3900 loss=(645.917724609375, 0.5597478747367859, 156.8950958251953)\n",
      "epoch 4000 loss=(625.7250366210938, 0.5667211413383484, 153.11988830566406)\n",
      "epoch 4100 loss=(597.8616333007812, 0.5590327382087708, 150.49708557128906)\n",
      "epoch 4200 loss=(577.168212890625, 0.5600817799568176, 148.20541381835938)\n",
      "epoch 4300 loss=(559.2005615234375, 0.5508821606636047, 144.72415161132812)\n",
      "epoch 4400 loss=(543.2869873046875, 0.5453429818153381, 139.70619201660156)\n",
      "epoch 4500 loss=(523.2572631835938, 0.5395669341087341, 138.15675354003906)\n",
      "epoch 4600 loss=(505.6709899902344, 0.5366877317428589, 133.2417755126953)\n",
      "epoch 4700 loss=(486.7520751953125, 0.5375150442123413, 130.79710388183594)\n",
      "epoch 4800 loss=(466.4288330078125, 0.5297451615333557, 126.55036163330078)\n",
      "epoch 4900 loss=(452.484619140625, 0.5297276377677917, 125.17414093017578)\n",
      "epoch 5000 loss=(435.9654846191406, 0.520440936088562, 120.95865631103516)\n",
      "epoch 5100 loss=(410.6609191894531, 0.5198086500167847, 116.36311340332031)\n",
      "epoch 5200 loss=(401.1783752441406, 0.5147671103477478, 114.25269317626953)\n",
      "epoch 5300 loss=(385.82574462890625, 0.5075887441635132, 110.95758819580078)\n",
      "epoch 5400 loss=(365.6136779785156, 0.5064547657966614, 105.59423065185547)\n",
      "epoch 5500 loss=(351.5025634765625, 0.5034065246582031, 104.6588363647461)\n",
      "epoch 5600 loss=(342.6401062011719, 0.5002520084381104, 99.49383544921875)\n",
      "epoch 5700 loss=(324.9825134277344, 0.4934443235397339, 96.14340209960938)\n",
      "epoch 5800 loss=(317.2119140625, 0.48899853229522705, 93.99186706542969)\n",
      "epoch 5900 loss=(297.3154602050781, 0.48278945684432983, 89.7354736328125)\n",
      "epoch 6000 loss=(281.1418151855469, 0.48059362173080444, 88.58067321777344)\n",
      "epoch 6100 loss=(266.94781494140625, 0.48008838295936584, 82.50010681152344)\n",
      "epoch 6200 loss=(255.93972778320312, 0.47602689266204834, 80.15965270996094)\n",
      "epoch 6300 loss=(243.33926391601562, 0.46804291009902954, 78.78294372558594)\n",
      "epoch 6400 loss=(228.7258758544922, 0.46682581305503845, 73.07412719726562)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6500 loss=(221.4879150390625, 0.4582119286060333, 72.70441436767578)\n",
      "epoch 6600 loss=(210.7046661376953, 0.45552390813827515, 67.57713317871094)\n",
      "epoch 6700 loss=(202.5347442626953, 0.45515304803848267, 63.64442825317383)\n",
      "epoch 6800 loss=(186.73574829101562, 0.4476466774940491, 60.99814987182617)\n",
      "epoch 6900 loss=(169.9232177734375, 0.44102004170417786, 59.19126510620117)\n",
      "epoch 7000 loss=(165.4702911376953, 0.44144436717033386, 56.424346923828125)\n",
      "epoch 7100 loss=(157.34266662597656, 0.4271910488605499, 54.35063934326172)\n",
      "epoch 7200 loss=(146.75527954101562, 0.4398230314254761, 52.3759880065918)\n",
      "epoch 7300 loss=(142.9299774169922, 0.4293438196182251, 48.31447982788086)\n",
      "epoch 7400 loss=(134.74989318847656, 0.4220675826072693, 47.08094024658203)\n",
      "epoch 7500 loss=(126.98931884765625, 0.4304651618003845, 43.406280517578125)\n",
      "epoch 7600 loss=(117.67220306396484, 0.4134591221809387, 41.15968704223633)\n",
      "epoch 7700 loss=(112.11023712158203, 0.41209691762924194, 39.28480911254883)\n",
      "epoch 7800 loss=(111.54743957519531, 0.4077589809894562, 37.18647003173828)\n",
      "epoch 7900 loss=(101.2033920288086, 0.40943482518196106, 36.69365692138672)\n",
      "epoch 8000 loss=(93.5140151977539, 0.3980758786201477, 33.08769226074219)\n",
      "epoch 8100 loss=(89.11425018310547, 0.4025622308254242, 32.63511276245117)\n",
      "epoch 8200 loss=(90.85733032226562, 0.41083502769470215, 32.03036880493164)\n",
      "epoch 8300 loss=(82.0822525024414, 0.3972458839416504, 29.283390045166016)\n",
      "epoch 8400 loss=(77.6286849975586, 0.39683690667152405, 28.07806396484375)\n",
      "epoch 8500 loss=(73.62266540527344, 0.3879894018173218, 27.909303665161133)\n",
      "epoch 8600 loss=(72.97638702392578, 0.3862471580505371, 23.77241325378418)\n",
      "epoch 8700 loss=(68.9809799194336, 0.37491583824157715, 23.742984771728516)\n",
      "epoch 8800 loss=(68.20287322998047, 0.37530115246772766, 24.021352767944336)\n",
      "epoch 8900 loss=(61.93657302856445, 0.37195441126823425, 22.43630027770996)\n",
      "epoch 9000 loss=(61.94482421875, 0.36706575751304626, 20.7440128326416)\n",
      "epoch 9100 loss=(64.37744140625, 0.3565707206726074, 20.53540802001953)\n",
      "epoch 9200 loss=(61.232818603515625, 0.35355255007743835, 18.63504409790039)\n",
      "epoch 9300 loss=(58.233970642089844, 0.3467388153076172, 18.530054092407227)\n",
      "epoch 9400 loss=(57.15366744995117, 0.362673819065094, 18.437515258789062)\n",
      "epoch 9500 loss=(47.421669006347656, 0.34047797322273254, 16.642990112304688)\n",
      "epoch 9600 loss=(54.27002716064453, 0.3489716053009033, 16.479394912719727)\n",
      "epoch 9700 loss=(43.07511520385742, 0.33409297466278076, 17.612863540649414)\n",
      "epoch 9800 loss=(48.94171905517578, 0.3325614929199219, 15.466767311096191)\n",
      "epoch 9900 loss=(44.76445388793945, 0.333373486995697, 15.215219497680664)\n",
      "epoch 10000 loss=(49.41823196411133, 0.3278402090072632, 16.127370834350586)\n",
      "epoch 10100 loss=(43.77705764770508, 0.32503724098205566, 14.5720796585083)\n",
      "epoch 10200 loss=(44.702877044677734, 0.3139669597148895, 14.863301277160645)\n",
      "epoch 10300 loss=(41.04825973510742, 0.33271273970603943, 12.716513633728027)\n",
      "epoch 10400 loss=(39.42147445678711, 0.30704763531684875, 12.359118461608887)\n",
      "epoch 10500 loss=(42.02426528930664, 0.30844512581825256, 14.715110778808594)\n",
      "epoch 10600 loss=(40.93895721435547, 0.3018764853477478, 12.37441349029541)\n",
      "epoch 10700 loss=(37.713050842285156, 0.29153817892074585, 12.365289688110352)\n",
      "epoch 10800 loss=(37.79888153076172, 0.29304516315460205, 12.87878131866455)\n",
      "epoch 10900 loss=(35.48722457885742, 0.29482272267341614, 12.827959060668945)\n",
      "epoch 11000 loss=(40.386924743652344, 0.27898073196411133, 12.750980377197266)\n",
      "epoch 11100 loss=(39.34003448486328, 0.2846050560474396, 13.728967666625977)\n",
      "epoch 11200 loss=(31.269495010375977, 0.2810609042644501, 11.677224159240723)\n",
      "epoch 11300 loss=(33.92285919189453, 0.2735656201839447, 12.214022636413574)\n",
      "epoch 11400 loss=(35.4994010925293, 0.2737099230289459, 12.25052261352539)\n",
      "epoch 11500 loss=(34.90158462524414, 0.2690848708152771, 11.102572441101074)\n",
      "epoch 11600 loss=(34.58265686035156, 0.25975117087364197, 12.141154289245605)\n",
      "epoch 11700 loss=(40.398773193359375, 0.27742528915405273, 10.464296340942383)\n",
      "epoch 11800 loss=(34.98884201049805, 0.26254260540008545, 10.186103820800781)\n",
      "epoch 11900 loss=(35.63691711425781, 0.2554134130477905, 10.760579109191895)\n",
      "epoch 12000 loss=(35.972476959228516, 0.261485755443573, 12.72315502166748)\n",
      "epoch 12100 loss=(37.90938949584961, 0.24882619082927704, 11.714926719665527)\n",
      "epoch 12200 loss=(35.79568862915039, 0.2516457140445709, 8.855414390563965)\n",
      "epoch 12300 loss=(37.015159606933594, 0.25221166014671326, 10.926994323730469)\n",
      "epoch 12400 loss=(28.92186737060547, 0.23856942355632782, 11.684115409851074)\n",
      "epoch 12500 loss=(35.98030471801758, 0.23956707119941711, 10.974431991577148)\n",
      "epoch 12600 loss=(32.16201400756836, 0.2428467720746994, 11.112740516662598)\n",
      "epoch 12700 loss=(35.55573272705078, 0.23476311564445496, 11.116228103637695)\n",
      "epoch 12800 loss=(36.500205993652344, 0.24043285846710205, 9.719666481018066)\n",
      "epoch 12900 loss=(39.306800842285156, 0.24861331284046173, 11.149739265441895)\n",
      "epoch 13000 loss=(36.52745819091797, 0.2330375611782074, 10.599984169006348)\n",
      "epoch 13100 loss=(34.64430618286133, 0.2212047129869461, 11.125168800354004)\n",
      "epoch 13200 loss=(28.99230194091797, 0.2245652824640274, 10.58367919921875)\n",
      "epoch 13300 loss=(37.94319152832031, 0.23245254158973694, 10.076003074645996)\n",
      "epoch 13400 loss=(30.74115562438965, 0.22115078568458557, 10.381095886230469)\n",
      "epoch 13500 loss=(32.31167221069336, 0.21149210631847382, 11.296968460083008)\n",
      "epoch 13600 loss=(31.48920249938965, 0.20960864424705505, 9.68560791015625)\n",
      "Early stopping\n",
      "Finished training\n",
      "4 (25.010723114013672, 0.7966101765632629, 13.383376121520996)\n",
      "epoch 0 loss=(34122.6015625, 0.6969277858734131, 10829.142578125)\n",
      "epoch 100 loss=(3303.413818359375, 13.841703414916992, 1092.4766845703125)\n",
      "epoch 200 loss=(1918.904541015625, 7.245641231536865, 282.1447448730469)\n",
      "epoch 300 loss=(1695.5767822265625, 0.7010160088539124, 284.37884521484375)\n",
      "epoch 400 loss=(1552.780029296875, 0.7563497424125671, 288.36187744140625)\n",
      "epoch 500 loss=(1448.96142578125, 0.9119060635566711, 285.708740234375)\n",
      "epoch 600 loss=(1374.4332275390625, 1.1086976528167725, 276.9353332519531)\n",
      "epoch 700 loss=(1312.5443115234375, 1.2787734270095825, 265.3820495605469)\n",
      "epoch 800 loss=(1264.4512939453125, 1.3916475772857666, 255.3113250732422)\n",
      "epoch 900 loss=(1226.684326171875, 1.450223445892334, 246.03903198242188)\n",
      "epoch 1000 loss=(1193.3421630859375, 1.4512016773223877, 238.07957458496094)\n",
      "epoch 1100 loss=(1165.41455078125, 1.3879272937774658, 231.77047729492188)\n",
      "epoch 1200 loss=(1138.5330810546875, 1.2959601879119873, 227.36007690429688)\n",
      "epoch 1300 loss=(1111.9989013671875, 1.1691337823867798, 223.19393920898438)\n",
      "epoch 1400 loss=(1088.6260986328125, 1.0337713956832886, 220.31358337402344)\n",
      "epoch 1500 loss=(1067.2677001953125, 0.90044105052948, 217.27337646484375)\n",
      "epoch 1600 loss=(1040.654296875, 0.8032989501953125, 214.4043731689453)\n",
      "epoch 1700 loss=(1018.2412109375, 0.7370957136154175, 212.16043090820312)\n",
      "epoch 1800 loss=(994.7885131835938, 0.7117223739624023, 209.8954315185547)\n",
      "epoch 1900 loss=(972.3997192382812, 0.696201503276825, 207.4597930908203)\n",
      "epoch 2000 loss=(949.4570922851562, 0.7017391920089722, 204.81533813476562)\n",
      "epoch 2100 loss=(925.8619384765625, 0.6966517567634583, 203.4314727783203)\n",
      "epoch 2200 loss=(899.7611083984375, 0.6994651556015015, 200.5001220703125)\n",
      "epoch 2300 loss=(877.0071411132812, 0.7019794583320618, 198.67025756835938)\n",
      "epoch 2400 loss=(854.6339111328125, 0.7038615942001343, 197.1686553955078)\n",
      "epoch 2500 loss=(831.55126953125, 0.7013100981712341, 195.55697631835938)\n",
      "epoch 2600 loss=(803.9683837890625, 0.6973962783813477, 193.17477416992188)\n",
      "epoch 2700 loss=(788.1746215820312, 0.691243588924408, 190.85592651367188)\n",
      "epoch 2800 loss=(764.3718872070312, 0.6926084160804749, 188.377685546875)\n",
      "epoch 2900 loss=(738.8525390625, 0.6915963292121887, 186.47720336914062)\n",
      "epoch 3000 loss=(714.615234375, 0.6865518689155579, 183.06045532226562)\n",
      "epoch 3100 loss=(687.585693359375, 0.6825037598609924, 183.32504272460938)\n",
      "epoch 3200 loss=(671.4356689453125, 0.6802621483802795, 178.58729553222656)\n",
      "epoch 3300 loss=(650.32080078125, 0.6750534772872925, 174.9413604736328)\n",
      "epoch 3400 loss=(629.2246704101562, 0.669747531414032, 173.41529846191406)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3500 loss=(605.090576171875, 0.6698721647262573, 172.4866485595703)\n",
      "epoch 3600 loss=(593.9544067382812, 0.667195737361908, 166.6100616455078)\n",
      "epoch 3700 loss=(569.49609375, 0.6623127460479736, 166.15782165527344)\n",
      "epoch 3800 loss=(544.2706298828125, 0.6596415638923645, 161.92617797851562)\n",
      "epoch 3900 loss=(526.5192260742188, 0.6583424210548401, 158.31175231933594)\n",
      "epoch 4000 loss=(504.8553466796875, 0.6520377993583679, 155.9981231689453)\n",
      "epoch 4100 loss=(481.1515197753906, 0.6538997888565063, 152.1075897216797)\n",
      "epoch 4200 loss=(469.56591796875, 0.6512361764907837, 148.94607543945312)\n",
      "epoch 4300 loss=(451.828369140625, 0.645906388759613, 146.04287719726562)\n",
      "epoch 4400 loss=(434.7247314453125, 0.6463437080383301, 142.21258544921875)\n",
      "epoch 4500 loss=(415.01556396484375, 0.6480115056037903, 139.2886199951172)\n",
      "epoch 4600 loss=(396.3794860839844, 0.6460322141647339, 136.4446258544922)\n",
      "epoch 4700 loss=(372.56756591796875, 0.6401165127754211, 133.8375244140625)\n",
      "epoch 4800 loss=(363.71514892578125, 0.6372405290603638, 127.8286361694336)\n",
      "epoch 4900 loss=(348.94171142578125, 0.6408368945121765, 126.23815155029297)\n",
      "epoch 5000 loss=(325.97747802734375, 0.6321128606796265, 124.16802978515625)\n",
      "epoch 5100 loss=(319.2906799316406, 0.6288343071937561, 117.304931640625)\n",
      "epoch 5200 loss=(295.1289367675781, 0.6328153014183044, 114.19598388671875)\n",
      "epoch 5300 loss=(294.22979736328125, 0.6279472708702087, 111.1457290649414)\n",
      "epoch 5400 loss=(277.8659362792969, 0.6192315220832825, 107.91311645507812)\n",
      "epoch 5500 loss=(262.39520263671875, 0.619240403175354, 102.93014526367188)\n",
      "epoch 5600 loss=(244.01885986328125, 0.6172361373901367, 101.59452819824219)\n",
      "epoch 5700 loss=(229.58192443847656, 0.6164539456367493, 97.40486907958984)\n",
      "epoch 5800 loss=(221.3406219482422, 0.6133579611778259, 93.6197509765625)\n",
      "epoch 5900 loss=(207.00982666015625, 0.6066697239875793, 90.20249938964844)\n",
      "epoch 6000 loss=(194.49378967285156, 0.6018475890159607, 86.61053466796875)\n",
      "epoch 6100 loss=(190.01312255859375, 0.601374089717865, 84.67781829833984)\n",
      "epoch 6200 loss=(167.54173278808594, 0.5963574647903442, 80.08870697021484)\n",
      "epoch 6300 loss=(163.45806884765625, 0.5923393964767456, 76.23426055908203)\n",
      "epoch 6400 loss=(152.69317626953125, 0.5825083255767822, 72.00434875488281)\n",
      "epoch 6500 loss=(146.63558959960938, 0.5855880975723267, 68.91092681884766)\n",
      "epoch 6600 loss=(140.4888916015625, 0.5717208385467529, 64.96772003173828)\n",
      "epoch 6700 loss=(135.53182983398438, 0.5699198842048645, 64.96956634521484)\n",
      "epoch 6800 loss=(122.89393615722656, 0.568026065826416, 61.124351501464844)\n",
      "epoch 6900 loss=(113.96726989746094, 0.5522142052650452, 58.11858367919922)\n",
      "epoch 7000 loss=(107.7020034790039, 0.5562849640846252, 54.9959716796875)\n",
      "epoch 7100 loss=(100.01655578613281, 0.5450994968414307, 51.86793518066406)\n",
      "epoch 7200 loss=(97.81330871582031, 0.5454655289649963, 49.093448638916016)\n",
      "epoch 7300 loss=(88.07720947265625, 0.538281261920929, 44.81531524658203)\n",
      "epoch 7400 loss=(83.63226318359375, 0.5289366841316223, 44.117523193359375)\n",
      "epoch 7500 loss=(78.45549011230469, 0.5199508666992188, 42.966270446777344)\n",
      "epoch 7600 loss=(79.59584045410156, 0.5141208171844482, 40.58952331542969)\n",
      "epoch 7700 loss=(70.06459045410156, 0.5141476988792419, 38.10039520263672)\n",
      "epoch 7800 loss=(66.82392120361328, 0.5057069063186646, 36.462379455566406)\n",
      "epoch 7900 loss=(67.58443450927734, 0.5000767111778259, 33.95486068725586)\n",
      "epoch 8000 loss=(65.37950897216797, 0.49615800380706787, 30.43228530883789)\n",
      "epoch 8100 loss=(61.05784225463867, 0.4914013147354126, 29.832246780395508)\n",
      "epoch 8200 loss=(59.68277359008789, 0.48182958364486694, 29.8271427154541)\n",
      "epoch 8300 loss=(57.0298957824707, 0.4823300838470459, 28.335386276245117)\n",
      "epoch 8400 loss=(54.61882781982422, 0.4742860198020935, 25.682504653930664)\n",
      "epoch 8500 loss=(52.84148406982422, 0.4734331965446472, 24.65955924987793)\n",
      "epoch 8600 loss=(49.74118423461914, 0.4566790461540222, 21.57929229736328)\n",
      "epoch 8700 loss=(45.475128173828125, 0.45842698216438293, 22.210107803344727)\n",
      "epoch 8800 loss=(48.529869079589844, 0.45788437128067017, 21.570465087890625)\n",
      "epoch 8900 loss=(47.65577697753906, 0.4499298632144928, 19.962261199951172)\n",
      "epoch 9000 loss=(43.50806427001953, 0.4444052577018738, 19.841703414916992)\n",
      "epoch 9100 loss=(39.54191589355469, 0.43216902017593384, 18.317340850830078)\n",
      "epoch 9200 loss=(40.22670364379883, 0.4377138018608093, 18.58294677734375)\n",
      "epoch 9300 loss=(41.62080001831055, 0.4353238344192505, 16.614599227905273)\n",
      "epoch 9400 loss=(39.27473831176758, 0.4433158338069916, 17.478017807006836)\n",
      "epoch 9500 loss=(41.34076690673828, 0.42934879660606384, 14.218761444091797)\n",
      "epoch 9600 loss=(37.27811813354492, 0.4177766442298889, 16.689929962158203)\n",
      "epoch 9700 loss=(36.527523040771484, 0.41643035411834717, 15.450952529907227)\n",
      "epoch 9800 loss=(42.708953857421875, 0.4123849868774414, 15.435711860656738)\n",
      "epoch 9900 loss=(42.72720718383789, 0.4011099934577942, 13.342296600341797)\n",
      "epoch 10000 loss=(35.699790954589844, 0.4072772264480591, 14.230884552001953)\n",
      "epoch 10100 loss=(36.034645080566406, 0.3994613289833069, 12.354076385498047)\n",
      "epoch 10200 loss=(31.85723114013672, 0.39549100399017334, 12.781562805175781)\n",
      "epoch 10300 loss=(32.87844467163086, 0.39996179938316345, 11.804298400878906)\n",
      "epoch 10400 loss=(35.24151611328125, 0.39578890800476074, 12.339107513427734)\n",
      "epoch 10500 loss=(35.48633575439453, 0.3959675133228302, 12.620932579040527)\n",
      "epoch 10600 loss=(33.93827438354492, 0.38013216853141785, 11.738201141357422)\n",
      "epoch 10700 loss=(37.01375961303711, 0.36902663111686707, 11.653660774230957)\n",
      "epoch 10800 loss=(35.73974609375, 0.3700648546218872, 12.124493598937988)\n",
      "epoch 10900 loss=(30.82973861694336, 0.3759174048900604, 12.300358772277832)\n",
      "epoch 11000 loss=(34.21977615356445, 0.3793432414531708, 11.739367485046387)\n",
      "epoch 11100 loss=(35.29836654663086, 0.36802423000335693, 9.943238258361816)\n",
      "epoch 11200 loss=(35.32440185546875, 0.3599231243133545, 10.9160795211792)\n",
      "epoch 11300 loss=(32.61738967895508, 0.3594858646392822, 10.64477252960205)\n",
      "epoch 11400 loss=(34.560943603515625, 0.35934633016586304, 12.346510887145996)\n",
      "epoch 11500 loss=(33.61979293823242, 0.3525812327861786, 11.00684642791748)\n",
      "epoch 11600 loss=(34.58517837524414, 0.34448888897895813, 11.256345748901367)\n",
      "epoch 11700 loss=(33.51609802246094, 0.34564802050590515, 11.613561630249023)\n",
      "epoch 11800 loss=(31.791059494018555, 0.3329501450061798, 10.474108695983887)\n",
      "epoch 11900 loss=(32.49430465698242, 0.33600860834121704, 11.864744186401367)\n",
      "epoch 12000 loss=(35.0594367980957, 0.34788239002227783, 11.09451675415039)\n",
      "epoch 12100 loss=(29.257976531982422, 0.32437580823898315, 11.208513259887695)\n",
      "epoch 12200 loss=(33.50385665893555, 0.3333929479122162, 11.118892669677734)\n",
      "epoch 12300 loss=(37.9195442199707, 0.3308843672275543, 11.093405723571777)\n",
      "epoch 12400 loss=(34.7269287109375, 0.33716723322868347, 9.872312545776367)\n",
      "epoch 12500 loss=(34.65925598144531, 0.3238167464733124, 10.04336166381836)\n",
      "epoch 12600 loss=(36.32682800292969, 0.32104426622390747, 11.36269474029541)\n",
      "epoch 12700 loss=(35.772640228271484, 0.3154502213001251, 11.19754409790039)\n",
      "epoch 12800 loss=(34.62468719482422, 0.3168276846408844, 9.95316219329834)\n",
      "epoch 12900 loss=(31.044208526611328, 0.3056384027004242, 10.689618110656738)\n",
      "epoch 13000 loss=(34.25481414794922, 0.31219956278800964, 11.061911582946777)\n",
      "epoch 13100 loss=(34.75009536743164, 0.30007803440093994, 10.143940925598145)\n",
      "epoch 13200 loss=(32.27494430541992, 0.30783575773239136, 11.098179817199707)\n",
      "epoch 13300 loss=(34.964962005615234, 0.30106812715530396, 11.19697093963623)\n",
      "epoch 13400 loss=(34.57950210571289, 0.300937294960022, 9.95856761932373)\n",
      "epoch 13500 loss=(35.441749572753906, 0.29409804940223694, 10.80424976348877)\n",
      "epoch 13600 loss=(35.81524658203125, 0.2984643280506134, 9.958341598510742)\n",
      "epoch 13700 loss=(31.607879638671875, 0.2861449122428894, 10.010336875915527)\n",
      "epoch 13800 loss=(34.01589584350586, 0.2933274209499359, 9.181154251098633)\n",
      "epoch 13900 loss=(33.4755744934082, 0.29528549313545227, 11.063770294189453)\n",
      "epoch 14000 loss=(32.76865768432617, 0.2880198061466217, 9.654767990112305)\n",
      "epoch 14100 loss=(37.05421829223633, 0.2761637270450592, 11.617240905761719)\n",
      "epoch 14200 loss=(29.641721725463867, 0.2732328474521637, 9.177105903625488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14300 loss=(30.149717330932617, 0.2800443470478058, 9.977638244628906)\n",
      "epoch 14400 loss=(34.95531463623047, 0.2868320345878601, 10.662153244018555)\n",
      "epoch 14500 loss=(34.75075149536133, 0.2719411253929138, 10.770304679870605)\n",
      "epoch 14600 loss=(37.492774963378906, 0.27594897150993347, 9.312315940856934)\n",
      "epoch 14700 loss=(31.075557708740234, 0.27125436067581177, 10.361973762512207)\n",
      "epoch 14800 loss=(31.213512420654297, 0.2652588486671448, 10.3928804397583)\n",
      "epoch 14900 loss=(29.805927276611328, 0.2654329240322113, 10.699374198913574)\n",
      "epoch 14999 loss=(30.595930099487305, 0.2614014446735382, 11.001111030578613)\n",
      "Finished training\n",
      "5 (22.267778396606445, 0.8135592937469482, 13.721420288085938)\n",
      "epoch 0 loss=(34216.4375, 0.7088958024978638, 10780.791015625)\n",
      "epoch 100 loss=(3726.66845703125, 1.4443830251693726, 1384.2283935546875)\n",
      "epoch 200 loss=(1897.245849609375, 1.258518934249878, 266.4745788574219)\n",
      "epoch 300 loss=(1772.355224609375, 1.0275307893753052, 248.54696655273438)\n",
      "epoch 400 loss=(1654.603515625, 0.7974655628204346, 246.64524841308594)\n",
      "epoch 500 loss=(1552.8818359375, 0.7364094853401184, 244.98236083984375)\n",
      "epoch 600 loss=(1470.7523193359375, 0.7780001759529114, 243.85150146484375)\n",
      "epoch 700 loss=(1403.45068359375, 0.8155405521392822, 242.58053588867188)\n",
      "epoch 800 loss=(1352.2454833984375, 0.8167637586593628, 241.60733032226562)\n",
      "epoch 900 loss=(1312.0555419921875, 0.8202545046806335, 240.66859436035156)\n",
      "epoch 1000 loss=(1282.8055419921875, 0.8263660073280334, 239.701171875)\n",
      "epoch 1100 loss=(1258.3873291015625, 0.8464148640632629, 238.66151428222656)\n",
      "epoch 1200 loss=(1238.054443359375, 0.869094729423523, 237.33006286621094)\n",
      "epoch 1300 loss=(1218.956298828125, 0.8868799209594727, 236.21945190429688)\n",
      "epoch 1400 loss=(1200.9339599609375, 0.9057939052581787, 235.0453643798828)\n",
      "epoch 1500 loss=(1182.6824951171875, 0.9362610578536987, 233.63624572753906)\n",
      "epoch 1600 loss=(1164.9482421875, 0.9498655796051025, 232.260498046875)\n",
      "epoch 1700 loss=(1144.3270263671875, 0.9747042655944824, 230.83958435058594)\n",
      "epoch 1800 loss=(1126.8642578125, 0.9786601066589355, 229.306884765625)\n",
      "epoch 1900 loss=(1106.206298828125, 0.9997054934501648, 227.80740356445312)\n",
      "epoch 2000 loss=(1085.625, 1.0077887773513794, 225.94290161132812)\n",
      "epoch 2100 loss=(1064.580810546875, 1.0101929903030396, 224.70387268066406)\n",
      "epoch 2200 loss=(1044.1893310546875, 1.004135251045227, 222.7107696533203)\n",
      "epoch 2300 loss=(1022.9210205078125, 1.0038912296295166, 221.4998016357422)\n",
      "epoch 2400 loss=(1000.1754760742188, 1.0037933588027954, 219.3397674560547)\n",
      "epoch 2500 loss=(979.5647583007812, 0.9893820881843567, 217.73019409179688)\n",
      "epoch 2600 loss=(955.2424926757812, 0.9854105710983276, 215.4574432373047)\n",
      "epoch 2700 loss=(935.1591796875, 0.9731900095939636, 214.04969787597656)\n",
      "epoch 2800 loss=(906.808837890625, 0.9538707137107849, 211.63699340820312)\n",
      "epoch 2900 loss=(884.9798583984375, 0.9421383738517761, 210.16432189941406)\n",
      "epoch 3000 loss=(865.7352905273438, 0.9345449209213257, 207.70501708984375)\n",
      "epoch 3100 loss=(840.6141357421875, 0.9216282367706299, 205.7201690673828)\n",
      "epoch 3200 loss=(815.6806640625, 0.9105808734893799, 204.49964904785156)\n",
      "epoch 3300 loss=(789.573486328125, 0.8990015983581543, 201.64077758789062)\n",
      "epoch 3400 loss=(768.8031616210938, 0.8852965235710144, 199.88232421875)\n",
      "epoch 3500 loss=(743.6738891601562, 0.8631671667098999, 197.9193878173828)\n",
      "epoch 3600 loss=(720.683837890625, 0.850852370262146, 195.69906616210938)\n",
      "epoch 3700 loss=(697.335693359375, 0.8337128162384033, 193.31524658203125)\n",
      "epoch 3800 loss=(673.9468994140625, 0.8112949728965759, 190.12362670898438)\n",
      "epoch 3900 loss=(653.321533203125, 0.7934325337409973, 188.28707885742188)\n",
      "epoch 4000 loss=(628.2098388671875, 0.7717620134353638, 186.25018310546875)\n",
      "epoch 4100 loss=(605.426513671875, 0.7413349747657776, 183.98309326171875)\n",
      "epoch 4200 loss=(585.1618041992188, 0.7238045930862427, 181.47474670410156)\n",
      "epoch 4300 loss=(559.6384887695312, 0.7107906937599182, 178.94505310058594)\n",
      "epoch 4400 loss=(539.6983032226562, 0.6912088394165039, 176.71376037597656)\n",
      "epoch 4500 loss=(518.7151489257812, 0.6723387241363525, 173.82876586914062)\n",
      "epoch 4600 loss=(508.72845458984375, 0.6547675728797913, 170.31556701660156)\n",
      "epoch 4700 loss=(487.66082763671875, 0.6409817337989807, 167.6614532470703)\n",
      "epoch 4800 loss=(467.7112731933594, 0.6314691305160522, 165.52951049804688)\n",
      "epoch 4900 loss=(456.47955322265625, 0.625311553478241, 162.91810607910156)\n",
      "epoch 5000 loss=(425.1957092285156, 0.6120333075523376, 159.9085235595703)\n",
      "epoch 5100 loss=(415.6654357910156, 0.6053484678268433, 157.53781127929688)\n",
      "epoch 5200 loss=(401.19903564453125, 0.5977326035499573, 153.85733032226562)\n",
      "epoch 5300 loss=(382.9052734375, 0.5892707705497742, 150.38211059570312)\n",
      "epoch 5400 loss=(364.3671569824219, 0.5868312120437622, 146.4302520751953)\n",
      "epoch 5500 loss=(344.4674072265625, 0.5813233256340027, 143.41493225097656)\n",
      "epoch 5600 loss=(337.989990234375, 0.5739986896514893, 140.38302612304688)\n",
      "epoch 5700 loss=(319.5191650390625, 0.5667580366134644, 138.0535125732422)\n",
      "epoch 5800 loss=(302.1808166503906, 0.560346245765686, 135.82640075683594)\n",
      "epoch 5900 loss=(286.1467590332031, 0.5527234077453613, 131.65972900390625)\n",
      "epoch 6000 loss=(276.15325927734375, 0.5441047549247742, 126.95108032226562)\n",
      "epoch 6100 loss=(263.0608215332031, 0.5373240113258362, 123.86727905273438)\n",
      "epoch 6200 loss=(256.63134765625, 0.5315526723861694, 120.9502944946289)\n",
      "epoch 6300 loss=(239.06166076660156, 0.5259432196617126, 116.02030944824219)\n",
      "epoch 6400 loss=(233.4710693359375, 0.5207692980766296, 111.88599395751953)\n",
      "epoch 6500 loss=(217.15557861328125, 0.5142706036567688, 109.3211898803711)\n",
      "epoch 6600 loss=(206.1841583251953, 0.506994903087616, 104.34173583984375)\n",
      "epoch 6700 loss=(203.79507446289062, 0.5025462508201599, 100.63037109375)\n",
      "epoch 6800 loss=(183.9012908935547, 0.4966530203819275, 95.64411926269531)\n",
      "epoch 6900 loss=(171.0828094482422, 0.4853344261646271, 92.79036712646484)\n",
      "epoch 7000 loss=(161.40292358398438, 0.48533666133880615, 89.22211456298828)\n",
      "epoch 7100 loss=(147.66415405273438, 0.47967466711997986, 87.40717315673828)\n",
      "epoch 7200 loss=(143.24559020996094, 0.4699232876300812, 82.54012298583984)\n",
      "epoch 7300 loss=(141.4612274169922, 0.4636482000350952, 80.14348602294922)\n",
      "epoch 7400 loss=(130.98989868164062, 0.45750558376312256, 74.58231353759766)\n",
      "epoch 7500 loss=(123.10549926757812, 0.4608009159564972, 73.02378845214844)\n",
      "epoch 7600 loss=(109.69053649902344, 0.4523949921131134, 68.60145568847656)\n",
      "epoch 7700 loss=(105.04991912841797, 0.44507402181625366, 66.67560577392578)\n",
      "epoch 7800 loss=(97.68054962158203, 0.43931707739830017, 62.10184860229492)\n",
      "epoch 7900 loss=(102.33832550048828, 0.4299023747444153, 60.233192443847656)\n",
      "epoch 8000 loss=(90.9803695678711, 0.43292805552482605, 54.45500564575195)\n",
      "epoch 8100 loss=(88.6069107055664, 0.4240368902683258, 51.65824890136719)\n",
      "epoch 8200 loss=(86.92041015625, 0.4144285321235657, 50.90833282470703)\n",
      "epoch 8300 loss=(77.46980285644531, 0.410339891910553, 47.61176681518555)\n",
      "epoch 8400 loss=(73.1513900756836, 0.41619181632995605, 44.670040130615234)\n",
      "epoch 8500 loss=(65.69927978515625, 0.40476372838020325, 41.66843032836914)\n",
      "epoch 8600 loss=(72.72852325439453, 0.39839404821395874, 39.712650299072266)\n",
      "epoch 8700 loss=(55.80076217651367, 0.3972831666469574, 37.832725524902344)\n",
      "epoch 8800 loss=(57.64375686645508, 0.3837955892086029, 35.37384796142578)\n",
      "epoch 8900 loss=(53.61579895019531, 0.3892619013786316, 32.44865036010742)\n",
      "epoch 9000 loss=(55.795101165771484, 0.37528911232948303, 30.271081924438477)\n",
      "epoch 9100 loss=(53.07880401611328, 0.374326229095459, 27.988676071166992)\n",
      "epoch 9200 loss=(52.478816986083984, 0.3659498989582062, 27.94474220275879)\n",
      "epoch 9300 loss=(48.071109771728516, 0.3619610071182251, 25.493953704833984)\n",
      "epoch 9400 loss=(49.4779052734375, 0.35907164216041565, 22.8135929107666)\n",
      "epoch 9500 loss=(47.98037338256836, 0.35290709137916565, 23.510787963867188)\n",
      "epoch 9600 loss=(45.1038703918457, 0.3447418808937073, 21.951427459716797)\n",
      "epoch 9700 loss=(44.353580474853516, 0.3411642909049988, 21.967451095581055)\n",
      "epoch 9800 loss=(47.93349838256836, 0.3340786099433899, 21.11493492126465)\n",
      "epoch 9900 loss=(43.30436325073242, 0.3301352858543396, 20.792739868164062)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000 loss=(36.425079345703125, 0.32855716347694397, 17.264381408691406)\n",
      "epoch 10100 loss=(38.729740142822266, 0.3234878480434418, 18.241525650024414)\n",
      "epoch 10200 loss=(38.02396011352539, 0.31328803300857544, 18.092248916625977)\n",
      "epoch 10300 loss=(40.115394592285156, 0.3135978877544403, 16.211658477783203)\n",
      "epoch 10400 loss=(39.13098907470703, 0.3001745939254761, 17.745338439941406)\n",
      "epoch 10500 loss=(36.55449676513672, 0.30422499775886536, 16.34383773803711)\n",
      "epoch 10600 loss=(34.700748443603516, 0.2974322736263275, 14.503067016601562)\n",
      "epoch 10700 loss=(34.88117980957031, 0.28934338688850403, 14.308460235595703)\n",
      "epoch 10800 loss=(31.670040130615234, 0.28957220911979675, 14.983240127563477)\n",
      "epoch 10900 loss=(33.335594177246094, 0.2888162434101105, 14.290151596069336)\n",
      "epoch 11000 loss=(31.892812728881836, 0.2822679579257965, 14.22936725616455)\n",
      "epoch 11100 loss=(37.06501770019531, 0.2789819836616516, 12.843792915344238)\n",
      "epoch 11200 loss=(33.30848693847656, 0.2775033116340637, 13.041495323181152)\n",
      "epoch 11300 loss=(36.374935150146484, 0.2744710445404053, 11.13027286529541)\n",
      "epoch 11400 loss=(33.17101287841797, 0.2644652724266052, 11.57536506652832)\n",
      "epoch 11500 loss=(37.85032653808594, 0.2635231614112854, 11.052157402038574)\n",
      "epoch 11600 loss=(33.17596435546875, 0.2499336451292038, 12.595165252685547)\n",
      "epoch 11700 loss=(30.98870277404785, 0.2547512650489807, 11.693544387817383)\n",
      "epoch 11800 loss=(35.52452087402344, 0.2577199637889862, 10.97718334197998)\n",
      "epoch 11900 loss=(30.430681228637695, 0.25399625301361084, 11.97374439239502)\n",
      "epoch 12000 loss=(34.127315521240234, 0.239315003156662, 11.259174346923828)\n",
      "epoch 12100 loss=(31.78403663635254, 0.24317224323749542, 11.870579719543457)\n",
      "epoch 12200 loss=(30.907230377197266, 0.24060842394828796, 11.079293251037598)\n",
      "epoch 12300 loss=(31.22925567626953, 0.23328180611133575, 10.179317474365234)\n",
      "epoch 12400 loss=(31.774919509887695, 0.2288782000541687, 11.864871978759766)\n",
      "epoch 12500 loss=(32.418209075927734, 0.22843562066555023, 11.189188003540039)\n",
      "epoch 12600 loss=(33.92959976196289, 0.22504888474941254, 11.066338539123535)\n",
      "epoch 12700 loss=(32.0311393737793, 0.22592401504516602, 9.513055801391602)\n",
      "epoch 12800 loss=(31.39145851135254, 0.22498825192451477, 10.023744583129883)\n",
      "epoch 12900 loss=(33.68654251098633, 0.21476297080516815, 10.999861717224121)\n",
      "epoch 13000 loss=(33.785587310791016, 0.21962209045886993, 9.826409339904785)\n",
      "epoch 13100 loss=(33.507904052734375, 0.21087229251861572, 11.131343841552734)\n",
      "epoch 13200 loss=(33.82892990112305, 0.20355118811130524, 10.030167579650879)\n",
      "Early stopping\n",
      "Finished training\n",
      "6 (25.686952590942383, 0.8474576473236084, 15.574153900146484)\n",
      "epoch 0 loss=(34445.35546875, 0.7189623713493347, 10928.548828125)\n",
      "epoch 100 loss=(6257.40771484375, 7.34492301940918, 4838.32958984375)\n",
      "epoch 200 loss=(2254.660400390625, 3.2625203132629395, 991.9434814453125)\n",
      "epoch 300 loss=(1936.53369140625, 2.8570339679718018, 347.5818176269531)\n",
      "epoch 400 loss=(1818.9052734375, 2.4972171783447266, 312.5663146972656)\n",
      "epoch 500 loss=(1705.948486328125, 2.0388760566711426, 310.4356994628906)\n",
      "epoch 600 loss=(1603.9610595703125, 1.5634655952453613, 309.4429931640625)\n",
      "epoch 700 loss=(1513.4669189453125, 1.1242902278900146, 307.1475830078125)\n",
      "epoch 800 loss=(1435.206298828125, 0.8319504261016846, 302.36688232421875)\n",
      "epoch 900 loss=(1368.5887451171875, 0.7256218194961548, 296.3907775878906)\n",
      "epoch 1000 loss=(1311.84130859375, 0.7505181431770325, 289.0135803222656)\n",
      "epoch 1100 loss=(1261.4847412109375, 0.7846817970275879, 280.6434326171875)\n",
      "epoch 1200 loss=(1219.2532958984375, 0.8134117722511292, 272.41796875)\n",
      "epoch 1300 loss=(1184.08984375, 0.8205289840698242, 263.54266357421875)\n",
      "epoch 1400 loss=(1151.278564453125, 0.7948094010353088, 256.21331787109375)\n",
      "epoch 1500 loss=(1122.3956298828125, 0.7792025804519653, 249.7893829345703)\n",
      "epoch 1600 loss=(1093.7413330078125, 0.7633556127548218, 244.03042602539062)\n",
      "epoch 1700 loss=(1066.075927734375, 0.7425466179847717, 239.57933044433594)\n",
      "epoch 1800 loss=(1038.397216796875, 0.7375103831291199, 235.50746154785156)\n",
      "epoch 1900 loss=(1013.275146484375, 0.7325961589813232, 232.28781127929688)\n",
      "epoch 2000 loss=(993.119384765625, 0.7197177410125732, 228.98800659179688)\n",
      "epoch 2100 loss=(965.5343017578125, 0.7143813371658325, 226.6419677734375)\n",
      "epoch 2200 loss=(944.5470581054688, 0.7105798125267029, 224.8627166748047)\n",
      "epoch 2300 loss=(921.280517578125, 0.7074801325798035, 222.3052215576172)\n",
      "epoch 2400 loss=(896.5733032226562, 0.6935442090034485, 220.86505126953125)\n",
      "epoch 2500 loss=(876.381591796875, 0.691001296043396, 219.2851104736328)\n",
      "epoch 2600 loss=(848.01318359375, 0.6929720044136047, 217.87962341308594)\n",
      "epoch 2700 loss=(828.044921875, 0.6871601343154907, 216.85940551757812)\n",
      "epoch 2800 loss=(803.4390258789062, 0.6890789866447449, 215.04617309570312)\n",
      "epoch 2900 loss=(783.2964477539062, 0.6865036487579346, 213.0472412109375)\n",
      "epoch 3000 loss=(762.89013671875, 0.6835516691207886, 212.12928771972656)\n",
      "epoch 3100 loss=(741.2984619140625, 0.6796256899833679, 210.6357879638672)\n",
      "epoch 3200 loss=(722.3088989257812, 0.6794533133506775, 209.25787353515625)\n",
      "epoch 3300 loss=(696.0025634765625, 0.6827377676963806, 208.38214111328125)\n",
      "epoch 3400 loss=(675.8485717773438, 0.6751616597175598, 206.71026611328125)\n",
      "epoch 3500 loss=(656.2805786132812, 0.6668394804000854, 205.03636169433594)\n",
      "epoch 3600 loss=(635.8311157226562, 0.6638960838317871, 203.26229858398438)\n",
      "epoch 3700 loss=(612.1554565429688, 0.6647376418113708, 201.73497009277344)\n",
      "epoch 3800 loss=(598.5234375, 0.6536175012588501, 200.15576171875)\n",
      "epoch 3900 loss=(574.7897338867188, 0.6525914072990417, 198.72377014160156)\n",
      "epoch 4000 loss=(557.0233154296875, 0.6444565653800964, 197.99644470214844)\n",
      "epoch 4100 loss=(540.3599243164062, 0.6406080722808838, 194.09442138671875)\n",
      "epoch 4200 loss=(524.49853515625, 0.6351693272590637, 193.91262817382812)\n",
      "epoch 4300 loss=(508.40478515625, 0.6279931664466858, 190.83006286621094)\n",
      "epoch 4400 loss=(489.9750061035156, 0.6271633505821228, 190.03465270996094)\n",
      "epoch 4500 loss=(468.077880859375, 0.6220118999481201, 186.51806640625)\n",
      "epoch 4600 loss=(455.8244323730469, 0.6061186194419861, 184.2411346435547)\n",
      "epoch 4700 loss=(437.7590026855469, 0.6071576476097107, 183.36228942871094)\n",
      "epoch 4800 loss=(424.896240234375, 0.596524715423584, 180.66429138183594)\n",
      "epoch 4900 loss=(407.7610168457031, 0.6018629670143127, 179.0716552734375)\n",
      "epoch 5000 loss=(391.06817626953125, 0.5903804898262024, 176.46754455566406)\n",
      "epoch 5100 loss=(382.21038818359375, 0.5943093299865723, 173.83651733398438)\n",
      "epoch 5200 loss=(367.1002502441406, 0.5883456468582153, 171.54017639160156)\n",
      "epoch 5300 loss=(350.9207763671875, 0.5824436545372009, 168.06016540527344)\n",
      "epoch 5400 loss=(342.8969421386719, 0.5728878378868103, 164.66824340820312)\n",
      "epoch 5500 loss=(323.45440673828125, 0.574630856513977, 163.31285095214844)\n",
      "epoch 5600 loss=(314.0780029296875, 0.5724527835845947, 161.10513305664062)\n",
      "epoch 5700 loss=(295.4624938964844, 0.5639487504959106, 158.17132568359375)\n",
      "epoch 5800 loss=(283.07257080078125, 0.5526233911514282, 155.904541015625)\n",
      "epoch 5900 loss=(275.5605163574219, 0.5519527792930603, 153.89439392089844)\n",
      "epoch 6000 loss=(261.2666015625, 0.5495530962944031, 150.56936645507812)\n",
      "epoch 6100 loss=(251.24591064453125, 0.5427290797233582, 147.6717071533203)\n",
      "epoch 6200 loss=(231.7490692138672, 0.5394400954246521, 144.3904571533203)\n",
      "epoch 6300 loss=(233.78512573242188, 0.533115029335022, 141.7102508544922)\n",
      "epoch 6400 loss=(229.70904541015625, 0.5300633311271667, 137.45538330078125)\n",
      "epoch 6500 loss=(218.92530822753906, 0.525370717048645, 134.2622833251953)\n",
      "epoch 6600 loss=(212.324462890625, 0.5184152126312256, 131.3997344970703)\n",
      "epoch 6700 loss=(197.91102600097656, 0.5175037980079651, 129.85598754882812)\n",
      "epoch 6800 loss=(190.77267456054688, 0.5193048119544983, 125.75122833251953)\n",
      "epoch 6900 loss=(182.1788330078125, 0.5158164501190186, 122.26991271972656)\n",
      "epoch 7000 loss=(174.36373901367188, 0.5007603168487549, 119.4090576171875)\n",
      "epoch 7100 loss=(174.1667022705078, 0.49950069189071655, 116.33267974853516)\n",
      "epoch 7200 loss=(160.36441040039062, 0.49157172441482544, 111.45890808105469)\n",
      "epoch 7300 loss=(146.67919921875, 0.4916749894618988, 111.0276107788086)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7400 loss=(151.65599060058594, 0.4893481731414795, 105.99469757080078)\n",
      "epoch 7500 loss=(137.67881774902344, 0.4808658957481384, 102.45347595214844)\n",
      "epoch 7600 loss=(125.00226593017578, 0.4760008454322815, 100.75926208496094)\n",
      "epoch 7700 loss=(126.20233917236328, 0.47569939494132996, 95.85832977294922)\n",
      "epoch 7800 loss=(122.15460968017578, 0.4718310236930847, 93.56887817382812)\n",
      "epoch 7900 loss=(114.28532409667969, 0.4620448350906372, 88.54042053222656)\n",
      "epoch 8000 loss=(107.3095932006836, 0.4588846266269684, 86.99846649169922)\n",
      "epoch 8100 loss=(102.70323944091797, 0.4578593671321869, 82.06681823730469)\n",
      "epoch 8200 loss=(99.75977325439453, 0.4445746839046478, 79.12495422363281)\n",
      "epoch 8300 loss=(97.37805938720703, 0.45047956705093384, 79.30146026611328)\n",
      "epoch 8400 loss=(91.26023864746094, 0.44197753071784973, 71.76367950439453)\n",
      "epoch 8500 loss=(86.80066680908203, 0.432553231716156, 71.37754821777344)\n",
      "epoch 8600 loss=(83.86345672607422, 0.4263506233692169, 68.5189208984375)\n",
      "epoch 8700 loss=(77.13922119140625, 0.42037734389305115, 66.162353515625)\n",
      "epoch 8800 loss=(82.03275299072266, 0.4189773201942444, 63.320167541503906)\n",
      "epoch 8900 loss=(73.79548645019531, 0.4184601604938507, 60.18671417236328)\n",
      "epoch 9000 loss=(73.09013366699219, 0.41400471329689026, 56.46556091308594)\n",
      "epoch 9100 loss=(71.74903869628906, 0.41540828347206116, 55.34339141845703)\n",
      "epoch 9200 loss=(64.04342651367188, 0.4047815799713135, 51.634910583496094)\n",
      "epoch 9300 loss=(62.496604919433594, 0.39648300409317017, 50.951202392578125)\n",
      "epoch 9400 loss=(58.209407806396484, 0.39796727895736694, 45.23911666870117)\n",
      "epoch 9500 loss=(61.50930404663086, 0.3919004499912262, 45.39462661743164)\n",
      "epoch 9600 loss=(62.211509704589844, 0.3889770805835724, 41.52684020996094)\n",
      "epoch 9700 loss=(55.80891418457031, 0.3830576241016388, 40.929466247558594)\n",
      "epoch 9800 loss=(56.48140335083008, 0.38092735409736633, 38.39840316772461)\n",
      "epoch 9900 loss=(49.63847351074219, 0.37053951621055603, 36.71071243286133)\n",
      "epoch 10000 loss=(44.4704704284668, 0.3784104883670807, 35.41969680786133)\n",
      "epoch 10100 loss=(44.836185455322266, 0.36050108075141907, 33.20880889892578)\n",
      "epoch 10200 loss=(44.57550048828125, 0.36218708753585815, 31.760730743408203)\n",
      "epoch 10300 loss=(49.934486389160156, 0.3615715801715851, 30.21509552001953)\n",
      "epoch 10400 loss=(44.16261672973633, 0.35600385069847107, 29.110336303710938)\n",
      "epoch 10500 loss=(45.73651885986328, 0.35012924671173096, 26.593019485473633)\n",
      "epoch 10600 loss=(44.9852180480957, 0.3564184606075287, 27.635969161987305)\n",
      "epoch 10700 loss=(46.66569137573242, 0.34524959325790405, 24.88326644897461)\n",
      "epoch 10800 loss=(43.75782775878906, 0.34245699644088745, 25.7032527923584)\n",
      "epoch 10900 loss=(42.692588806152344, 0.33818286657333374, 23.249771118164062)\n",
      "epoch 11000 loss=(45.07242202758789, 0.34241950511932373, 21.24012565612793)\n",
      "epoch 11100 loss=(34.43467712402344, 0.3339100182056427, 22.492429733276367)\n",
      "epoch 11200 loss=(43.73778533935547, 0.3308263123035431, 20.15984535217285)\n",
      "epoch 11300 loss=(42.26509094238281, 0.3259137272834778, 19.525362014770508)\n",
      "epoch 11400 loss=(42.751060485839844, 0.3231503963470459, 19.844646453857422)\n",
      "epoch 11500 loss=(37.87091827392578, 0.32840296626091003, 19.030656814575195)\n",
      "epoch 11600 loss=(35.12761306762695, 0.3146060109138489, 16.867347717285156)\n",
      "epoch 11700 loss=(34.488956451416016, 0.3104935586452484, 14.87900447845459)\n",
      "epoch 11800 loss=(34.81104278564453, 0.3025752007961273, 17.071409225463867)\n",
      "epoch 11900 loss=(39.36308288574219, 0.31035274267196655, 16.9648380279541)\n",
      "epoch 12000 loss=(32.44356155395508, 0.3034166395664215, 16.44249153137207)\n",
      "epoch 12100 loss=(34.86870574951172, 0.3030600845813751, 15.09080696105957)\n",
      "epoch 12200 loss=(36.664913177490234, 0.29801636934280396, 15.153280258178711)\n",
      "epoch 12300 loss=(34.45408248901367, 0.29800358414649963, 16.815807342529297)\n",
      "epoch 12400 loss=(32.32341384887695, 0.29217278957366943, 14.599539756774902)\n",
      "epoch 12500 loss=(33.873199462890625, 0.29934197664260864, 14.172578811645508)\n",
      "epoch 12600 loss=(36.27212142944336, 0.29015854001045227, 12.730117797851562)\n",
      "epoch 12700 loss=(37.86728286743164, 0.2818553149700165, 12.504697799682617)\n",
      "epoch 12800 loss=(34.681636810302734, 0.28945207595825195, 13.072379112243652)\n",
      "epoch 12900 loss=(34.0141716003418, 0.27631571888923645, 12.38332462310791)\n",
      "epoch 13000 loss=(37.28108596801758, 0.27929383516311646, 12.601217269897461)\n",
      "epoch 13100 loss=(30.54752540588379, 0.2736643850803375, 13.585521697998047)\n",
      "epoch 13200 loss=(36.874725341796875, 0.27053216099739075, 12.251349449157715)\n",
      "epoch 13300 loss=(32.4260368347168, 0.2697065472602844, 11.093323707580566)\n",
      "epoch 13400 loss=(32.05602264404297, 0.26807931065559387, 12.736015319824219)\n",
      "epoch 13500 loss=(32.61289596557617, 0.27358680963516235, 10.987789154052734)\n",
      "epoch 13600 loss=(36.224308013916016, 0.2665264904499054, 11.964316368103027)\n",
      "epoch 13700 loss=(34.25248336791992, 0.26243600249290466, 10.563236236572266)\n",
      "epoch 13800 loss=(29.823963165283203, 0.25970298051834106, 12.213104248046875)\n",
      "epoch 13900 loss=(35.87770080566406, 0.26827335357666016, 11.221028327941895)\n",
      "epoch 14000 loss=(29.861146926879883, 0.2612340748310089, 10.96655559539795)\n",
      "epoch 14100 loss=(33.048797607421875, 0.2558494210243225, 10.002198219299316)\n",
      "epoch 14200 loss=(33.768253326416016, 0.2437116950750351, 11.173541069030762)\n",
      "epoch 14300 loss=(32.1380500793457, 0.24568922817707062, 11.756298065185547)\n",
      "epoch 14400 loss=(30.31813621520996, 0.2408370077610016, 11.529336929321289)\n",
      "epoch 14500 loss=(30.304977416992188, 0.2418312281370163, 10.666351318359375)\n",
      "epoch 14600 loss=(32.79371643066406, 0.23896777629852295, 9.709063529968262)\n",
      "epoch 14700 loss=(32.800846099853516, 0.23391786217689514, 11.515860557556152)\n",
      "epoch 14800 loss=(34.406829833984375, 0.23860734701156616, 9.90357780456543)\n",
      "epoch 14900 loss=(31.824825286865234, 0.24443623423576355, 10.665650367736816)\n",
      "epoch 14999 loss=(32.345401763916016, 0.23591235280036926, 10.051406860351562)\n",
      "Finished training\n",
      "7 (27.94972038269043, 0.694915235042572, 13.86160659790039)\n",
      "epoch 0 loss=(34285.6875, 0.6927363872528076, 10776.17578125)\n",
      "epoch 100 loss=(2345.6220703125, 2.862241506576538, 349.2168884277344)\n",
      "epoch 200 loss=(1806.0994873046875, 3.3370609283447266, 277.54571533203125)\n",
      "epoch 300 loss=(1669.989013671875, 3.326474189758301, 256.64312744140625)\n",
      "epoch 400 loss=(1577.00634765625, 3.064598560333252, 248.92433166503906)\n",
      "epoch 500 loss=(1497.488037109375, 2.7737858295440674, 245.06192016601562)\n",
      "epoch 600 loss=(1431.7547607421875, 2.455005407333374, 243.86959838867188)\n",
      "epoch 700 loss=(1378.0010986328125, 2.109489679336548, 243.56141662597656)\n",
      "epoch 800 loss=(1335.909423828125, 1.7146763801574707, 243.91331481933594)\n",
      "epoch 900 loss=(1301.0770263671875, 1.3466534614562988, 243.09323120117188)\n",
      "epoch 1000 loss=(1271.4820556640625, 0.9999101758003235, 242.0714874267578)\n",
      "epoch 1100 loss=(1245.6783447265625, 0.789880633354187, 240.53985595703125)\n",
      "epoch 1200 loss=(1224.3487548828125, 0.7092002034187317, 238.62258911132812)\n",
      "epoch 1300 loss=(1201.9398193359375, 0.6866136193275452, 237.11685180664062)\n",
      "epoch 1400 loss=(1182.7120361328125, 0.6819297075271606, 235.64073181152344)\n",
      "epoch 1500 loss=(1163.115966796875, 0.6797341108322144, 234.1448974609375)\n",
      "epoch 1600 loss=(1143.1729736328125, 0.6792064905166626, 232.20140075683594)\n",
      "epoch 1700 loss=(1123.7239990234375, 0.6755569577217102, 230.85853576660156)\n",
      "epoch 1800 loss=(1103.873779296875, 0.6721929907798767, 229.70932006835938)\n",
      "epoch 1900 loss=(1082.2940673828125, 0.673047661781311, 227.8695831298828)\n",
      "epoch 2000 loss=(1061.13232421875, 0.6756073832511902, 226.59750366210938)\n",
      "epoch 2100 loss=(1040.257080078125, 0.661736786365509, 224.84771728515625)\n",
      "epoch 2200 loss=(1019.6414184570312, 0.6627146005630493, 223.13690185546875)\n",
      "epoch 2300 loss=(997.1694946289062, 0.6551432013511658, 221.31858825683594)\n",
      "epoch 2400 loss=(971.3651733398438, 0.6563982367515564, 218.94497680664062)\n",
      "epoch 2500 loss=(949.6100463867188, 0.6517033576965332, 216.8880157470703)\n",
      "epoch 2600 loss=(929.6240844726562, 0.6465786695480347, 214.86944580078125)\n",
      "epoch 2700 loss=(905.4520874023438, 0.6464061141014099, 211.88792419433594)\n",
      "epoch 2800 loss=(877.0214233398438, 0.6397594213485718, 210.38478088378906)\n",
      "epoch 2900 loss=(856.7522583007812, 0.6391027569770813, 207.88441467285156)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3000 loss=(830.6767578125, 0.6306726336479187, 204.52139282226562)\n",
      "epoch 3100 loss=(809.9974365234375, 0.6295167207717896, 200.74400329589844)\n",
      "epoch 3200 loss=(785.46826171875, 0.6285452842712402, 199.45071411132812)\n",
      "epoch 3300 loss=(759.9725341796875, 0.6217827796936035, 196.23988342285156)\n",
      "epoch 3400 loss=(738.0792236328125, 0.6157298684120178, 192.18057250976562)\n",
      "epoch 3500 loss=(713.239013671875, 0.6087775826454163, 188.70150756835938)\n",
      "epoch 3600 loss=(690.6488647460938, 0.6050072312355042, 185.84185791015625)\n",
      "epoch 3700 loss=(664.3353881835938, 0.605449914932251, 180.78025817871094)\n",
      "epoch 3800 loss=(638.5076293945312, 0.5963945388793945, 178.09278869628906)\n",
      "epoch 3900 loss=(610.7803344726562, 0.5946617126464844, 174.1092987060547)\n",
      "epoch 4000 loss=(596.498046875, 0.597113311290741, 170.47474670410156)\n",
      "epoch 4100 loss=(573.2755126953125, 0.5865548253059387, 167.15090942382812)\n",
      "epoch 4200 loss=(552.6295776367188, 0.5834662914276123, 163.35348510742188)\n",
      "epoch 4300 loss=(527.8392944335938, 0.5770045518875122, 158.67234802246094)\n",
      "epoch 4400 loss=(511.78533935546875, 0.5737419128417969, 156.91810607910156)\n",
      "epoch 4500 loss=(486.0602111816406, 0.5591684579849243, 152.48048400878906)\n",
      "epoch 4600 loss=(473.3268737792969, 0.5610986948013306, 147.9763641357422)\n",
      "epoch 4700 loss=(454.98992919921875, 0.5524709224700928, 146.04776000976562)\n",
      "epoch 4800 loss=(434.89691162109375, 0.5534433126449585, 140.6298065185547)\n",
      "epoch 4900 loss=(419.1534729003906, 0.5409665107727051, 137.4374542236328)\n",
      "epoch 5000 loss=(400.90460205078125, 0.5461905598640442, 133.32347106933594)\n",
      "epoch 5100 loss=(378.5746154785156, 0.5353560447692871, 130.6309814453125)\n",
      "epoch 5200 loss=(361.8874816894531, 0.5258746147155762, 126.2654037475586)\n",
      "epoch 5300 loss=(347.1968078613281, 0.526057243347168, 123.24652099609375)\n",
      "epoch 5400 loss=(337.02752685546875, 0.5105590224266052, 119.75114440917969)\n",
      "epoch 5500 loss=(323.87408447265625, 0.5109580755233765, 116.86093139648438)\n",
      "epoch 5600 loss=(300.8356018066406, 0.5005974173545837, 114.37301635742188)\n",
      "epoch 5700 loss=(283.7277526855469, 0.49577754735946655, 107.76253509521484)\n",
      "epoch 5800 loss=(273.13677978515625, 0.48946207761764526, 107.08536529541016)\n",
      "epoch 5900 loss=(258.0370788574219, 0.48667800426483154, 102.03446960449219)\n",
      "epoch 6000 loss=(250.81887817382812, 0.4826146960258484, 98.08417510986328)\n",
      "epoch 6100 loss=(239.26834106445312, 0.47076520323753357, 96.30259704589844)\n",
      "epoch 6200 loss=(219.76348876953125, 0.46929600834846497, 92.36764526367188)\n",
      "epoch 6300 loss=(208.30519104003906, 0.4603699743747711, 86.21076965332031)\n",
      "epoch 6400 loss=(199.1913604736328, 0.45226457715034485, 85.36214447021484)\n",
      "epoch 6500 loss=(190.1344757080078, 0.44707590341567993, 80.62245178222656)\n",
      "epoch 6600 loss=(182.92442321777344, 0.44619324803352356, 76.67472839355469)\n",
      "epoch 6700 loss=(162.7049102783203, 0.43398988246917725, 73.39859771728516)\n",
      "epoch 6800 loss=(162.78065490722656, 0.4345265030860901, 71.56996154785156)\n",
      "epoch 6900 loss=(149.1968994140625, 0.43040701746940613, 65.95494842529297)\n",
      "epoch 7000 loss=(139.33609008789062, 0.41871827840805054, 65.2292709350586)\n",
      "epoch 7100 loss=(127.82960510253906, 0.4145440459251404, 62.12923049926758)\n",
      "epoch 7200 loss=(122.20874786376953, 0.40752077102661133, 56.493812561035156)\n",
      "epoch 7300 loss=(123.2310562133789, 0.41113561391830444, 55.354705810546875)\n",
      "epoch 7400 loss=(105.75843048095703, 0.39332315325737, 51.293853759765625)\n",
      "epoch 7500 loss=(108.01294708251953, 0.38879838585853577, 48.76307678222656)\n",
      "epoch 7600 loss=(100.5643539428711, 0.3856961131095886, 46.8001823425293)\n",
      "epoch 7700 loss=(89.90154266357422, 0.38463932275772095, 41.729774475097656)\n",
      "epoch 7800 loss=(84.32797241210938, 0.37333908677101135, 39.385379791259766)\n",
      "epoch 7900 loss=(77.99324798583984, 0.3721213638782501, 37.150211334228516)\n",
      "epoch 8000 loss=(76.02461242675781, 0.3642679750919342, 36.2755126953125)\n",
      "epoch 8100 loss=(76.07183074951172, 0.35798144340515137, 33.225120544433594)\n",
      "epoch 8200 loss=(64.99250793457031, 0.35415947437286377, 32.483463287353516)\n",
      "epoch 8300 loss=(66.82525634765625, 0.3435705006122589, 28.973073959350586)\n",
      "epoch 8400 loss=(64.3378677368164, 0.3481355607509613, 28.50136947631836)\n",
      "epoch 8500 loss=(61.41706848144531, 0.3444519639015198, 27.002761840820312)\n",
      "epoch 8600 loss=(55.91508483886719, 0.335604727268219, 26.343673706054688)\n",
      "epoch 8700 loss=(51.1409912109375, 0.32898038625717163, 24.9112491607666)\n",
      "epoch 8800 loss=(59.265872955322266, 0.3199080526828766, 23.32155990600586)\n",
      "epoch 8900 loss=(50.882755279541016, 0.318666011095047, 22.70526885986328)\n",
      "epoch 9000 loss=(49.50917434692383, 0.30121752619743347, 19.95803451538086)\n",
      "epoch 9100 loss=(53.567352294921875, 0.30839091539382935, 19.478473663330078)\n",
      "epoch 9200 loss=(47.05677795410156, 0.30606701970100403, 17.5688533782959)\n",
      "epoch 9300 loss=(45.047149658203125, 0.29550084471702576, 17.945470809936523)\n",
      "epoch 9400 loss=(44.626792907714844, 0.2972850501537323, 16.510595321655273)\n",
      "epoch 9500 loss=(41.05329132080078, 0.2910156846046448, 17.620365142822266)\n",
      "epoch 9600 loss=(38.2220458984375, 0.27964961528778076, 15.15135669708252)\n",
      "epoch 9700 loss=(38.4502067565918, 0.2695175111293793, 13.690898895263672)\n",
      "epoch 9800 loss=(41.45706558227539, 0.27597567439079285, 14.661822319030762)\n",
      "epoch 9900 loss=(37.37610626220703, 0.2679346799850464, 14.43459701538086)\n",
      "epoch 10000 loss=(41.79206085205078, 0.26952046155929565, 14.956912994384766)\n",
      "epoch 10100 loss=(38.14833450317383, 0.25455528497695923, 13.454426765441895)\n",
      "epoch 10200 loss=(35.68190002441406, 0.26451945304870605, 12.801775932312012)\n",
      "epoch 10300 loss=(37.76649475097656, 0.252799928188324, 11.650983810424805)\n",
      "epoch 10400 loss=(37.45368194580078, 0.24374058842658997, 11.873470306396484)\n",
      "epoch 10500 loss=(34.00181579589844, 0.24100089073181152, 12.457439422607422)\n",
      "epoch 10600 loss=(32.04142379760742, 0.24298503994941711, 10.821731567382812)\n",
      "epoch 10700 loss=(31.921483993530273, 0.22880133986473083, 12.07441234588623)\n",
      "epoch 10800 loss=(35.335880279541016, 0.22821693122386932, 11.72031021118164)\n",
      "epoch 10900 loss=(37.2718505859375, 0.22040955722332, 11.724628448486328)\n",
      "epoch 11000 loss=(37.23960494995117, 0.23824112117290497, 11.81389331817627)\n",
      "epoch 11100 loss=(32.08552169799805, 0.2272828370332718, 11.541069030761719)\n",
      "epoch 11200 loss=(35.51505661010742, 0.22044137120246887, 10.7642183303833)\n",
      "epoch 11300 loss=(36.44878005981445, 0.21355287730693817, 11.327604293823242)\n",
      "epoch 11400 loss=(35.69875717163086, 0.21469636261463165, 11.038960456848145)\n",
      "epoch 11500 loss=(37.12285614013672, 0.21679602563381195, 11.857131958007812)\n",
      "epoch 11600 loss=(33.82460021972656, 0.20569893717765808, 9.903100967407227)\n",
      "Early stopping\n",
      "Finished training\n",
      "8 (26.380115509033203, 0.7796609997749329, 15.188675880432129)\n",
      "epoch 0 loss=(34253.03515625, 0.7023053169250488, 10901.2099609375)\n",
      "epoch 100 loss=(3785.23681640625, 7.813519477844238, 2342.157958984375)\n",
      "epoch 200 loss=(2020.342529296875, 4.853017330169678, 341.2819519042969)\n",
      "epoch 300 loss=(1904.546875, 3.6752421855926514, 309.84417724609375)\n",
      "epoch 400 loss=(1803.6373291015625, 2.8732666969299316, 309.9411315917969)\n",
      "epoch 500 loss=(1708.743896484375, 2.070392608642578, 308.4620361328125)\n",
      "epoch 600 loss=(1619.6859130859375, 1.2977683544158936, 305.9044189453125)\n",
      "epoch 700 loss=(1542.163818359375, 0.7838682532310486, 301.5518493652344)\n",
      "epoch 800 loss=(1475.5706787109375, 0.8420189619064331, 295.9725341796875)\n",
      "epoch 900 loss=(1419.618896484375, 1.1707711219787598, 289.37396240234375)\n",
      "epoch 1000 loss=(1371.0540771484375, 1.5355935096740723, 282.3448791503906)\n",
      "epoch 1100 loss=(1333.398681640625, 1.867962121963501, 274.5055236816406)\n",
      "epoch 1200 loss=(1300.7237548828125, 2.111236810684204, 267.2190246582031)\n",
      "epoch 1300 loss=(1273.025146484375, 2.2779910564422607, 260.63250732421875)\n",
      "epoch 1400 loss=(1248.4168701171875, 2.4013402462005615, 254.1846923828125)\n",
      "epoch 1500 loss=(1226.70166015625, 2.450773239135742, 249.64036560058594)\n",
      "epoch 1600 loss=(1206.8974609375, 2.441028356552124, 245.5395050048828)\n",
      "epoch 1700 loss=(1189.5374755859375, 2.3996682167053223, 241.53512573242188)\n",
      "epoch 1800 loss=(1170.003173828125, 2.299387216567993, 238.62432861328125)\n",
      "epoch 1900 loss=(1154.8680419921875, 2.1937952041625977, 235.6787872314453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2000 loss=(1136.1402587890625, 2.0523769855499268, 232.67567443847656)\n",
      "epoch 2100 loss=(1119.3775634765625, 1.8857440948486328, 230.54319763183594)\n",
      "epoch 2200 loss=(1101.89892578125, 1.718154788017273, 228.55300903320312)\n",
      "epoch 2300 loss=(1086.2249755859375, 1.5510512590408325, 226.3761444091797)\n",
      "epoch 2400 loss=(1066.6612548828125, 1.3844772577285767, 224.20037841796875)\n",
      "epoch 2500 loss=(1047.289794921875, 1.2620465755462646, 222.5711669921875)\n",
      "epoch 2600 loss=(1026.4178466796875, 1.1788040399551392, 220.74725341796875)\n",
      "epoch 2700 loss=(1011.7186279296875, 1.1201653480529785, 219.66848754882812)\n",
      "epoch 2800 loss=(991.1627197265625, 1.0919378995895386, 217.79420471191406)\n",
      "epoch 2900 loss=(968.3270874023438, 1.1077044010162354, 216.16409301757812)\n",
      "epoch 3000 loss=(952.223876953125, 1.1040475368499756, 214.6087646484375)\n",
      "epoch 3100 loss=(931.0908813476562, 1.123435378074646, 212.5877227783203)\n",
      "epoch 3200 loss=(906.4154052734375, 1.13889479637146, 210.0135498046875)\n",
      "epoch 3300 loss=(887.1290893554688, 1.1561899185180664, 208.4542694091797)\n",
      "epoch 3400 loss=(866.3792724609375, 1.157806158065796, 207.22146606445312)\n",
      "epoch 3500 loss=(842.394775390625, 1.174735188484192, 205.39378356933594)\n",
      "epoch 3600 loss=(823.7731323242188, 1.171532392501831, 204.37037658691406)\n",
      "epoch 3700 loss=(798.9713745117188, 1.1789907217025757, 201.5925750732422)\n",
      "epoch 3800 loss=(776.2777709960938, 1.1829843521118164, 199.13134765625)\n",
      "epoch 3900 loss=(757.7750854492188, 1.1900876760482788, 197.76174926757812)\n",
      "epoch 4000 loss=(729.1080322265625, 1.1852407455444336, 195.86900329589844)\n",
      "epoch 4100 loss=(712.1146240234375, 1.1781874895095825, 192.3095245361328)\n",
      "epoch 4200 loss=(688.011474609375, 1.1606025695800781, 190.7043914794922)\n",
      "epoch 4300 loss=(664.1643676757812, 1.157099962234497, 189.47201538085938)\n",
      "epoch 4400 loss=(646.2692260742188, 1.139146089553833, 186.55923461914062)\n",
      "epoch 4500 loss=(623.0553588867188, 1.1292158365249634, 183.96673583984375)\n",
      "epoch 4600 loss=(600.3773193359375, 1.1019299030303955, 179.74607849121094)\n",
      "epoch 4700 loss=(582.2374877929688, 1.0860346555709839, 176.255615234375)\n",
      "epoch 4800 loss=(564.3311157226562, 1.0512800216674805, 174.52870178222656)\n",
      "epoch 4900 loss=(534.4899291992188, 1.034701943397522, 172.6306610107422)\n",
      "epoch 5000 loss=(519.8079833984375, 1.0038484334945679, 168.44297790527344)\n",
      "epoch 5100 loss=(503.5503234863281, 0.965907871723175, 165.26519775390625)\n",
      "epoch 5200 loss=(482.67291259765625, 0.9386656880378723, 164.4987335205078)\n",
      "epoch 5300 loss=(463.4443054199219, 0.9066853523254395, 158.7822723388672)\n",
      "epoch 5400 loss=(442.6891174316406, 0.8517473936080933, 156.2098388671875)\n",
      "epoch 5500 loss=(430.53839111328125, 0.8216586112976074, 152.45953369140625)\n",
      "epoch 5600 loss=(404.9833679199219, 0.7968379855155945, 149.84579467773438)\n",
      "epoch 5700 loss=(387.28277587890625, 0.7586436867713928, 145.9547576904297)\n",
      "epoch 5800 loss=(374.3494873046875, 0.7251073122024536, 141.76817321777344)\n",
      "epoch 5900 loss=(358.5799865722656, 0.6972036957740784, 138.1415557861328)\n",
      "epoch 6000 loss=(342.603759765625, 0.676317572593689, 134.10751342773438)\n",
      "epoch 6100 loss=(330.24298095703125, 0.6543505787849426, 131.1735076904297)\n",
      "epoch 6200 loss=(313.6598205566406, 0.6386361122131348, 125.65827178955078)\n",
      "epoch 6300 loss=(296.467041015625, 0.6233941316604614, 121.41443634033203)\n",
      "epoch 6400 loss=(283.9627685546875, 0.6116650700569153, 119.81446075439453)\n",
      "epoch 6500 loss=(262.239013671875, 0.599367618560791, 114.9313735961914)\n",
      "epoch 6600 loss=(248.17713928222656, 0.5926326513290405, 111.38170623779297)\n",
      "epoch 6700 loss=(247.47044372558594, 0.591293215751648, 106.20906829833984)\n",
      "epoch 6800 loss=(224.77700805664062, 0.5775476694107056, 102.02877807617188)\n",
      "epoch 6900 loss=(212.50982666015625, 0.572182297706604, 98.28206634521484)\n",
      "epoch 7000 loss=(204.9947052001953, 0.5672054290771484, 95.2854995727539)\n",
      "epoch 7100 loss=(194.84030151367188, 0.5560734868049622, 92.79731750488281)\n",
      "epoch 7200 loss=(184.1262664794922, 0.551044225692749, 87.34261322021484)\n",
      "epoch 7300 loss=(178.92002868652344, 0.5475119948387146, 84.66007232666016)\n",
      "epoch 7400 loss=(161.22068786621094, 0.5392518639564514, 78.73577880859375)\n",
      "epoch 7500 loss=(164.6254119873047, 0.5346329808235168, 75.20478057861328)\n",
      "epoch 7600 loss=(147.728515625, 0.5235958099365234, 72.49170684814453)\n",
      "epoch 7700 loss=(140.24818420410156, 0.5230123400688171, 69.39579772949219)\n",
      "epoch 7800 loss=(136.16183471679688, 0.5181459188461304, 63.79436111450195)\n",
      "epoch 7900 loss=(119.67575073242188, 0.5131921768188477, 61.49732971191406)\n",
      "epoch 8000 loss=(115.7061767578125, 0.508426308631897, 56.275508880615234)\n",
      "epoch 8100 loss=(114.67041778564453, 0.5027018189430237, 58.05280685424805)\n",
      "epoch 8200 loss=(101.39695739746094, 0.48774462938308716, 50.42424774169922)\n",
      "epoch 8300 loss=(99.52958679199219, 0.48856934905052185, 48.55257797241211)\n",
      "epoch 8400 loss=(93.85896301269531, 0.48399028182029724, 46.08137512207031)\n",
      "epoch 8500 loss=(83.44566345214844, 0.47925087809562683, 44.68236541748047)\n",
      "epoch 8600 loss=(82.805419921875, 0.46854615211486816, 42.68796157836914)\n",
      "epoch 8700 loss=(80.81580352783203, 0.46579933166503906, 39.037071228027344)\n",
      "epoch 8800 loss=(73.17133331298828, 0.4681294560432434, 37.33565902709961)\n",
      "epoch 8900 loss=(69.26142120361328, 0.46556225419044495, 33.90569305419922)\n",
      "epoch 9000 loss=(68.95475769042969, 0.4561142325401306, 33.61888885498047)\n",
      "epoch 9100 loss=(59.04719924926758, 0.4531114399433136, 31.424205780029297)\n",
      "epoch 9200 loss=(65.12415313720703, 0.44707128405570984, 29.243255615234375)\n",
      "epoch 9300 loss=(63.866092681884766, 0.43687060475349426, 28.838926315307617)\n",
      "epoch 9400 loss=(60.44831085205078, 0.4347287118434906, 26.449542999267578)\n",
      "epoch 9500 loss=(62.765743255615234, 0.4303176999092102, 25.079923629760742)\n",
      "epoch 9600 loss=(60.6347770690918, 0.42524296045303345, 25.353763580322266)\n",
      "epoch 9700 loss=(49.87821578979492, 0.4220344126224518, 22.085474014282227)\n",
      "epoch 9800 loss=(50.09282684326172, 0.42179593443870544, 21.3941650390625)\n",
      "epoch 9900 loss=(46.501129150390625, 0.41673582792282104, 19.723876953125)\n",
      "epoch 10000 loss=(47.38947296142578, 0.40853413939476013, 18.0158634185791)\n",
      "epoch 10100 loss=(43.586036682128906, 0.4012284278869629, 20.483346939086914)\n",
      "epoch 10200 loss=(44.7628288269043, 0.4016512930393219, 19.25599479675293)\n",
      "epoch 10300 loss=(49.2108154296875, 0.39324241876602173, 17.125995635986328)\n",
      "epoch 10400 loss=(43.80900573730469, 0.3921750485897064, 17.889041900634766)\n",
      "epoch 10500 loss=(40.288639068603516, 0.3872021436691284, 17.933753967285156)\n",
      "epoch 10600 loss=(43.306392669677734, 0.3867817521095276, 16.64514923095703)\n",
      "epoch 10700 loss=(39.4253044128418, 0.3804526925086975, 15.676984786987305)\n",
      "epoch 10800 loss=(38.710968017578125, 0.37543877959251404, 13.596404075622559)\n",
      "epoch 10900 loss=(37.48480987548828, 0.37118715047836304, 15.194928169250488)\n",
      "epoch 11000 loss=(39.08967590332031, 0.36830613017082214, 15.420469284057617)\n",
      "epoch 11100 loss=(40.3009147644043, 0.36480340361595154, 13.20124340057373)\n",
      "epoch 11200 loss=(40.054527282714844, 0.3587844967842102, 14.19483757019043)\n",
      "epoch 11300 loss=(37.059852600097656, 0.36120927333831787, 12.475081443786621)\n",
      "epoch 11400 loss=(35.601043701171875, 0.354358047246933, 12.52262020111084)\n",
      "epoch 11500 loss=(38.217872619628906, 0.3491450250148773, 13.791324615478516)\n",
      "epoch 11600 loss=(41.17304229736328, 0.3430555760860443, 12.006094932556152)\n",
      "epoch 11700 loss=(36.115455627441406, 0.3443826138973236, 13.804548263549805)\n",
      "epoch 11800 loss=(32.006568908691406, 0.33772432804107666, 12.705440521240234)\n",
      "epoch 11900 loss=(36.7266731262207, 0.3403753936290741, 11.161748886108398)\n",
      "epoch 12000 loss=(36.475746154785156, 0.32341328263282776, 11.592254638671875)\n",
      "epoch 12100 loss=(35.431758880615234, 0.33071786165237427, 13.631237983703613)\n",
      "epoch 12200 loss=(35.934078216552734, 0.315491646528244, 11.827507019042969)\n",
      "epoch 12300 loss=(36.808311462402344, 0.3218138515949249, 10.902966499328613)\n",
      "epoch 12400 loss=(35.250938415527344, 0.3117687702178955, 11.660154342651367)\n",
      "epoch 12500 loss=(37.36960983276367, 0.31071099638938904, 11.181694030761719)\n",
      "epoch 12600 loss=(35.44318389892578, 0.32106173038482666, 12.201882362365723)\n",
      "epoch 12700 loss=(40.147605895996094, 0.31291675567626953, 12.9999361038208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12800 loss=(31.45516586303711, 0.3111113905906677, 11.0332612991333)\n",
      "epoch 12900 loss=(34.11162567138672, 0.2985171377658844, 11.562252044677734)\n",
      "epoch 13000 loss=(34.8726806640625, 0.2920713424682617, 11.051010131835938)\n",
      "epoch 13100 loss=(31.434741973876953, 0.29236724972724915, 11.104040145874023)\n",
      "epoch 13200 loss=(34.69563293457031, 0.2895403504371643, 10.472949028015137)\n",
      "epoch 13300 loss=(31.1857967376709, 0.28274354338645935, 10.517436027526855)\n",
      "epoch 13400 loss=(31.393230438232422, 0.28018319606781006, 10.307604789733887)\n",
      "epoch 13500 loss=(31.09272575378418, 0.285115510225296, 11.130152702331543)\n",
      "epoch 13600 loss=(30.78389549255371, 0.2770124673843384, 11.093765258789062)\n",
      "epoch 13700 loss=(34.771419525146484, 0.2744654417037964, 10.476217269897461)\n",
      "epoch 13800 loss=(33.55084228515625, 0.27401676774024963, 11.500442504882812)\n",
      "epoch 13900 loss=(36.58882522583008, 0.2741244435310364, 11.053423881530762)\n",
      "epoch 14000 loss=(30.772808074951172, 0.27387210726737976, 10.942647933959961)\n",
      "epoch 14100 loss=(32.5157356262207, 0.2697937488555908, 10.280158996582031)\n",
      "epoch 14200 loss=(28.2476749420166, 0.26129937171936035, 9.406525611877441)\n",
      "epoch 14300 loss=(32.38732147216797, 0.2601531147956848, 11.152959823608398)\n",
      "epoch 14400 loss=(30.35468864440918, 0.25543269515037537, 10.501823425292969)\n",
      "epoch 14500 loss=(28.561283111572266, 0.2564900815486908, 10.775418281555176)\n",
      "epoch 14600 loss=(29.14862060546875, 0.25197330117225647, 12.033720970153809)\n",
      "epoch 14700 loss=(32.23754119873047, 0.25468122959136963, 10.5012845993042)\n",
      "epoch 14800 loss=(29.542160034179688, 0.25089460611343384, 11.371611595153809)\n",
      "epoch 14900 loss=(32.038658142089844, 0.2555503845214844, 9.03282642364502)\n",
      "epoch 14999 loss=(28.701921463012695, 0.244598850607872, 10.95062255859375)\n",
      "Finished training\n",
      "9 (26.091154098510742, 0.7796609997749329, 11.32404613494873)\n"
     ]
    }
   ],
   "source": [
    "ceLoss = torch.nn.CrossEntropyLoss()\n",
    "mseLoss = torch.nn.MSELoss()\n",
    "nEpochs = 15000\n",
    "pPeriod = 100\n",
    "thresh = torch.Tensor((80,2e-1,40)).float().cuda()\n",
    "\n",
    "nPoly = 1\n",
    "para = [makePoly(nback_p_t), makePoly(emoid_p_t)]\n",
    "    \n",
    "rmse = []\n",
    "\n",
    "for i in range(10):\n",
    "#     pgigcn = MiniPgi(10, len(para), nPoly, 3, 0.5, (0.2, 0.01, 1))\n",
    "#     optim = torch.optim.Adam(pgigcn.masks, lr=2e-5, weight_decay=2e-5)\n",
    "    pgigcn = GCN(40, len(para), nPoly, 3, 0.5)\n",
    "    optim = torch.optim.Adam(pgigcn.parameters(), lr=2e-5, weight_decay=2e-5)\n",
    "\n",
    "    trainIdcs = groups[i][0]\n",
    "    testIdcs = groups[i][1]\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    X = X[trainIdcs]\n",
    "    Y = torch.from_numpy(X_all[trainIdcs]).float().cuda()\n",
    "    \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t[trainIdcs].unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        optim.zero_grad()\n",
    "        res = pgigcn(X, age=age, gender=gen, wrat=wrt)\n",
    "        loss0 = mseLoss(res[0], age)\n",
    "        loss1 = ceLoss(res[1], gen)\n",
    "        loss2 = mseLoss(res[2], wrt)\n",
    "        loss = torch.stack([loss0, loss1, loss2])\n",
    "        torch.sum(loss).backward()\n",
    "        optim.step()\n",
    "        if (epoch % pPeriod == 0 or epoch == nEpochs-1):\n",
    "            print(f'epoch {epoch} loss={(float(loss0), float(loss1), float(loss2))}')\n",
    "        if torch.all(loss < thresh):\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    print('Finished training')\n",
    "    \n",
    "    pgigcn.eval()\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    Y = torch.from_numpy(X_all).float().cuda()\n",
    "        \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t.unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "\n",
    "    gen0 = gen.clone().detach()\n",
    "    gen0[testIdcs] = 0\n",
    "    wrt0 = wrt.clone().detach()\n",
    "    wrt0[testIdcs] = 0\n",
    "    age0 = age.clone().detach()\n",
    "    age0[testIdcs] = 0\n",
    "    \n",
    "    res = pgigcn(X, age=age0, gender=gen0, wrat=wrt0)\n",
    "    loss0 = mseLoss(res[0][testIdcs].detach(), age[testIdcs]).cpu().numpy()**0.5\n",
    "    frac1 = torch.sum(torch.argmax(res[1].detach(), dim=1)[testIdcs] \n",
    "                     == torch.argmax(gen[testIdcs], dim=1))/testIdcs.shape[0]\n",
    "    loss2 = mseLoss(res[2][testIdcs].detach(), wrt[testIdcs]).cpu().numpy()**0.5\n",
    "    \n",
    "    rmse.append((float(loss0), float(frac1), float(loss2)))\n",
    "    print(i, end=' ')\n",
    "    print(rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "aa843bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.322284698486328\n",
      "26.60723114013672\n",
      "19.459274291992188\n",
      "25.55489730834961\n",
      "25.010723114013672\n",
      "22.267778396606445\n",
      "25.686952590942383\n",
      "27.94972038269043\n",
      "26.380115509033203\n",
      "26.091154098510742\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in rmse:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "074f5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "class PgiBack(nn.Module):\n",
    "    def __init__(self, w, nPara, nPoly, nTgts, dp=0.5, relu=0.1, mask=None):\n",
    "        super(PgiBack, self).__init__()\n",
    "        if mask is None:\n",
    "            self.mask = nn.Parameter(\n",
    "                0.0001*torch.ones(nPara,nPoly,arith(263),w).float().cuda()\n",
    "                +0.00001*torch.randn(nPara,nPoly,arith(263),w).float().cuda()\n",
    "            )\n",
    "        else:\n",
    "            self.mask = nn.Parameter(mask.clone().detach()+0.1*torch.std(mask))\n",
    "        self.dp = nn.Dropout(p=dp)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=relu)\n",
    "        \n",
    "    def forward(self, x, orig, idx, lbl):\n",
    "        x = self.dp(x)\n",
    "        with torch.no_grad():\n",
    "            y = torch.einsum('abcd,bcde->ae', x, orig.masks[idx])\n",
    "        z = torch.einsum('abcd,bcde->ae', x, self.mask)\n",
    "        e = y@z.T\n",
    "        idcs = torch.logical_not(torch.any(lbl, dim=1))\n",
    "        e[:,idcs] = 0\n",
    "        e = self.relu(mask(e))\n",
    "        s = torch.sum(e, dim=1)\n",
    "        e = e/s.unsqueeze(1)\n",
    "        return e@lbl, y, z, e\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7db2c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=0.6989180445671082\n",
      "epoch 100 loss=0.7022110819816589\n",
      "epoch 200 loss=0.6991075277328491\n",
      "epoch 300 loss=0.7057473659515381\n",
      "epoch 400 loss=0.7027868628501892\n",
      "epoch 500 loss=0.6912396550178528\n",
      "epoch 600 loss=0.7050478458404541\n",
      "epoch 700 loss=1.445185899734497\n",
      "epoch 800 loss=0.6930423974990845\n",
      "epoch 900 loss=0.7172635197639465\n",
      "epoch 1000 loss=0.7246091365814209\n",
      "epoch 1100 loss=0.6988819241523743\n",
      "epoch 1200 loss=0.7006752490997314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1944/2635712830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         loss = mseLoss(res, age)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#         loss2 = mseLoss(res[2], wrt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         loss = torch.stack([loss0, loss1, loss2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nEpochs = 5000\n",
    "pPeriod = 100\n",
    "cPeriod = 50\n",
    "thresh = torch.Tensor(20).float().cuda()\n",
    "\n",
    "nPoly = 1\n",
    "para = [makePoly(nback_p_t), makePoly(emoid_p_t)]\n",
    "    \n",
    "rmse = []\n",
    "\n",
    "for i in range(1):\n",
    "    back = PgiBack(10, len(para), nPoly, 3, 0.5, 1, mask=pgigcn.masks[0])\n",
    "    optim = torch.optim.Adam([back.mask], lr=2e-5, weight_decay=2e-5)\n",
    "\n",
    "    trainIdcs = groups[i][0]\n",
    "    testIdcs = groups[i][1]\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    X = X[trainIdcs]\n",
    "    Y = torch.from_numpy(X_all[trainIdcs]).float().cuda()\n",
    "    \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t[trainIdcs].unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        optim.zero_grad()\n",
    "        res, y, z, e = back(X, pgigcn, 0, gen)\n",
    "#         loss = mseLoss(res, age)\n",
    "        loss = ceLoss(res, gen)\n",
    "        loss.backward()\n",
    "#         loss2 = mseLoss(res[2], wrt)\n",
    "#         loss = torch.stack([loss0, loss1, loss2])\n",
    "#         torch.sum(loss).backward()\n",
    "        optim.step()\n",
    "        if (epoch % pPeriod == 0 or epoch == nEpochs-1):\n",
    "            print(f'epoch {epoch} loss={(float(loss))}')\n",
    "        if torch.all(loss < thresh):\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    print('Finished training')\n",
    "    \n",
    "#     pgigcn.load_state_dict(torch.load(savePath))\n",
    "    pgigcn.eval()\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    Y = torch.from_numpy(X_all).float().cuda()\n",
    "        \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t.unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "\n",
    "    gen0 = gen.clone().detach()\n",
    "    gen0[testIdcs] = 0\n",
    "    wrt0 = wrt.clone().detach()\n",
    "    wrt0[testIdcs] = 0\n",
    "    age0 = age.clone().detach()\n",
    "    age0[testIdcs] = 0\n",
    "    \n",
    "    res = back(X, pgigcn, 0, gen0)\n",
    "#     loss = mseLoss(res[0][testIdcs].detach(), age[testIdcs]).cpu().numpy()**0.5\n",
    "    frac1 = torch.sum(torch.argmax(res[0].detach(), dim=1)[testIdcs] \n",
    "                     == torch.argmax(gen[testIdcs], dim=1))/testIdcs.shape[0]\n",
    "#     loss2 = mseLoss(res[2][testIdcs].detach(), wrt[testIdcs]).cpu().numpy()**0.5\n",
    "    \n",
    "#     rmse.append((float(loss0), float(frac1), float(loss2)))\n",
    "    rmse.append((float(frac1)))\n",
    "    print(i, end=' ')\n",
    "    print(rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "358780df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X = torch.stack(para, dim=1)\n",
    "    X = X[trainIdcs]\n",
    "    Y = torch.from_numpy(X_all[trainIdcs]).float().cuda()\n",
    "    \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t[trainIdcs].unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "    \n",
    "    res, es = pgigcn(X, age, gen, wrt)\n",
    "\n",
    "e0 = es[0].detach().cpu().numpy()\n",
    "e1 = es[1].detach().cpu().numpy()\n",
    "e2 = es[2].detach().cpu().numpy()\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "870915dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1431.7374\n"
     ]
    }
   ],
   "source": [
    "w0, v0 = np.linalg.eig(e0)\n",
    "w1, v1 = np.linalg.eig(e1)\n",
    "w2, v2 = np.linalg.eig(e2)\n",
    "\n",
    "print(np.min(e0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f3d97c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=10, init='random', random_state=0, max_iter=2000, tol=1e-4)\n",
    "W = model.fit_transform(1500+e0)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3b70815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.3573\n",
      "-97.33789\n"
     ]
    }
   ],
   "source": [
    "print(np.max(1500+e0 - W@H))\n",
    "print(np.min(1500+e0 - W@H))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b7ee6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor([0.1707, 0.4221, 0.1876, 0.1144, 0.1839, 0.2683, 0.1689, 0.1238, 0.2664,\n",
      "        0.0038, 0.3508, 0.0657, 0.1069, 0.2383, 0.1426, 0.0263, 0.1351, 0.1501,\n",
      "        0.3490, 0.1632, 0.1463, 0.2195, 0.0413, 0.1088, 0.0788, 0.3471, 0.1032,\n",
      "        0.1614, 0.2908, 0.1388, 0.1463, 0.0319, 0.2627, 0.0225, 0.1839, 0.3734,\n",
      "        0.2008, 0.2702, 0.0094, 0.0713, 0.1088, 0.2289, 0.3977, 0.0957, 0.3321,\n",
      "        0.1220, 0.1932, 0.2983, 0.2777, 0.0750, 0.3058, 0.3583, 0.2458, 0.0150,\n",
      "        0.3227, 0.1745, 0.1989, 0.3321, 0.3902, 0.1163, 0.2251, 0.2627, 0.0788,\n",
      "        0.0000, 0.3884, 0.2495, 0.1689, 0.3959, 0.2927, 0.2608, 0.0976, 0.3415,\n",
      "        0.0056, 0.0000, 0.0638, 0.2270, 0.3002, 0.2101, 0.3621, 0.3977, 0.2101,\n",
      "        0.1632, 0.2270, 0.0056, 0.2627, 0.2964, 0.2608, 0.1782, 0.2608, 0.0957,\n",
      "        0.1220, 0.2233, 0.1707, 0.2983, 0.1163, 0.0075, 0.3865, 0.0544, 0.1670,\n",
      "        0.1745, 0.0844, 0.3790, 0.2270, 0.2477, 0.1801, 0.3433, 0.2514, 0.3133,\n",
      "        0.0657, 0.4578, 0.2045, 0.3246, 0.1782, 0.0169, 0.4053, 0.0544, 0.2008,\n",
      "        0.1407, 0.2083, 0.2083, 0.1088, 0.1857, 0.1670, 0.0075, 0.0075, 0.0675,\n",
      "        0.3865, 0.1313, 0.1088, 0.1914, 0.3377, 0.0713, 0.0769, 0.1107, 0.1801,\n",
      "        0.3246, 0.3321, 0.3227, 0.0582, 0.1745, 0.2251, 0.2927, 0.0450, 0.0338,\n",
      "        0.2758, 0.3565, 0.0319, 0.2064, 0.1876, 0.1332, 0.3096, 0.2814, 0.0976,\n",
      "        0.1013, 0.2683, 0.2101, 0.2101, 0.2777, 0.2158, 0.0356, 0.0281, 0.2402,\n",
      "        0.2702, 0.3565, 0.2608, 0.4034, 0.2720, 0.4034, 0.0000, 0.2045, 0.0882,\n",
      "        0.1313, 0.3077, 0.1295, 0.0019, 0.4015, 0.0957, 0.2383, 0.3302, 0.4034,\n",
      "        0.1726, 0.2795, 0.2045, 0.0469, 0.0206, 0.2270, 0.0788, 0.0713, 0.3246,\n",
      "        0.1745, 0.0375, 0.0994, 0.2589, 0.3246, 0.1614, 0.2345, 0.1876, 0.1407,\n",
      "        0.0000, 0.3039, 0.0788, 0.2645, 0.0413, 0.0000, 0.3921, 0.2495, 0.3902,\n",
      "        0.0488, 0.1501, 0.1088, 0.1220, 0.2777, 0.1670, 0.2983, 0.0619, 0.3227,\n",
      "        0.1764, 0.1163, 0.0300, 0.2627, 0.2702, 0.2702, 0.2458, 0.1689, 0.3640,\n",
      "        0.0638, 0.2251, 0.1295, 0.2101, 0.0450, 0.2308, 0.0000, 0.2008, 0.2758,\n",
      "        0.2420, 0.0113, 0.1332, 0.0788, 0.3827, 0.1482, 0.1407, 0.1501, 0.1351,\n",
      "        0.2008, 0.2495, 0.0244, 0.3002, 0.1989, 0.2251, 0.0075, 0.1051, 0.4015,\n",
      "        0.0338, 0.0263, 0.2683, 0.1970, 0.2345, 0.2458, 0.1426, 0.2120, 0.3377,\n",
      "        0.2158, 0.3640, 0.1501, 0.3152, 0.1370, 0.1857, 0.3996, 0.2964, 0.2083,\n",
      "        0.3340, 0.0938, 0.0600, 0.1801, 0.0375, 0.1932, 0.2364, 0.1051, 0.0525,\n",
      "        0.3565, 0.2402, 0.1820, 0.2477, 0.3546, 0.2402, 0.0225, 0.1782, 0.2008,\n",
      "        0.0713, 0.2345, 0.1332, 0.0169, 0.1144, 0.2139, 0.2026, 0.2908, 0.0169,\n",
      "        0.0263, 0.2326, 0.3771, 0.3189, 0.1482, 0.1651, 0.2645, 0.1820, 0.0244,\n",
      "        0.1126, 0.1670, 0.1726, 0.1895, 0.1764, 0.0394, 0.3959, 0.1426, 0.2176,\n",
      "        0.0000, 0.2026, 0.3227, 0.0807, 0.3902, 0.3546, 0.3227, 0.1632, 0.2289,\n",
      "        0.1257, 0.1632, 0.2045, 0.1107, 0.2139, 0.1332, 0.1463, 0.1426, 0.0919,\n",
      "        0.3583, 0.1820, 0.2420, 0.0488, 0.3415, 0.4090, 0.0000, 0.3527, 0.1069,\n",
      "        0.3208, 0.3265, 0.0432, 0.2477, 0.1726, 0.0300, 0.1632, 0.1257, 0.2289,\n",
      "        0.3827, 0.3659, 0.2552, 0.1745, 0.1595, 0.0938, 0.0563, 0.0188, 0.3884,\n",
      "        0.0000, 0.1257, 0.1463, 0.2495, 0.0694, 0.1970, 0.0413, 0.2101, 0.0000,\n",
      "        0.0919, 0.0976, 0.2083, 0.3058, 0.2383, 0.3039, 0.0244, 0.1857, 0.3021,\n",
      "        0.0000, 0.2720, 0.3846, 0.0000, 0.2326, 0.1032, 0.2195, 0.1538, 0.0750,\n",
      "        0.0338, 0.1069, 0.2026, 0.2101, 0.0000, 0.1501, 0.1445, 0.0469, 0.3865,\n",
      "        0.1857, 0.0000, 0.1220, 0.2927, 0.1069, 0.0094, 0.0525, 0.3452, 0.1651,\n",
      "        0.1126, 0.1707, 0.3921, 0.2251, 0.0394, 0.0038, 0.2026, 0.3452, 0.1726,\n",
      "        0.3640, 0.1576, 0.0113, 0.1407, 0.1707, 0.3340, 0.2158, 0.3021, 0.0000,\n",
      "        0.0300, 0.1276, 0.3152, 0.0169, 0.1088, 0.3527, 0.4053, 0.0919, 0.2270,\n",
      "        0.2664, 0.3996, 0.2308, 0.2439, 0.2383, 0.2570, 0.2402, 0.3340, 0.2852,\n",
      "        0.0188, 0.2514, 0.3546, 0.1820, 0.3471, 0.1876, 0.0488, 0.0356, 0.3415,\n",
      "        0.3358, 0.3377, 0.0525, 0.2158, 0.0788, 0.3358, 0.2927, 0.2477, 0.3208,\n",
      "        0.2589, 0.2458, 0.3959, 0.1707, 0.1970, 0.0450, 0.2814, 0.0769, 0.2214,\n",
      "        0.2289, 0.2233, 0.2083, 0.1463, 0.1876, 0.2871, 0.3546, 0.2214, 0.1351,\n",
      "        0.0563, 0.0319, 0.0281, 0.1764, 0.1576, 0.1426, 0.0563, 0.0882, 0.3002,\n",
      "        0.0131, 0.3189, 0.2871, 0.3396, 0.3659, 0.2158, 0.1501, 0.1126, 0.0976,\n",
      "        0.2777, 0.1782, 0.1126, 0.0394, 0.1857, 0.2889, 0.2552, 0.0657, 0.2308,\n",
      "        0.1782, 0.0319, 0.0056, 0.1332, 0.0131, 0.0300, 0.1839, 0.0394, 0.1445,\n",
      "        0.1238, 0.1388, 0.0019, 0.3077, 0.1013, 0.3996, 0.0976, 0.1501, 0.2345,\n",
      "        0.2458, 0.1370, 0.1745, 0.3283, 0.2364, 0.1201, 0.1220, 0.0901, 0.1595,\n",
      "        0.2026, 0.1651], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(torch.sum(es[2], dim=1) < 0))\n",
    "print(torch.sum(es[2] < 0, dim=1)/533)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b25d4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0002,  0.0087, -0.0005, -0.0004, -0.0025,  0.0031,  0.0044,  0.0018,\n",
      "         0.0031, -0.0041], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0027, -0.0021, -0.0025, -0.0019, -0.0042, -0.0003, -0.0001, -0.0026,\n",
      "        -0.0033, -0.0030], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0071, -0.0052,  0.0002, -0.0058,  0.0072,  0.0026,  0.0102, -0.0021,\n",
      "        -0.0059, -0.0036], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(pgigcn.masks[i][0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dcce56f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=65363.76171875\n",
      "epoch 10000 loss=65360.75\n",
      "epoch 20000 loss=65358.0390625\n",
      "epoch 30000 loss=65355.515625\n",
      "epoch 40000 loss=65353.1015625\n",
      "epoch 50000 loss=65350.7578125\n",
      "epoch 60000 loss=65348.46484375\n",
      "epoch 70000 loss=65346.2109375\n",
      "epoch 80000 loss=65343.99609375\n",
      "epoch 90000 loss=65341.8046875\n",
      "epoch 100000 loss=65339.63671875\n",
      "epoch 110000 loss=65337.4921875\n",
      "epoch 120000 loss=65335.390625\n",
      "epoch 130000 loss=65333.296875\n",
      "epoch 140000 loss=65331.23828125\n",
      "epoch 150000 loss=65329.17578125\n",
      "epoch 160000 loss=65327.15625\n",
      "epoch 170000 loss=65325.15234375\n",
      "epoch 180000 loss=65323.16796875\n",
      "epoch 190000 loss=65321.203125\n",
      "epoch 200000 loss=65319.25390625\n",
      "epoch 210000 loss=65317.33203125\n",
      "epoch 220000 loss=65315.41796875\n",
      "epoch 230000 loss=65313.53125\n",
      "epoch 240000 loss=65311.65625\n",
      "epoch 250000 loss=65309.796875\n",
      "epoch 260000 loss=65307.96484375\n",
      "epoch 270000 loss=65306.1328125\n",
      "epoch 280000 loss=65304.33203125\n",
      "epoch 290000 loss=65302.54296875\n",
      "epoch 300000 loss=65300.76953125\n",
      "epoch 310000 loss=65299.01171875\n",
      "epoch 320000 loss=65297.265625\n",
      "epoch 330000 loss=65295.5390625\n",
      "epoch 340000 loss=65293.8203125\n",
      "epoch 350000 loss=65292.12109375\n",
      "epoch 360000 loss=65290.43359375\n",
      "epoch 370000 loss=65288.76171875\n",
      "epoch 380000 loss=65287.09375\n",
      "epoch 390000 loss=65285.4453125\n",
      "epoch 400000 loss=65283.80859375\n",
      "epoch 410000 loss=65282.1796875\n",
      "epoch 420000 loss=65280.5703125\n",
      "epoch 430000 loss=65278.96484375\n",
      "epoch 440000 loss=65277.37109375\n",
      "epoch 450000 loss=65275.78515625\n",
      "epoch 460000 loss=65274.21484375\n",
      "epoch 470000 loss=65272.6484375\n",
      "epoch 480000 loss=65271.09765625\n",
      "epoch 490000 loss=65269.55078125\n",
      "epoch 499999 loss=65268.01171875\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# class USolve(nn.Module):\n",
    "#     def __init__(self, nSub, nLat, nGraph):\n",
    "#         super(USolve, self).__init__()\n",
    "#         self.U = nn.Parameter(torch.rand(nSub,nLat).float().cuda())\n",
    "#         self.mats = []\n",
    "#         for i in range(nGraph):\n",
    "#             self.mats.append(torch.rand(nLat, nSub).float().cuda())\n",
    "#         self.params = [self.U] + self.mats\n",
    "        \n",
    "#     def forward(self):\n",
    "#         return [self.U@m for m in self.mats]\n",
    "\n",
    "# solve = USolve(es[0].shape[0], 10, 3)\n",
    "optim = torch.optim.Adam(solve.params, lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "nEpochs = 500_000\n",
    "pPeriod = 10_000\n",
    "thresh = 200\n",
    "\n",
    "for epoch in range(nEpochs):\n",
    "    optim.zero_grad()\n",
    "    res = solve()\n",
    "    loss = 0\n",
    "    for i,r in enumerate(res[0:1]):\n",
    "        loss += mseLoss(es[i], res[i])\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if (epoch % pPeriod == 0 or epoch == nEpochs-1):\n",
    "        print(f'epoch {epoch} loss={(float(loss))}')\n",
    "    if loss < thresh:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "363d1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn.dense import DenseGraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, w, nPara, nPoly, nTgts, dp=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn0 = DenseGraphConv(nPara*nPoly*arith(263),w).float().cuda()\n",
    "        self.gcnAge = DenseGraphConv(w,1).float().cuda()\n",
    "        self.gcnGen = DenseGraphConv(w,2).float().cuda()\n",
    "        self.gcnWrt = DenseGraphConv(w,1).float().cuda()\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        e = torch.ones(2*[x.shape[0]]).float().cuda()/x.shape[0]\n",
    "        e = mask(e)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.gcn0(x,e)).squeeze()\n",
    "        age = self.gcnAge(x,e).squeeze(0)\n",
    "        gen = self.gcnGen(x,e).squeeze(0)\n",
    "        wrt = self.gcnWrt(x,e).squeeze(0)\n",
    "        tab = {'age': age, 'gender': gen, 'wrat': wrt}\n",
    "        res = [tab[key] for key in kwargs]\n",
    "        return res\n",
    "#         return res[0] if len(res) == 1 else res\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, w, nPara, nPoly, nTgts, dp=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.gcn0 = nn.Linear(nPara*nPoly*arith(263),w).float().cuda()\n",
    "        self.gcnAge = nn.Linear(w,1).float().cuda()\n",
    "        self.gcnGen = nn.Linear(w,2).float().cuda()\n",
    "        self.gcnWrt = nn.Linear(w,1).float().cuda()\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.gcn0(x)).squeeze()\n",
    "#         age = self.gcnAge(x)\n",
    "#         gen = self.gcnGen(x)\n",
    "        wrt = self.gcnWrt(x)\n",
    "        tab = {'age': age, 'gender': gen, 'wrat': wrt}\n",
    "        res = [tab[key] for key in kwargs]\n",
    "        return res\n",
    "#         return res[0] if len(res) == 1 else res\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "59676606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = torch.einsum('abcd,bcde->ae', X, pgigcn.masks[0]).detach().cpu().numpy()\n",
    "m = pgigcn.masks[0].reshape(-1,10).detach().cpu().numpy()\n",
    "\n",
    "plt.hist(y)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd025d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
