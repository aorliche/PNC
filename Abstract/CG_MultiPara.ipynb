{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884a3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load meta dict\n",
    "\n",
    "with open('../../PNC/AllSubjectsMeta.bin', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "# Load rest subject ids and splits\n",
    "\n",
    "with open('../../Work/Abstract/PaperBin/AllThreeSplit.bin', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "    subids = splits['allThreeYesWrat']\n",
    "    groups = splits['groups']\n",
    "    \n",
    "print(len(subids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7592a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "subidsNp = np.array(subids)\n",
    "\n",
    "# Load timeseries\n",
    "\n",
    "def loadSeries(prefix, para, idx):\n",
    "    with open('{:}/{:}_fmri_power264/timeseries/{:}.bin'.format(prefix, para, idx), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "rest_ts = [loadSeries('../../PNC', 'rest', meta[subid]['rest']) for subid in subidsNp]\n",
    "nback_ts = [loadSeries('../../PNC', 'nback', meta[subid]['nback']) for subid in subidsNp]\n",
    "emoid_ts = [loadSeries('../../PNC', 'emoid', meta[subid]['emoid']) for subid in subidsNp]\n",
    "\n",
    "print('Loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679fb558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalizeSubjects(subjects):\n",
    "    for i in range(len(subjects)):\n",
    "        subj = subjects[i]\n",
    "        subj -= np.mean(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        subj /= np.std(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        if np.sum(np.isnan(subj)) > 0:\n",
    "            print(i)\n",
    "        if np.sum(np.isinf(subj)) > 0:\n",
    "            print(i)\n",
    "\n",
    "normalizeSubjects(rest_ts)\n",
    "normalizeSubjects(nback_ts)\n",
    "normalizeSubjects(emoid_ts)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab449c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 264, 264)\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Calculate pearson matrices\n",
    "\n",
    "rest_p = np.stack([np.corrcoef(sub) for sub in rest_ts])\n",
    "nback_p = np.stack([np.corrcoef(sub) for sub in nback_ts])\n",
    "emoid_p = np.stack([np.corrcoef(sub) for sub in emoid_ts])\n",
    "\n",
    "print(rest_p.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfc1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 322\n",
      "[[223   1   0]\n",
      " [190   0   1]\n",
      " [197   0   1]\n",
      " [145   1   0]\n",
      " [148   0   1]\n",
      " [142   0   1]\n",
      " [123   1   0]\n",
      " [176   1   0]\n",
      " [129   0   1]\n",
      " [173   1   0]]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors (right now just ages, maleness, and femaless)\n",
    "\n",
    "males = 0\n",
    "females = 0\n",
    "\n",
    "X_all = []\n",
    "for subid in subidsNp:\n",
    "    subj = meta[subid]\n",
    "    maleness = 1 if subj['meta']['Gender'] == 'M' else 0\n",
    "    femaleness = 1 if maleness == 0 else 0\n",
    "    feat = np.array([subj['meta']['AgeInMonths'], maleness, femaleness])\n",
    "    X_all.append(feat)\n",
    "    if maleness == 1:\n",
    "        males += 1\n",
    "    if femaleness == 1:\n",
    "        females += 1\n",
    "X_all = np.vstack(X_all)\n",
    "\n",
    "print(f'{males} {females}')\n",
    "print(X_all[10:20])\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbd2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convertTorch(p):\n",
    "    t = torch.from_numpy(p).float()\n",
    "    u = []\n",
    "    for i in range(t.shape[0]):\n",
    "        u.append(t[i][torch.triu_indices(264,264,offset=1).unbind()])\n",
    "    return torch.stack(u).cuda()\n",
    "\n",
    "def normalizeP(p):\n",
    "    return p - torch.mean(p, dim=1, keepdim=True)\n",
    "\n",
    "rest_p_t = convertTorch(rest_p)\n",
    "nback_p_t = convertTorch(nback_p)\n",
    "emoid_p_t = convertTorch(emoid_p)\n",
    "\n",
    "# rest_p_t = normalizeP(rest_p_t)\n",
    "# nback_p_t = normalizeP(nback_p_t)\n",
    "# emoid_p_t = normalizeP(emoid_p_t)\n",
    "\n",
    "print(rest_p_t.shape)\n",
    "print(nback_p_t.shape)\n",
    "print(emoid_p_t.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b8a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "wratDict = dict()\n",
    "\n",
    "with open('../../PNC/wrat.csv', 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        wratDict[line[0]] = {'raw': line[2], 'std': line[3]}\n",
    "\n",
    "wrat = []\n",
    "\n",
    "for key in subids:\n",
    "    wrat.append(float(wratDict[str(key)]['std']))\n",
    "    \n",
    "wrat = np.array(wrat)\n",
    "wrat_t = torch.from_numpy(wrat).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad268dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def arith(n):\n",
    "    return int(n*(n+1)/2)\n",
    "\n",
    "def createCG(nPara, layerSizes, lossFn, dp=0.5, negSlope=0):\n",
    "    class CG(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CG, self).__init__()\n",
    "            self.nPara = nPara\n",
    "            self.heads = []\n",
    "            self.params = []\n",
    "            for n in range(arith(nPara)):\n",
    "                layers = []\n",
    "                for i in range(len(layerSizes)-1):\n",
    "                    layers.append(nn.Linear(layerSizes[i], layerSizes[i+1]).float().cuda())\n",
    "                    self.params.append(layers[-1])\n",
    "                layers = nn.Sequential(*layers)\n",
    "                self.heads.append(layers)\n",
    "            self.params = [layer.weight for layer in self.params] + [layer.bias for layer in self.params]\n",
    "            self.loss = lossFn\n",
    "            self.dp = nn.Dropout(p=dp)\n",
    "            self.relu = nn.LeakyReLU(negative_slope=negSlope)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.dp(x)\n",
    "            res = []\n",
    "            for n in range(arith(self.nPara)):\n",
    "                yA = x[:,floor(n/self.nPara),0,:]\n",
    "                yB = x[:,floor(n%self.nPara),1,:]\n",
    "                y = torch.cat([yA,yB],dim=1)\n",
    "                for layer in self.heads[n]:\n",
    "                    y = self.relu(layer(y)) if layer != self.heads[n][-1] else layer(y).squeeze()\n",
    "                res.append(y)\n",
    "            return torch.stack(res, dim=1)\n",
    "    return CG()\n",
    "\n",
    "\n",
    "def trainCG(cg, trainFeat, trainLabels, nEpochs=50, bSize=1000, pPeriod=1000, lr=2e-5, wd=2e-5, verbose=True, thresh=150):\n",
    "    N = trainLabels.shape[0]\n",
    "    allIdcs = torch.arange(N).long().cuda()\n",
    "    pairs = list(itertools.combinations_with_replacement(np.arange(N),2))\n",
    "    optim = torch.optim.Adam(cg.params, lr=lr, weight_decay=wd)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Training for {nEpochs} epochs')\n",
    "\n",
    "    cg.train()\n",
    "        \n",
    "    for epoch in range(nEpochs):\n",
    "        randPairs = copy.copy(pairs)\n",
    "        random.shuffle(randPairs)\n",
    "        nComplete = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f'epoch {epoch}')\n",
    "\n",
    "        for n in range(0,len(randPairs),bSize):\n",
    "            batchPairs = randPairs[n:n+bSize]\n",
    "            Ai, Bi = zip(*batchPairs)\n",
    "            Ai = torch.tensor(list(Ai)).long().cuda()\n",
    "            Bi = torch.tensor(list(Bi)).long().cuda()\n",
    "            A = trainFeat[Ai,:,:]\n",
    "            B = trainFeat[Bi,:,:]\n",
    "            a = trainLabels[Ai]\n",
    "            b = trainLabels[Bi]\n",
    "            optim.zero_grad()\n",
    "            pos = torch.stack([A,B],dim=2)\n",
    "            neg = torch.stack([B,A],dim=2)\n",
    "            pres = cg(pos)\n",
    "            nres = cg(neg)\n",
    "#             t = pres-nres-(a-b)\n",
    "#             r = pres+nres\n",
    "#             tLoss = cg.loss(t, torch.zeros(t.shape).float().cuda())\n",
    "#             rLoss = cg.loss(r, torch.zeros(r.shape).float().cuda())\n",
    "#             (tLoss+rLoss).backward()\n",
    "            pp = pres-(a-b)\n",
    "            nn = nres-(b-a)\n",
    "            pLoss = cg.loss(pp, torch.zeros(pp.shape).float().cuda())\n",
    "            nLoss = cg.loss(nn, torch.zeros(nn.shape).float().cuda())\n",
    "            rLoss = 0#1*cg.loss(pres+nres, torch.zeros(pp.shape).float().cuda())\n",
    "            (pLoss+nLoss+rLoss).backward()\n",
    "            optim.step()\n",
    "            if n % pPeriod == 0 and verbose:\n",
    "                print(f'\\tpLoss={pLoss} nLoss={nLoss}')\n",
    "#                 print(f'\\ttLoss={tLoss} rLoss={rLoss}')\n",
    "            if pLoss < thresh:\n",
    "                break\n",
    "        if pLoss < thresh:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "                \n",
    "    if verbose:\n",
    "        print(f'Completed {nEpochs*len(pairs)} comparisons') if verbose else None\n",
    "\n",
    "def evalCG(cg, trainFeat, trainLabels, testFeat):\n",
    "    N = testFeat.shape[0]\n",
    "    Ntrain = trainFeat.shape[0]\n",
    "\n",
    "#     wp = np.zeros(N)\n",
    "#     wn = np.zeros(N)\n",
    "#     up = np.zeros(N)\n",
    "#     un = np.zeros(N)\n",
    "    res = torch.zeros(N)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(N):\n",
    "            part = testFeat[i:i+1]\n",
    "            A = part.expand(trainFeat.shape)\n",
    "            B = trainFeat\n",
    "            b = trainLabels.unsqueeze(1)\n",
    "            pos = torch.stack([A,B],dim=2)\n",
    "            neg = torch.stack([B,A],dim=2)\n",
    "            pdelta = cg(pos)\n",
    "            ndelta = cg(neg)\n",
    "            res[i] = torch.mean(pdelta-ndelta+2*b)/2\n",
    "#             pres = torch.mean(pdelta + b)\n",
    "#             nres = torch.mean(b - ndelta)\n",
    "#             res[i] = (pres+nres)/2\n",
    "\n",
    "#             wp[i] = pres.detach().cpu().numpy()\n",
    "#             wn[i] = nres.detach().cpu().numpy()\n",
    "#             up[i] = torch.std(pdelta).detach().cpu().numpy()\n",
    "#             un[i] = torch.std(ndelta).detach().cpu().numpy()\n",
    "        \n",
    "#     return wp,wn,up,un\n",
    "    return res\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3e792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=483.1373291015625 nLoss=482.896484375\n",
      "\tpLoss=390.46124267578125 nLoss=390.9689025878906\n",
      "\tpLoss=364.5735778808594 nLoss=365.931884765625\n",
      "\tpLoss=311.6099853515625 nLoss=309.0485534667969\n",
      "\tpLoss=244.58265686035156 nLoss=240.88641357421875\n",
      "\tpLoss=180.76129150390625 nLoss=180.8467254638672\n",
      "\tpLoss=138.0111846923828 nLoss=142.1614532470703\n",
      "\tpLoss=121.11915588378906 nLoss=119.94320678710938\n",
      "epoch 1\n",
      "\tpLoss=136.9781494140625 nLoss=131.78274536132812\n",
      "\tpLoss=106.66012573242188 nLoss=105.58208465576172\n",
      "\tpLoss=99.29898071289062 nLoss=98.96348571777344\n",
      "\tpLoss=90.45470428466797 nLoss=97.81735229492188\n",
      "\tpLoss=85.58731842041016 nLoss=86.44679260253906\n",
      "\tpLoss=67.29454040527344 nLoss=68.74165344238281\n",
      "\tpLoss=70.41695404052734 nLoss=66.3091049194336\n",
      "Early stopping\n",
      "Completed 1423110 comparisons\n",
      "0 14.296786949011645\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=478.3077392578125 nLoss=478.17791748046875\n",
      "\tpLoss=381.24737548828125 nLoss=380.6124267578125\n",
      "\tpLoss=321.21136474609375 nLoss=320.9117431640625\n",
      "\tpLoss=287.30267333984375 nLoss=287.6386413574219\n",
      "\tpLoss=226.74671936035156 nLoss=228.93174743652344\n",
      "\tpLoss=192.34872436523438 nLoss=194.20745849609375\n",
      "\tpLoss=163.7544708251953 nLoss=163.53651428222656\n",
      "\tpLoss=126.21724700927734 nLoss=120.61995697021484\n",
      "epoch 1\n",
      "\tpLoss=119.0565414428711 nLoss=125.0322494506836\n",
      "\tpLoss=108.095458984375 nLoss=111.84718322753906\n",
      "\tpLoss=98.25860595703125 nLoss=94.34566497802734\n",
      "\tpLoss=86.25223541259766 nLoss=78.5448989868164\n",
      "\tpLoss=76.13034057617188 nLoss=78.4669189453125\n",
      "\tpLoss=70.02806854248047 nLoss=70.69754791259766\n",
      "Early stopping\n",
      "Completed 1423110 comparisons\n",
      "1 14.821192770049013\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=477.53265380859375 nLoss=477.9945373535156\n",
      "\tpLoss=370.4727478027344 nLoss=371.72027587890625\n",
      "\tpLoss=309.3351135253906 nLoss=308.61871337890625\n",
      "\tpLoss=268.6573181152344 nLoss=268.2284240722656\n",
      "\tpLoss=215.41830444335938 nLoss=211.37448120117188\n",
      "\tpLoss=194.03274536132812 nLoss=193.880859375\n",
      "\tpLoss=137.2645263671875 nLoss=136.51177978515625\n",
      "\tpLoss=134.1361083984375 nLoss=134.3822021484375\n",
      "epoch 1\n",
      "\tpLoss=120.07975006103516 nLoss=117.98052215576172\n",
      "\tpLoss=95.16993713378906 nLoss=99.85520935058594\n",
      "\tpLoss=86.26009368896484 nLoss=91.53271484375\n",
      "\tpLoss=83.18572235107422 nLoss=77.68778228759766\n",
      "\tpLoss=71.69403076171875 nLoss=71.5823974609375\n",
      "Early stopping\n",
      "Completed 1423110 comparisons\n",
      "2 15.9138064918657\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=509.2662048339844 nLoss=509.1440734863281\n",
      "\tpLoss=401.8392028808594 nLoss=399.3312072753906\n",
      "\tpLoss=327.8287353515625 nLoss=325.9189758300781\n",
      "\tpLoss=257.8390197753906 nLoss=260.67529296875\n",
      "\tpLoss=220.12608337402344 nLoss=219.6860809326172\n",
      "\tpLoss=185.72137451171875 nLoss=185.42007446289062\n",
      "\tpLoss=163.25802612304688 nLoss=162.45394897460938\n",
      "\tpLoss=117.25139617919922 nLoss=117.37572479248047\n",
      "epoch 1\n",
      "\tpLoss=113.43889617919922 nLoss=114.79261016845703\n",
      "\tpLoss=92.54119873046875 nLoss=90.9403305053711\n",
      "\tpLoss=83.73863220214844 nLoss=86.99818420410156\n",
      "\tpLoss=71.76219940185547 nLoss=77.3797607421875\n",
      "\tpLoss=70.83802032470703 nLoss=70.12409210205078\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "3 14.782493361586589\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=475.74749755859375 nLoss=475.6679992675781\n",
      "\tpLoss=422.6095886230469 nLoss=420.9778137207031\n",
      "\tpLoss=341.8177490234375 nLoss=342.88226318359375\n",
      "\tpLoss=273.28350830078125 nLoss=274.65020751953125\n",
      "\tpLoss=206.16053771972656 nLoss=209.17431640625\n",
      "\tpLoss=205.52452087402344 nLoss=206.70274353027344\n",
      "\tpLoss=159.77650451660156 nLoss=158.06570434570312\n",
      "\tpLoss=125.28475189208984 nLoss=124.33283996582031\n",
      "epoch 1\n",
      "\tpLoss=107.2223129272461 nLoss=99.7608642578125\n",
      "\tpLoss=110.7734603881836 nLoss=107.43256378173828\n",
      "\tpLoss=81.3360595703125 nLoss=83.79579162597656\n",
      "\tpLoss=82.48822021484375 nLoss=78.87776947021484\n",
      "\tpLoss=69.10992431640625 nLoss=76.34028625488281\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "4 13.234105779618538\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=418.37432861328125 nLoss=418.45166015625\n",
      "\tpLoss=441.1098327636719 nLoss=442.3258361816406\n",
      "\tpLoss=315.7231140136719 nLoss=320.95440673828125\n",
      "\tpLoss=303.61962890625 nLoss=302.4366760253906\n",
      "\tpLoss=208.2314910888672 nLoss=211.13800048828125\n",
      "\tpLoss=201.67385864257812 nLoss=201.30499267578125\n",
      "\tpLoss=156.41177368164062 nLoss=153.32650756835938\n",
      "\tpLoss=115.76214599609375 nLoss=117.74383544921875\n",
      "epoch 1\n",
      "\tpLoss=111.56851959228516 nLoss=113.06159973144531\n",
      "\tpLoss=108.12052154541016 nLoss=104.96453094482422\n",
      "\tpLoss=92.81871795654297 nLoss=94.24239349365234\n",
      "\tpLoss=80.98107147216797 nLoss=80.53022003173828\n",
      "\tpLoss=69.48192596435547 nLoss=71.6084976196289\n",
      "\tpLoss=69.08720397949219 nLoss=63.70875930786133\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "5 14.082729048245197\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=470.34808349609375 nLoss=470.78857421875\n",
      "\tpLoss=392.8361511230469 nLoss=393.0875244140625\n",
      "\tpLoss=332.8296203613281 nLoss=329.2405700683594\n",
      "\tpLoss=241.66578674316406 nLoss=245.8110809326172\n",
      "\tpLoss=235.10208129882812 nLoss=238.5205841064453\n",
      "\tpLoss=173.93145751953125 nLoss=174.4313507080078\n",
      "\tpLoss=133.3170166015625 nLoss=135.0870361328125\n",
      "\tpLoss=135.30059814453125 nLoss=138.68948364257812\n",
      "epoch 1\n",
      "\tpLoss=123.20391845703125 nLoss=123.44009399414062\n",
      "\tpLoss=98.02965545654297 nLoss=96.5836410522461\n",
      "\tpLoss=92.37733459472656 nLoss=89.5234603881836\n",
      "\tpLoss=85.48553466796875 nLoss=84.8359603881836\n",
      "\tpLoss=73.9653091430664 nLoss=72.96143341064453\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "6 15.182124941015648\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=488.8565673828125 nLoss=489.3075256347656\n",
      "\tpLoss=396.9359130859375 nLoss=394.49267578125\n",
      "\tpLoss=369.8414001464844 nLoss=371.7562561035156\n",
      "\tpLoss=273.7446594238281 nLoss=274.2588806152344\n",
      "\tpLoss=224.8965606689453 nLoss=219.3867645263672\n",
      "\tpLoss=209.01535034179688 nLoss=213.22547912597656\n",
      "\tpLoss=160.9779052734375 nLoss=166.85137939453125\n",
      "\tpLoss=135.45323181152344 nLoss=132.6527557373047\n",
      "epoch 1\n",
      "\tpLoss=145.09181213378906 nLoss=140.74571228027344\n",
      "\tpLoss=113.64425659179688 nLoss=115.64441680908203\n",
      "\tpLoss=96.3619155883789 nLoss=97.85591888427734\n",
      "\tpLoss=97.05266571044922 nLoss=95.53084564208984\n",
      "\tpLoss=79.26083374023438 nLoss=84.49287414550781\n",
      "\tpLoss=63.97922897338867 nLoss=68.45333099365234\n",
      "\tpLoss=66.44779968261719 nLoss=67.31659698486328\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "7 13.183582754625045\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=508.10675048828125 nLoss=508.08758544921875\n",
      "\tpLoss=386.06561279296875 nLoss=386.2066650390625\n",
      "\tpLoss=326.2427062988281 nLoss=325.7922058105469\n",
      "\tpLoss=267.7438659667969 nLoss=270.3291931152344\n",
      "\tpLoss=204.31546020507812 nLoss=204.31825256347656\n",
      "\tpLoss=173.71849060058594 nLoss=173.07882690429688\n",
      "\tpLoss=145.11227416992188 nLoss=145.92724609375\n",
      "\tpLoss=104.52789306640625 nLoss=103.74242401123047\n",
      "epoch 1\n",
      "\tpLoss=107.3870620727539 nLoss=114.98490142822266\n",
      "\tpLoss=105.51428985595703 nLoss=100.65760040283203\n",
      "\tpLoss=88.80332946777344 nLoss=90.17250061035156\n",
      "\tpLoss=73.35496520996094 nLoss=79.22638702392578\n",
      "\tpLoss=70.42428588867188 nLoss=68.33694458007812\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "8 15.71625451969806\n",
      "Training for 10 epochs\n",
      "epoch 0\n",
      "\tpLoss=528.2626342773438 nLoss=527.9761352539062\n",
      "\tpLoss=427.26806640625 nLoss=427.79815673828125\n",
      "\tpLoss=330.1418762207031 nLoss=331.5425720214844\n",
      "\tpLoss=303.2776184082031 nLoss=306.00927734375\n",
      "\tpLoss=239.90379333496094 nLoss=242.7109832763672\n",
      "\tpLoss=161.9151611328125 nLoss=163.55487060546875\n",
      "\tpLoss=144.0528106689453 nLoss=142.8933868408203\n",
      "\tpLoss=129.86500549316406 nLoss=132.3577117919922\n",
      "epoch 1\n",
      "\tpLoss=120.31685638427734 nLoss=118.98170471191406\n",
      "\tpLoss=113.84355926513672 nLoss=114.75202941894531\n",
      "\tpLoss=95.03321075439453 nLoss=95.67472839355469\n",
      "\tpLoss=86.81017303466797 nLoss=83.3388442993164\n",
      "\tpLoss=76.90116882324219 nLoss=78.14482879638672\n",
      "\tpLoss=76.1317367553711 nLoss=73.10284423828125\n",
      "\tpLoss=70.5904312133789 nLoss=73.07455444335938\n",
      "Early stopping\n",
      "Completed 1428450 comparisons\n",
      "9 11.300121975130713\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "para = [nback_p_t, emoid_p_t]\n",
    "d = para[0].shape[1]\n",
    "rmse = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    trainIdcs = groups[i][0]\n",
    "    testIdcs = groups[i][1]\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    Y = torch.from_numpy(X_all).float().cuda()\n",
    "    \n",
    "    gen = Y[trainIdcs][:,1:]\n",
    "    wrt = wrat_t[trainIdcs].unsqueeze(1)\n",
    "    age = Y[trainIdcs][:,0].unsqueeze(1)\n",
    "    \n",
    "    cg = createCG(len(para), [2*d,100,1], torch.nn.MSELoss(), dp=0.5, negSlope=0)\n",
    "    trainCG(cg, X[trainIdcs], wrt, nEpochs=10, bSize=500, pPeriod=20000, lr=1e-4, wd=1e-4, thresh=60) \n",
    "#     wp,wn,up,un = evalCG(cg, X[trainIdcs], wrt, X[testIdcs])\n",
    "    res = evalCG(cg, X[trainIdcs], wrt, X[testIdcs])\n",
    "    \n",
    "    rmse.append(np.mean((res.detach().cpu().numpy() - wrat_t[testIdcs].detach().cpu().numpy())**2)**0.5)\n",
    "    print(f'{i} {rmse[-1]}')\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2ddbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.296786949011645\n",
      "14.821192770049013\n",
      "15.9138064918657\n",
      "14.782493361586589\n",
      "13.234105779618538\n",
      "14.082729048245197\n",
      "15.182124941015648\n",
      "13.183582754625045\n",
      "15.71625451969806\n",
      "11.300121975130713\n",
      "res 14.251319859084614\n"
     ]
    }
   ],
   "source": [
    "for a in rmse:\n",
    "    print(a)\n",
    "\n",
    "print(f'res {sum(rmse)/10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5301fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  7.7489, -59.7659,  28.6001,  30.5715,  35.3205],\n",
      "        [ 46.6275,   7.2129,  66.9271,  89.7787,  84.0614],\n",
      "        [-27.2935, -78.1412,  -3.0699,  25.9270,  10.2581],\n",
      "        [-29.9585, -78.3382, -21.9948,  -9.0342, -21.3083],\n",
      "        [-24.9221, -83.1050, -16.6878,  16.0613,   4.0183]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    D = torch.zeros(3,trainIdcs.shape[0],trainIdcs.shape[0]).float().cuda()\n",
    "    for j in range(trainIdcs.shape[0]):\n",
    "        A = X[j].unsqueeze(0).expand(X[trainIdcs].shape)\n",
    "        B = X[trainIdcs]\n",
    "        pos = torch.stack([A,B],dim=2)\n",
    "        neg = torch.stack([B,A],dim=2)\n",
    "        pdelta = cg(pos)\n",
    "        ndelta = cg(neg)\n",
    "        D[:,j,:] = (pdelta-ndelta).T.detach()\n",
    "\n",
    "print(D[0,0:5,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de62db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.0000, -53.1967,  27.9468,  30.2650,  30.1213],\n",
      "        [ 53.1967,   0.0000,  72.5342,  84.0585,  83.5832],\n",
      "        [-27.9468, -72.5342,   0.0000,  23.9609,  13.4729],\n",
      "        [-30.2650, -84.0585, -23.9609,   0.0000, -18.6848],\n",
      "        [-30.1213, -83.5832, -13.4729,  18.6848,   0.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# D0 = (D[0] + D[1] + D[2])/3\n",
    "D0 = 0.5*(D[0]-D[0].T)\n",
    "print(D0[0:5,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239c38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.eye(wrt.shape[0]).float().cuda()\n",
    "E0 = wrt.T@I-I@wrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73439bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,  31., -21., -21., -14.],\n",
      "        [-31.,   0., -52., -52., -45.],\n",
      "        [ 21.,  52.,   0.,   0.,   7.],\n",
      "        [ 21.,  52.,   0.,   0.,   7.],\n",
      "        [ 14.,  45.,  -7.,  -7.,   0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(E0[0:5,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11dc12a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34716, 534])\n",
      "tensor([])\n",
      "tensor(365)\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "X1,res,rank,sigma = torch.linalg.lstsq(X[trainIdcs,0,:].cpu(),D0.cpu(),driver='gelsy')\n",
    "print(X1.shape)\n",
    "print(res)\n",
    "print(rank)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aebd5579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34716, 34716])\n",
      "tensor([])\n",
      "tensor(364)\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "A,res,rank,sigma = torch.linalg.lstsq(X[trainIdcs,0,:].cpu(),X1.T)\n",
    "print(A.shape)\n",
    "print(res)\n",
    "print(rank)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0f4e2bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17508212, 0.0953139 ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "np.random.seed(1)\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(A)\n",
    "svd.singular_values_\n",
    "svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51ce9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34716, 2])\n",
      "(2, 34716)\n",
      "tensor([[ 3.7292e-02,  1.6202e-02],\n",
      "        [ 4.8243e-05, -1.5062e-03],\n",
      "        [ 2.9018e-03, -9.1920e-03],\n",
      "        ...,\n",
      "        [-9.7185e-04,  3.7991e-03],\n",
      "        [-8.3358e-04,  3.0266e-04],\n",
      "        [ 3.1793e-03,  7.2965e-03]])\n",
      "[[ 0.00397236 -0.014714   -0.00755993 ... -0.01145009  0.00210089\n",
      "  -0.01092923]\n",
      " [-0.03290593 -0.00287274 -0.00399866 ... -0.00285507  0.001163\n",
      "  -0.00722377]]\n"
     ]
    }
   ],
   "source": [
    "MA = A@svd.components_.T\n",
    "MB = svd.components_\n",
    "print(MA.shape)\n",
    "print(MB.shape)\n",
    "print(MA)\n",
    "print(MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d03b70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.0000,  12.4801, -30.0143,  ..., -17.1752, -24.4820,   4.4301],\n",
      "        [-58.2693,  -0.0000, -66.1430,  ..., -53.6112, -60.8349, -32.7653],\n",
      "        [ -6.4929,  22.2627,  -0.0000,  ...,  -5.6484, -12.3976,  14.4512],\n",
      "        ...,\n",
      "        [  0.6876,  29.1233, -10.8730,  ...,   0.0000,  -5.7128,  21.1043],\n",
      "        [-37.5083,  -7.0028, -46.5535,  ..., -34.3828,  -0.0000, -14.0213],\n",
      "        [  7.9788,  36.0734,  -4.0839,  ...,   7.7609,   1.0788,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "MB = torch.from_numpy(MB)\n",
    "XA = X[:,0,:].cpu()@MA\n",
    "XB = MB@X[:,0,:].cpu().T\n",
    "E = XA@XB\n",
    "E = E*(torch.ones(E.shape)-torch.eye(E.shape[0]))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c799cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 85.9637,  54.2466,  85.9395,  84.6867,  88.0491,  69.7205, 111.4181,\n",
      "         99.3686,  90.8362,  88.7583,  88.6305, 104.9313, 107.4452,  86.9834,\n",
      "         76.1227,  98.6855, 102.6464,  97.0766,  82.9226,  79.4212,  78.8501,\n",
      "        113.1415,  95.6813, 102.7159, 101.1084,  93.0701,  94.5229,  87.0795,\n",
      "         84.4448, 113.7204, 105.3719, 101.4805, 101.9571,  93.5321,  87.6187,\n",
      "         92.3792,  69.3356,  99.2735,  84.5599,  90.0032, 115.8602,  94.0407,\n",
      "         91.2724,  96.8521,  94.6381,  90.1724, 100.7575,  97.0485,  67.3470,\n",
      "         79.3631, 104.6621,  88.1914,  70.1744,  92.0589,  93.5873,  73.2304,\n",
      "        112.0574,  89.9538,  89.1707])\n",
      "tensor(15.8584)\n"
     ]
    }
   ],
   "source": [
    "V = torch.zeros(wrat_t.shape)\n",
    "V[trainIdcs] = wrt[:,0].cpu()\n",
    "V = V.unsqueeze(0)\n",
    "Et = torch.mean((-E+V)[testIdcs], dim=1)\n",
    "print(Et)\n",
    "print(torch.mean((Et-wrat_t[testIdcs].cpu())**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06987788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 89.,  89., 110.,  87.,  92.,  88.,  98., 105.,  90.,  93.,  92.,  96.,\n",
      "        113., 126., 105., 110.,  80.,  94.,  85., 110.,  75., 111.,  91., 110.,\n",
      "        121., 117., 117., 117.,  90., 132.,  99., 121., 128.,  98., 102.,  96.,\n",
      "         72., 105.,  93.,  93., 107., 103., 100., 116.,  95., 114., 115., 117.,\n",
      "         90.,  73.,  97., 105.,  89., 106., 108.,  89., 105., 109.,  85.])\n"
     ]
    }
   ],
   "source": [
    "print(wrat_t[testIdcs].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e283360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "def powerMethod(A, nIter=10, u=None):\n",
    "    u = torch.rand(A.shape[1])/A.shape[1] if u is None else u\n",
    "    for i in range(nIter):\n",
    "        u = A@u\n",
    "        u = u/torch.linalg.vector_norm(u)\n",
    "        if i % 10 == 0 or i + 10 > nIter:\n",
    "            print(u)\n",
    "    return u\n",
    "\n",
    "print('Complete')\n",
    "# u = powerMethod(A, nIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01b92ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2462)\n"
     ]
    }
   ],
   "source": [
    "a = torch.mean(A@u/u)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13a00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
