{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f95250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load meta dict\n",
    "\n",
    "with open('../../PNC/AllSubjectsMeta.bin', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "# Load rest subject ids and splits\n",
    "\n",
    "with open('../../Work/Abstract/PaperBin/AllThreeSplit.bin', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "    subids = splits['allThreeYesWrat']\n",
    "    groups = splits['groups']\n",
    "    \n",
    "print(len(subids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407fe22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "subidsNp = np.array(subids)\n",
    "\n",
    "# Load timeseries\n",
    "\n",
    "def loadSeries(prefix, para, idx):\n",
    "    with open('{:}/{:}_fmri_power264/timeseries/{:}.bin'.format(prefix, para, idx), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "rest_ts = [loadSeries('../../PNC', 'rest', meta[subid]['rest']) for subid in subidsNp]\n",
    "nback_ts = [loadSeries('../../PNC', 'nback', meta[subid]['nback']) for subid in subidsNp]\n",
    "emoid_ts = [loadSeries('../../PNC', 'emoid', meta[subid]['emoid']) for subid in subidsNp]\n",
    "\n",
    "print('Loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8951a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalizeSubjects(subjects):\n",
    "    for i in range(len(subjects)):\n",
    "        subj = subjects[i]\n",
    "        subj -= np.mean(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        subj /= np.std(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        if np.sum(np.isnan(subj)) > 0:\n",
    "            print(i)\n",
    "        if np.sum(np.isinf(subj)) > 0:\n",
    "            print(i)\n",
    "\n",
    "normalizeSubjects(rest_ts)\n",
    "normalizeSubjects(nback_ts)\n",
    "normalizeSubjects(emoid_ts)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d6a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 264, 264)\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Calculate pearson matrices\n",
    "\n",
    "rest_p = np.stack([np.corrcoef(sub) for sub in rest_ts])\n",
    "nback_p = np.stack([np.corrcoef(sub) for sub in nback_ts])\n",
    "emoid_p = np.stack([np.corrcoef(sub) for sub in emoid_ts])\n",
    "\n",
    "print(rest_p.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483bf314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 322\n",
      "[[223   1   0]\n",
      " [190   0   1]\n",
      " [197   0   1]\n",
      " [145   1   0]\n",
      " [148   0   1]\n",
      " [142   0   1]\n",
      " [123   1   0]\n",
      " [176   1   0]\n",
      " [129   0   1]\n",
      " [173   1   0]]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors (right now just ages, maleness, and femaless)\n",
    "\n",
    "males = 0\n",
    "females = 0\n",
    "\n",
    "X_all = []\n",
    "for subid in subidsNp:\n",
    "    subj = meta[subid]\n",
    "    maleness = 1 if subj['meta']['Gender'] == 'M' else 0\n",
    "    femaleness = 1 if maleness == 0 else 0\n",
    "    feat = np.array([subj['meta']['AgeInMonths'], maleness, femaleness])\n",
    "    X_all.append(feat)\n",
    "    if maleness == 1:\n",
    "        males += 1\n",
    "    if femaleness == 1:\n",
    "        females += 1\n",
    "X_all = np.vstack(X_all)\n",
    "\n",
    "print(f'{males} {females}')\n",
    "print(X_all[10:20])\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57717ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "torch.Size([593, 34716])\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convertTorch(p):\n",
    "    t = torch.from_numpy(p).float()\n",
    "    u = []\n",
    "    for i in range(t.shape[0]):\n",
    "        u.append(t[i][torch.triu_indices(264,264,offset=1).unbind()])\n",
    "    return torch.stack(u).cuda()\n",
    "\n",
    "def normalizeP(p):\n",
    "    return p - torch.mean(p, dim=1, keepdim=True)\n",
    "\n",
    "rest_p_t = convertTorch(rest_p)\n",
    "nback_p_t = convertTorch(nback_p)\n",
    "emoid_p_t = convertTorch(emoid_p)\n",
    "\n",
    "# rest_p_t = normalizeP(rest_p_t)\n",
    "# nback_p_t = normalizeP(nback_p_t)\n",
    "# emoid_p_t = normalizeP(emoid_p_t)\n",
    "\n",
    "print(rest_p_t.shape)\n",
    "print(nback_p_t.shape)\n",
    "print(emoid_p_t.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084aaaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "wratDict = dict()\n",
    "\n",
    "with open('../../PNC/wrat.csv', 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        wratDict[line[0]] = {'raw': line[2], 'std': line[3]}\n",
    "\n",
    "wrat = []\n",
    "\n",
    "for key in subids:\n",
    "    wrat.append(float(wratDict[str(key)]['std']))\n",
    "    \n",
    "wrat = np.array(wrat)\n",
    "wrat_t = torch.from_numpy(wrat).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159758d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import permutations\n",
    "\n",
    "def makePoly(ps, nPoly):\n",
    "    pps = []\n",
    "    for i in range(ps.shape[0]):\n",
    "        p = ps[i].flatten()\n",
    "        pp = nPoly*[None]\n",
    "        for j in range(nPoly):\n",
    "            pp[j] = p**(j+1)\n",
    "        pps.append(torch.stack(pp))\n",
    "    return torch.stack(pps)\n",
    "\n",
    "def arith(n):\n",
    "    return int(n*(n+1)/2)\n",
    "\n",
    "def mask(e):\n",
    "    return e - torch.diag(torch.diag(e.detach()))\n",
    "\n",
    "class PgiDiff(nn.Module):\n",
    "    def __init__(self, w, nPara, nTgts, dp=0.5, dp2=0.1):\n",
    "        super(PgiDiff, self).__init__()\n",
    "        self.nPara = nPara\n",
    "        self.nTgts = nTgts\n",
    "        self.masks = []\n",
    "        if type(w) == int:\n",
    "            w = (nTgts+1)*[w]\n",
    "        for i in range(nTgts+1):\n",
    "            self.masks.append(nn.Parameter(\n",
    "                1e-4*torch.randn(nPara,arith(263),w[i]).float().cuda()\n",
    "            ))\n",
    "        self.dp = nn.Dropout(p=dp)\n",
    "        self.dp2 = nn.Dropout(p=dp2)\n",
    "    \n",
    "    def getLatentsAndEdges(self, x, i, univ):\n",
    "        if univ:\n",
    "            y = torch.einsum('abc,bce->ae', x, self.masks[0])\n",
    "            z = torch.einsum('abc,bce->ae', x, self.masks[i+1])\n",
    "            e = y@z.T+z@y.T\n",
    "            return y, z, e\n",
    "        else:\n",
    "            y = torch.einsum('abc,bce->ae', x, self.masks[i+1])\n",
    "            e = y@y.T\n",
    "            return y, y, e\n",
    "        \n",
    "    def forward(self, x, y, testIdcs=None, univ=True):\n",
    "        x = self.dp(x)\n",
    "        res = []\n",
    "        for i in range(self.nTgts):\n",
    "            _, _, e = self.getLatentsAndEdges(x, i, univ)\n",
    "            if testIdcs is not None:\n",
    "                e[:,testIdcs] = 0\n",
    "            e = self.dp2(e)\n",
    "            e = mask(e)\n",
    "            e[e == 0] = float('-inf')\n",
    "            e = 1.1*F.softmax(e, dim=1)\n",
    "            e = e*y[i].unsqueeze(0)\n",
    "            res.append(torch.sum(e, dim=1))\n",
    "        return res\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77c27ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=(1574.7066650390625, 69.49566650390625, 257.9539489746094)\n",
      "epoch 200 loss=(333.02374267578125, 29.247385025024414, 165.604736328125)\n",
      "epoch 400 loss=(178.16184997558594, 24.014848709106445, 144.730712890625)\n",
      "epoch 600 loss=(131.04388427734375, 19.453920364379883, 124.00162506103516)\n",
      "epoch 800 loss=(212.84759521484375, 21.297258377075195, 115.05181121826172)\n",
      "epoch 1000 loss=(141.8602294921875, 15.439140319824219, 92.44693756103516)\n",
      "epoch 1200 loss=(57.05058288574219, 15.440918922424316, 71.92483520507812)\n",
      "epoch 1400 loss=(52.714599609375, 22.166305541992188, 64.61856079101562)\n",
      "epoch 1600 loss=(68.63125610351562, 14.600007057189941, 56.491050720214844)\n",
      "epoch 1800 loss=(45.799930572509766, 13.422881126403809, 68.13480377197266)\n",
      "epoch 2000 loss=(46.37934494018555, 12.660698890686035, 33.74162292480469)\n",
      "epoch 2200 loss=(43.656288146972656, 12.615516662597656, 49.67647933959961)\n",
      "epoch 2400 loss=(36.7657585144043, 12.338371276855469, 28.70901107788086)\n",
      "epoch 2600 loss=(35.937992095947266, 12.632678031921387, 22.12381362915039)\n",
      "Early stopping\n",
      "Finished training\n",
      "0 (24.895605087280273, 0.7666667103767395, 14.362210273742676)\n",
      "epoch 0 loss=(1529.910400390625, 69.51142883300781, 250.22918701171875)\n",
      "epoch 200 loss=(115.80096435546875, 32.0387077331543, 119.9880142211914)\n",
      "epoch 400 loss=(77.642578125, 23.022205352783203, 64.60686492919922)\n",
      "epoch 600 loss=(56.50796127319336, 16.472400665283203, 48.529541015625)\n",
      "epoch 800 loss=(41.84536361694336, 14.57559585571289, 36.443580627441406)\n",
      "epoch 1000 loss=(41.628143310546875, 14.818869590759277, 28.139562606811523)\n",
      "epoch 1200 loss=(49.78929901123047, 13.680112838745117, 32.78034210205078)\n",
      "epoch 1400 loss=(38.485992431640625, 12.426469802856445, 18.76245880126953)\n",
      "epoch 1600 loss=(31.81537628173828, 12.682493209838867, 30.381759643554688)\n",
      "epoch 1800 loss=(32.43267822265625, 12.678682327270508, 22.916362762451172)\n",
      "Early stopping\n",
      "Finished training\n",
      "1 (26.938852310180664, 0.7833333611488342, 15.599970817565918)\n",
      "epoch 0 loss=(1549.4322509765625, 69.63724517822266, 247.10421752929688)\n",
      "epoch 200 loss=(492.26165771484375, 34.09096145629883, 129.3181610107422)\n",
      "epoch 400 loss=(225.15328979492188, 23.81382942199707, 94.26383972167969)\n",
      "epoch 600 loss=(188.4402313232422, 16.904783248901367, 108.40728759765625)\n",
      "epoch 800 loss=(116.1567153930664, 14.558453559875488, 58.63197708129883)\n",
      "epoch 1000 loss=(85.06929779052734, 14.240044593811035, 119.22694396972656)\n",
      "epoch 1200 loss=(68.13096618652344, 17.211957931518555, 48.28260040283203)\n",
      "epoch 1400 loss=(57.92717361450195, 12.921440124511719, 42.91070556640625)\n",
      "epoch 1600 loss=(52.47496795654297, 12.774340629577637, 50.672481536865234)\n",
      "epoch 1800 loss=(53.72599792480469, 12.968648910522461, 83.56868743896484)\n",
      "epoch 2000 loss=(43.390506744384766, 12.545971870422363, 41.275474548339844)\n",
      "epoch 2200 loss=(40.80500411987305, 12.671871185302734, 30.886499404907227)\n",
      "epoch 2400 loss=(34.491119384765625, 12.306838989257812, 23.285398483276367)\n",
      "epoch 2600 loss=(48.23154067993164, 12.38126277923584, 24.75003433227539)\n",
      "Early stopping\n",
      "Finished training\n",
      "2 (20.273792266845703, 0.7500000596046448, 16.325719833374023)\n",
      "epoch 0 loss=(1517.2744140625, 69.54072570800781, 255.00538635253906)\n",
      "epoch 200 loss=(381.45074462890625, 27.625825881958008, 133.90768432617188)\n",
      "epoch 400 loss=(165.95655822753906, 18.05872917175293, 107.29722595214844)\n",
      "epoch 600 loss=(97.3786392211914, 15.582999229431152, 91.08207702636719)\n",
      "epoch 800 loss=(83.57982635498047, 14.815570831298828, 102.85608673095703)\n",
      "epoch 1000 loss=(63.12773513793945, 13.866059303283691, 60.02046203613281)\n",
      "epoch 1200 loss=(65.91937255859375, 14.870841979980469, 56.33011245727539)\n",
      "epoch 1400 loss=(43.11846160888672, 13.153020858764648, 46.34841537475586)\n",
      "epoch 1600 loss=(47.14475631713867, 12.842970848083496, 34.79291915893555)\n",
      "epoch 1800 loss=(44.83563232421875, 12.312676429748535, 32.20619583129883)\n",
      "epoch 2000 loss=(41.61467742919922, 11.888922691345215, 25.420162200927734)\n",
      "epoch 2200 loss=(32.177677154541016, 11.86180591583252, 21.33681297302246)\n",
      "Early stopping\n",
      "Finished training\n",
      "3 (27.16292953491211, 0.8983050584793091, 14.82560920715332)\n",
      "epoch 0 loss=(1500.102294921875, 69.40046691894531, 255.6019744873047)\n",
      "epoch 200 loss=(192.5327606201172, 26.709314346313477, 133.1545867919922)\n",
      "epoch 400 loss=(76.49636840820312, 16.877012252807617, 55.996219635009766)\n",
      "epoch 600 loss=(54.65082931518555, 15.26150894165039, 41.29671096801758)\n",
      "epoch 800 loss=(78.8564224243164, 12.94228744506836, 42.68438720703125)\n",
      "epoch 1000 loss=(39.49683380126953, 12.188955307006836, 30.302072525024414)\n",
      "Early stopping\n",
      "Finished training\n",
      "4 (26.762434005737305, 0.7796609997749329, 13.630986213684082)\n",
      "epoch 0 loss=(1521.609375, 69.53815460205078, 251.01654052734375)\n",
      "epoch 200 loss=(195.47146606445312, 29.444704055786133, 120.91770935058594)\n",
      "epoch 400 loss=(112.1795654296875, 20.82430076599121, 85.27389526367188)\n",
      "epoch 600 loss=(84.91275024414062, 13.60277271270752, 59.97908020019531)\n",
      "epoch 800 loss=(64.43739318847656, 13.002655029296875, 63.43329620361328)\n",
      "epoch 1000 loss=(60.50716781616211, 13.976825714111328, 45.32359313964844)\n",
      "epoch 1200 loss=(46.942447662353516, 12.341124534606934, 41.127567291259766)\n",
      "epoch 1400 loss=(53.43095397949219, 12.658645629882812, 43.680301666259766)\n",
      "epoch 1600 loss=(46.3067626953125, 12.57440185546875, 22.432477951049805)\n",
      "Early stopping\n",
      "Finished training\n",
      "5 (23.97622299194336, 0.7966101765632629, 14.559538841247559)\n",
      "epoch 0 loss=(1544.569091796875, 69.52204895019531, 249.14817810058594)\n",
      "epoch 200 loss=(436.5211486816406, 28.408077239990234, 216.9506072998047)\n",
      "epoch 400 loss=(180.88848876953125, 19.449655532836914, 118.02677154541016)\n",
      "epoch 600 loss=(110.78929901123047, 22.54104995727539, 100.26644897460938)\n",
      "epoch 800 loss=(71.72941589355469, 18.337039947509766, 77.74713134765625)\n",
      "epoch 1000 loss=(54.2315788269043, 13.706501960754395, 50.870296478271484)\n",
      "epoch 1200 loss=(63.364219665527344, 13.061131477355957, 50.118412017822266)\n",
      "epoch 1400 loss=(45.66379165649414, 12.945954322814941, 42.41832733154297)\n",
      "epoch 1600 loss=(37.1831169128418, 12.955080032348633, 34.3918342590332)\n",
      "epoch 1800 loss=(42.302433013916016, 12.027972221374512, 44.997520446777344)\n",
      "epoch 2000 loss=(35.15763854980469, 12.303653717041016, 34.88014602661133)\n",
      "epoch 2200 loss=(38.50687026977539, 11.763203620910645, 25.696260452270508)\n",
      "epoch 2400 loss=(35.870574951171875, 12.031059265136719, 24.497432708740234)\n",
      "epoch 2600 loss=(32.441444396972656, 11.729249000549316, 21.713146209716797)\n",
      "Early stopping\n",
      "Finished training\n",
      "6 (25.67154884338379, 0.8644067645072937, 16.16264533996582)\n",
      "epoch 0 loss=(1490.33544921875, 69.56712341308594, 254.7361297607422)\n",
      "epoch 200 loss=(55.7213020324707, 17.331787109375, 110.30107879638672)\n",
      "epoch 400 loss=(40.718780517578125, 13.205403327941895, 39.649600982666016)\n",
      "epoch 600 loss=(42.209354400634766, 12.262741088867188, 37.336036682128906)\n",
      "epoch 800 loss=(39.5380744934082, 11.800963401794434, 29.019723892211914)\n",
      "epoch 1000 loss=(32.13370132446289, 11.309622764587402, 30.336467742919922)\n",
      "epoch 1200 loss=(35.674903869628906, 11.372379302978516, 25.690013885498047)\n",
      "epoch 1400 loss=(29.82679557800293, 11.120888710021973, 24.468053817749023)\n",
      "epoch 1600 loss=(33.397335052490234, 10.964943885803223, 26.015249252319336)\n",
      "epoch 1800 loss=(30.84432029724121, 11.132402420043945, 26.415746688842773)\n",
      "epoch 2000 loss=(28.15494155883789, 10.935582160949707, 23.334774017333984)\n",
      "Early stopping\n",
      "Finished training\n",
      "7 (29.548250198364258, 0.7796609997749329, 13.279645919799805)\n",
      "epoch 0 loss=(1527.668701171875, 69.46450805664062, 252.42457580566406)\n",
      "epoch 200 loss=(198.9345245361328, 31.810375213623047, 125.61632537841797)\n",
      "epoch 400 loss=(124.30533599853516, 17.651103973388672, 93.16918182373047)\n",
      "epoch 600 loss=(55.4255485534668, 13.963820457458496, 59.88983917236328)\n",
      "epoch 800 loss=(50.15885543823242, 14.041810035705566, 63.16376876831055)\n",
      "epoch 1000 loss=(49.891658782958984, 13.039194107055664, 27.00827407836914)\n",
      "epoch 1200 loss=(44.13085174560547, 12.770970344543457, 22.965930938720703)\n",
      "epoch 1400 loss=(42.04880142211914, 12.998943328857422, 20.06013298034668)\n",
      "Early stopping\n",
      "Finished training\n",
      "8 (27.534029006958008, 0.7457627058029175, 14.814011573791504)\n",
      "epoch 0 loss=(1529.28564453125, 69.61339569091797, 261.3312683105469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=(139.32289123535156, 24.587682723999023, 131.6905975341797)\n",
      "epoch 400 loss=(132.99002075195312, 18.785064697265625, 81.69926452636719)\n",
      "epoch 600 loss=(48.246429443359375, 14.26829719543457, 48.420467376708984)\n",
      "epoch 800 loss=(44.785308837890625, 13.148442268371582, 64.20350646972656)\n",
      "epoch 1000 loss=(48.429359436035156, 12.28709602355957, 32.330482482910156)\n",
      "epoch 1200 loss=(47.733154296875, 12.553670883178711, 45.141685485839844)\n",
      "epoch 1400 loss=(45.571250915527344, 11.799363136291504, 25.61336898803711)\n",
      "epoch 1600 loss=(36.192745208740234, 11.730323791503906, 17.793481826782227)\n",
      "Early stopping\n",
      "Finished training\n",
      "9 (26.348876953125, 0.7457627058029175, 11.453042984008789)\n"
     ]
    }
   ],
   "source": [
    "ceLoss = torch.nn.CrossEntropyLoss()\n",
    "mseLoss = torch.nn.MSELoss()\n",
    "nEpochs = 5000\n",
    "pPeriod = 200\n",
    "thresh = torch.Tensor((40,12,20)).float().cuda()\n",
    "\n",
    "para = [nback_p_t, emoid_p_t]\n",
    "    \n",
    "rmse = []\n",
    "\n",
    "def xform(data, stats=None, fwd=True):\n",
    "    if stats is None:\n",
    "        mu = torch.mean(data, dim=0, keepdim=True)\n",
    "        sd = torch.std(data, dim=0, keepdim=True)\n",
    "        return (mu, sd)\n",
    "    elif fwd:\n",
    "        return (data - stats[0])/stats[1]\n",
    "    else:\n",
    "        return data*stats[1] + stats[0]\n",
    "\n",
    "for i in range(10):\n",
    "    pgigcn = PgiDiff(4, len(para), 4, 0.5, 0.2)\n",
    "    optim = torch.optim.Adam(pgigcn.masks, lr=2e-5, weight_decay=2e-5)\n",
    "\n",
    "    trainIdcs = groups[i][0]\n",
    "    testIdcs = groups[i][1]\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    X = X[trainIdcs]\n",
    "    Y = torch.from_numpy(X_all[trainIdcs]).float().cuda()\n",
    "    \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t[trainIdcs]\n",
    "    age = Y[:,0]\n",
    "    \n",
    "    # Normalize dataset\n",
    "    statsGen = xform(gen)\n",
    "    statsWrt = xform(wrt)\n",
    "    statsAge = xform(age)\n",
    "    \n",
    "    # Transformed\n",
    "    genT = xform(gen, statsGen)\n",
    "    wrtT = xform(wrt, statsWrt)\n",
    "    ageT = xform(age, statsAge)\n",
    "    \n",
    "    y = torch.cat([ageT.unsqueeze(1), genT, wrtT.unsqueeze(1)], dim=1).T\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        optim.zero_grad()\n",
    "        res = pgigcn(X, y, univ=True)\n",
    "        loss0 = mseLoss(xform(res[0], statsAge, fwd=False), age)\n",
    "        loss1 = 100*ceLoss(torch.stack([res[1], res[2]], dim=1), gen)\n",
    "        loss2 = mseLoss(xform(res[3], statsWrt, fwd=False), wrt)\n",
    "        loss = torch.stack([loss0, loss1, loss2])\n",
    "        torch.sum(loss).backward()\n",
    "        optim.step()\n",
    "        if (epoch % pPeriod == 0 or epoch == nEpochs-1):\n",
    "            print(f'epoch {epoch} loss={(float(loss0), float(loss1), float(loss2))}')\n",
    "        if torch.all(loss[0:3] < thresh):\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    print('Finished training')\n",
    "    \n",
    "    pgigcn.eval()\n",
    "    \n",
    "    X = torch.stack(para, dim=1)\n",
    "    Y = torch.from_numpy(X_all).float().cuda()\n",
    "        \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t\n",
    "    age = Y[:,0]\n",
    "    \n",
    "    # Transformed\n",
    "    genT = xform(gen, statsGen)\n",
    "    wrtT = xform(wrt, statsWrt)\n",
    "    ageT = xform(age, statsAge)\n",
    "    \n",
    "    y = torch.cat([ageT.unsqueeze(1), genT, wrtT.unsqueeze(1)], dim=1).T\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        res = pgigcn(X, y, testIdcs, univ=True)\n",
    "        loss0 = mseLoss(xform(res[0][testIdcs].detach(), statsAge, fwd=False), age[testIdcs]).cpu().numpy()**0.5\n",
    "        frac1 = torch.sum(torch.argmax(torch.stack([res[1], res[2]], dim=1).detach(), dim=1)[testIdcs] \n",
    "                         == torch.argmax(gen[testIdcs], dim=1))/testIdcs.shape[0]\n",
    "        loss2 = mseLoss(xform(res[3][testIdcs].detach(), statsWrt, fwd=False), wrt[testIdcs]).cpu().numpy()**0.5\n",
    "\n",
    "        rmse.append((float(loss0), float(frac1), float(loss2)))\n",
    "        \n",
    "    print(i, end=' ')\n",
    "    print(rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c41b3aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.362210273742676\n",
      "15.599970817565918\n",
      "16.325719833374023\n",
      "14.82560920715332\n",
      "13.630986213684082\n",
      "14.559538841247559\n",
      "16.16264533996582\n",
      "13.279645919799805\n",
      "14.814011573791504\n",
      "11.453042984008789\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in rmse:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed3f02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0761, 0.3380,  ..., 0.1505, 0.6623, 2.2184],\n",
      "        [0.0761, 0.0000, 0.1209,  ..., 0.4447, 0.1637, 0.0927],\n",
      "        [0.3380, 0.1209, 0.0000,  ..., 0.5613, 0.2664, 0.3323],\n",
      "        ...,\n",
      "        [0.1505, 0.4447, 0.5613,  ..., 0.0000, 0.7420, 0.2554],\n",
      "        [0.6623, 0.1637, 0.2664,  ..., 0.7420, 0.0000, 0.6312],\n",
      "        [2.2184, 0.0927, 0.3323,  ..., 0.2554, 0.6312, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lat,e = pgigcn.getLatentsAndEdges(X,0)\n",
    "e = mask(e)\n",
    "e = F.softmax(e)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cdc10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
