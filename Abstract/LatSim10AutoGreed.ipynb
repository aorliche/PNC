{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4b4edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../PNC_Good/MegaMeta3.pkl', 'rb') as f: \n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c267bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n",
      "620\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "with open('../../Work/LatentSim/Splits.pkl', 'rb') as f:\n",
    "    keys, groups = pickle.load(f)\n",
    "    \n",
    "print(len(keys))\n",
    "print(len(groups[0][0])+len(groups[0][1]))\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c155e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 264, 231)\n",
      "(620, 264, 210)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nback = np.stack([meta[key]['nback'] for key in keys])\n",
    "emoid = np.stack([meta[key]['emoid'] for key in keys])\n",
    "\n",
    "print(nback.shape)\n",
    "print(emoid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bb8175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 264, 264)\n"
     ]
    }
   ],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "def getFC(timeSeries, kind='correlation', transpose=True):\n",
    "    connMeasure = ConnectivityMeasure(kind=kind)\n",
    "    if transpose:\n",
    "        timeSeries = np.transpose(timeSeries, axes=(0,2,1))\n",
    "    return connMeasure.fit_transform(timeSeries)\n",
    "\n",
    "nback_p = getFC(nback)\n",
    "emoid_p = getFC(emoid)\n",
    "\n",
    "print(nback_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c6709a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([620, 34716])\n",
      "torch.Size([620, 34716])\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convertTorch(p):\n",
    "    t = torch.from_numpy(p).float()\n",
    "    u = []\n",
    "    for i in range(t.shape[0]):\n",
    "        u.append(t[i][torch.triu_indices(264,264,offset=1).unbind()])\n",
    "    return torch.stack(u).cuda()\n",
    "\n",
    "nback_p_t = convertTorch(nback_p)\n",
    "emoid_p_t = convertTorch(emoid_p)\n",
    "\n",
    "print(nback_p_t.shape)\n",
    "print(emoid_p_t.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3b60e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm complete\n"
     ]
    }
   ],
   "source": [
    "mu_nback = torch.mean(nback_p_t, dim=0, keepdim=True)\n",
    "mu_emoid = torch.mean(emoid_p_t, dim=0, keepdim=True)\n",
    "std_nback = torch.std(nback_p_t, dim=0, keepdim=True)\n",
    "std_emoid = torch.std(emoid_p_t, dim=0, keepdim=True)\n",
    "\n",
    "nback_p_t = (nback_p_t - mu_nback)/std_nback\n",
    "emoid_p_t = (emoid_p_t - mu_emoid)/std_emoid\n",
    "\n",
    "print('Norm complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e0614b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([-2.2998, -3.0133], device='cuda:0'),\n",
      "indices=tensor([603, 350], device='cuda:0'))\n",
      "torch.return_types.min(\n",
      "values=tensor([-2.7284, -3.0616], device='cuda:0'),\n",
      "indices=tensor([351,  33], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(nback_p_t[:,420:422],dim=0))\n",
    "print(torch.min(emoid_p_t[:,420:422],dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1285ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620,)\n",
      "(620, 2)\n",
      "(620,)\n"
     ]
    }
   ],
   "source": [
    "age = np.stack([meta[key]['AgeInMonths'] for key in keys])\n",
    "gen = np.stack([np.array([meta[key]['Gender'] == 'M', meta[key]['Gender'] == 'F']) for key in keys]).astype(int)\n",
    "wrt = np.stack([meta[key]['wratStd'] for key in keys])\n",
    "\n",
    "print(age.shape)\n",
    "print(gen.shape)\n",
    "print(wrt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3851f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "age_t = torch.from_numpy(age).float().cuda()\n",
    "gen_t = torch.from_numpy(gen).float().cuda()\n",
    "wrt_t = torch.from_numpy(wrt).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31067220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def arith(n):\n",
    "    return int(n*(n+1)/2)\n",
    "\n",
    "def mask(e):\n",
    "    return e - torch.diag(torch.diag(e.detach()))\n",
    "\n",
    "class LatSim(nn.Module):\n",
    "    def __init__(self, nTgts, inp, dp=0, edp=0.1):\n",
    "        super(LatSim, self).__init__()\n",
    "        self.active = nn.Parameter(1e-4*torch.randn(nTgts,inp.shape[-1],2).float().cuda())\n",
    "        self.dp = nn.Dropout(p=dp)\n",
    "        self.edp = nn.Dropout(p=edp)\n",
    "    \n",
    "    def getLatentsAndEdges(self, x, i):\n",
    "        e = 1e-10\n",
    "        y = torch.einsum('ac,ce->ae', x, self.active[i])\n",
    "        e = e+y@y.T\n",
    "        return y, y, e\n",
    "        \n",
    "    def forward(self, x, y, testIdcs=None):\n",
    "        res = []\n",
    "        es = []\n",
    "        x = self.dp(x)\n",
    "        for i in range(self.active.shape[0]):\n",
    "            _, _, e = self.getLatentsAndEdges(x[:,i,:], i)\n",
    "            if testIdcs is not None:\n",
    "                e[:,testIdcs] = 0\n",
    "            e = self.edp(e)\n",
    "            e = mask(e)\n",
    "            e[e == 0] = float('-inf')\n",
    "            e = F.softmax(e, dim=1)\n",
    "            es.append(e)\n",
    "            res.append(e@y)\n",
    "        return res, es\n",
    "\n",
    "def validate(model, X, y, testIdcs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res, _ = model(X, y, testIdcs)\n",
    "        avg = torch.mean(torch.stack(res), dim=0)\n",
    "        if res[0].dim() == 1:\n",
    "            loss = mseLoss(avg[testIdcs], y[testIdcs]).cpu().numpy()**0.5\n",
    "        else:\n",
    "            corr = (torch.argmax(avg, dim=1) == torch.argmax(y, dim=1))[testIdcs]\n",
    "            loss = torch.sum(corr)/len(testIdcs)\n",
    "    model.train()\n",
    "    return loss\n",
    "        \n",
    "ceLoss = torch.nn.CrossEntropyLoss()\n",
    "mseLoss = torch.nn.MSELoss()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "226099ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6715, 9030, 9025, 6708, 6712])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167])\n",
      "epoch 0 loss=267.51513671875\n",
      "New best validation epoch 0 loss=12.045499801635742\n",
      "epoch 200 loss=266.017578125\n",
      "New best validation epoch 200 loss=12.034635543823242\n",
      "epoch 400 loss=263.23431396484375\n",
      "New best validation epoch 400 loss=11.99848461151123\n",
      "epoch 600 loss=260.4278869628906\n",
      "New best validation epoch 600 loss=11.946208953857422\n",
      "epoch 800 loss=257.7495422363281\n",
      "New best validation epoch 800 loss=11.902220726013184\n",
      "epoch 1000 loss=254.4326629638672\n",
      "New best validation epoch 1000 loss=11.874161720275879\n",
      "epoch 1200 loss=253.9905242919922\n",
      "New best validation epoch 1200 loss=11.857236862182617\n",
      "epoch 1400 loss=256.4788818359375\n",
      "New best validation epoch 1400 loss=11.84655475616455\n",
      "epoch 1600 loss=255.51596069335938\n",
      "New best validation epoch 1600 loss=11.836874961853027\n",
      "epoch 1800 loss=254.7239990234375\n",
      "New best validation epoch 1800 loss=11.82974624633789\n",
      "epoch 2000 loss=255.45066833496094\n",
      "New best validation epoch 2000 loss=11.828038215637207\n",
      "epoch 2200 loss=252.3169403076172\n",
      "New best validation epoch 2200 loss=11.825119018554688\n",
      "epoch 2400 loss=260.20159912109375\n",
      "New best validation epoch 2400 loss=11.82388687133789\n",
      "epoch 2600 loss=249.98825073242188\n",
      "New best validation epoch 2600 loss=11.823198318481445\n",
      "epoch 2800 loss=254.81954956054688\n",
      "epoch 3000 loss=254.83932495117188\n",
      "New best validation epoch 3000 loss=11.819779396057129\n",
      "epoch 3200 loss=256.44775390625\n",
      "epoch 3400 loss=257.08544921875\n",
      "New best validation epoch 3400 loss=11.818493843078613\n",
      "epoch 3600 loss=253.86932373046875\n",
      "epoch 3800 loss=250.66427612304688\n",
      "epoch 4000 loss=250.69491577148438\n",
      "New best validation epoch 4000 loss=11.818347930908203\n",
      "epoch 4200 loss=258.3687744140625\n",
      "epoch 4400 loss=255.3768310546875\n",
      "New best validation epoch 4400 loss=11.816962242126465\n",
      "epoch 4600 loss=257.5633239746094\n",
      "New best validation epoch 4600 loss=11.816376686096191\n",
      "epoch 4800 loss=259.121826171875\n",
      "epoch 4999 loss=248.53750610351562\n",
      "Finished training\n",
      "FINISHED 0 0 13.735527038574219\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358])\n",
      "epoch 0 loss=266.9615478515625\n",
      "New best validation epoch 0 loss=12.045499801635742\n",
      "epoch 200 loss=263.640625\n",
      "New best validation epoch 200 loss=12.017356872558594\n",
      "epoch 400 loss=256.14666748046875\n",
      "New best validation epoch 400 loss=11.93799114227295\n",
      "epoch 600 loss=245.64450073242188\n",
      "New best validation epoch 600 loss=11.849886894226074\n",
      "epoch 800 loss=239.12608337402344\n",
      "New best validation epoch 800 loss=11.794140815734863\n",
      "epoch 1000 loss=242.462646484375\n",
      "New best validation epoch 1000 loss=11.759703636169434\n",
      "epoch 1200 loss=236.64842224121094\n",
      "New best validation epoch 1200 loss=11.733190536499023\n",
      "epoch 1400 loss=236.1396484375\n",
      "New best validation epoch 1400 loss=11.71445083618164\n",
      "epoch 1600 loss=235.11123657226562\n",
      "New best validation epoch 1600 loss=11.699926376342773\n",
      "epoch 1800 loss=240.6770477294922\n",
      "New best validation epoch 1800 loss=11.682889938354492\n",
      "epoch 2000 loss=240.97811889648438\n",
      "New best validation epoch 2000 loss=11.670804023742676\n",
      "epoch 2200 loss=239.01736450195312\n",
      "New best validation epoch 2200 loss=11.663715362548828\n",
      "epoch 2400 loss=238.8753662109375\n",
      "New best validation epoch 2400 loss=11.657526016235352\n",
      "epoch 2600 loss=243.0286865234375\n",
      "New best validation epoch 2600 loss=11.650751113891602\n",
      "epoch 2800 loss=234.65867614746094\n",
      "New best validation epoch 2800 loss=11.646326065063477\n",
      "epoch 3000 loss=243.95494079589844\n",
      "New best validation epoch 3000 loss=11.63977336883545\n",
      "epoch 3200 loss=243.648193359375\n",
      "epoch 3400 loss=242.5655517578125\n",
      "epoch 3600 loss=237.87625122070312\n",
      "epoch 3800 loss=245.8236083984375\n",
      "New best validation epoch 3800 loss=11.636086463928223\n",
      "epoch 4000 loss=239.3622283935547\n",
      "epoch 4200 loss=238.2296142578125\n",
      "New best validation epoch 4200 loss=11.633225440979004\n",
      "epoch 4400 loss=241.20079040527344\n",
      "New best validation epoch 4400 loss=11.63083553314209\n",
      "epoch 4600 loss=240.67608642578125\n",
      "epoch 4800 loss=239.27072143554688\n",
      "New best validation epoch 4800 loss=11.62982177734375\n",
      "epoch 4999 loss=240.327880859375\n",
      "New best validation epoch 4999 loss=11.629276275634766\n",
      "Finished training\n",
      "FINISHED 0 1 13.523994445800781\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980])\n",
      "epoch 0 loss=266.5695495605469\n",
      "New best validation epoch 0 loss=12.045500755310059\n",
      "epoch 200 loss=260.9239501953125\n",
      "New best validation epoch 200 loss=11.969285011291504\n",
      "epoch 400 loss=243.94503784179688\n",
      "New best validation epoch 400 loss=11.776510238647461\n",
      "epoch 600 loss=237.86654663085938\n",
      "New best validation epoch 600 loss=11.62795639038086\n",
      "epoch 800 loss=234.163818359375\n",
      "New best validation epoch 800 loss=11.55623722076416\n",
      "epoch 1000 loss=232.88623046875\n",
      "New best validation epoch 1000 loss=11.522469520568848\n",
      "epoch 1200 loss=224.9898681640625\n",
      "New best validation epoch 1200 loss=11.503541946411133\n",
      "epoch 1400 loss=234.00108337402344\n",
      "New best validation epoch 1400 loss=11.4893159866333\n",
      "epoch 1600 loss=220.9970703125\n",
      "New best validation epoch 1600 loss=11.482029914855957\n",
      "epoch 1800 loss=221.80630493164062\n",
      "New best validation epoch 1800 loss=11.476399421691895\n",
      "epoch 2000 loss=219.75645446777344\n",
      "epoch 2200 loss=233.00523376464844\n",
      "epoch 2400 loss=224.55723571777344\n",
      "New best validation epoch 2400 loss=11.472888946533203\n",
      "epoch 2600 loss=230.2649383544922\n",
      "New best validation epoch 2600 loss=11.471145629882812\n",
      "epoch 2800 loss=226.55685424804688\n",
      "New best validation epoch 2800 loss=11.46927261352539\n",
      "epoch 3000 loss=231.39907836914062\n",
      "New best validation epoch 3000 loss=11.467876434326172\n",
      "epoch 3200 loss=227.55186462402344\n",
      "epoch 3400 loss=224.5635986328125\n",
      "epoch 3600 loss=226.42977905273438\n",
      "epoch 3800 loss=226.14463806152344\n",
      "epoch 4000 loss=225.558837890625\n",
      "epoch 4200 loss=232.59732055664062\n",
      "epoch 4400 loss=224.76806640625\n",
      "epoch 4600 loss=226.6250457763672\n",
      "epoch 4800 loss=224.51170349121094\n",
      "epoch 4999 loss=230.98483276367188\n",
      "Finished training\n",
      "FINISHED 0 2 13.423012733459473\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621])\n",
      "epoch 0 loss=266.9701843261719\n",
      "New best validation epoch 0 loss=12.04549789428711\n",
      "epoch 200 loss=257.348876953125\n",
      "New best validation epoch 200 loss=11.944292068481445\n",
      "epoch 400 loss=233.49424743652344\n",
      "New best validation epoch 400 loss=11.737678527832031\n",
      "epoch 600 loss=227.99432373046875\n",
      "New best validation epoch 600 loss=11.644580841064453\n",
      "epoch 800 loss=223.39837646484375\n",
      "New best validation epoch 800 loss=11.606094360351562\n",
      "epoch 1000 loss=222.2979736328125\n",
      "New best validation epoch 1000 loss=11.580974578857422\n",
      "epoch 1200 loss=224.5449676513672\n",
      "New best validation epoch 1200 loss=11.556337356567383\n",
      "epoch 1400 loss=218.2997283935547\n",
      "New best validation epoch 1400 loss=11.537293434143066\n",
      "epoch 1600 loss=228.42173767089844\n",
      "New best validation epoch 1600 loss=11.524710655212402\n",
      "epoch 1800 loss=222.52279663085938\n",
      "New best validation epoch 1800 loss=11.519187927246094\n",
      "epoch 2000 loss=217.33822631835938\n",
      "New best validation epoch 2000 loss=11.515342712402344\n",
      "epoch 2200 loss=217.70652770996094\n",
      "New best validation epoch 2200 loss=11.514992713928223\n",
      "epoch 2400 loss=214.9067840576172\n",
      "epoch 2600 loss=211.5325927734375\n",
      "epoch 2800 loss=217.74838256835938\n",
      "epoch 3000 loss=225.8121337890625\n",
      "epoch 3200 loss=218.08700561523438\n",
      "epoch 3400 loss=213.87594604492188\n",
      "epoch 3600 loss=218.28802490234375\n",
      "New best validation epoch 3600 loss=11.514084815979004\n",
      "epoch 3800 loss=217.2131805419922\n",
      "epoch 4000 loss=218.2394256591797\n",
      "epoch 4200 loss=221.95993041992188\n",
      "epoch 4400 loss=228.18341064453125\n",
      "epoch 4600 loss=232.3687744140625\n",
      "epoch 4800 loss=222.19265747070312\n",
      "epoch 4999 loss=215.7295379638672\n",
      "Finished training\n",
      "FINISHED 0 3 13.25246810913086\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147])\n",
      "epoch 0 loss=267.4255065917969\n",
      "New best validation epoch 0 loss=12.04549789428711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=255.10374450683594\n",
      "New best validation epoch 200 loss=11.923432350158691\n",
      "epoch 400 loss=227.0555877685547\n",
      "New best validation epoch 400 loss=11.706305503845215\n",
      "epoch 600 loss=214.0481414794922\n",
      "New best validation epoch 600 loss=11.638792991638184\n",
      "epoch 800 loss=225.33189392089844\n",
      "New best validation epoch 800 loss=11.618167877197266\n",
      "epoch 1000 loss=212.7674560546875\n",
      "New best validation epoch 1000 loss=11.608253479003906\n",
      "epoch 1200 loss=213.53289794921875\n",
      "New best validation epoch 1200 loss=11.601897239685059\n",
      "epoch 1400 loss=208.0714569091797\n",
      "New best validation epoch 1400 loss=11.591455459594727\n",
      "epoch 1600 loss=211.3763885498047\n",
      "New best validation epoch 1600 loss=11.586431503295898\n",
      "epoch 1800 loss=214.655517578125\n",
      "New best validation epoch 1800 loss=11.58082389831543\n",
      "epoch 2000 loss=204.21157836914062\n",
      "New best validation epoch 2000 loss=11.573877334594727\n",
      "epoch 2200 loss=216.2638397216797\n",
      "New best validation epoch 2200 loss=11.572600364685059\n",
      "epoch 2400 loss=207.99661254882812\n",
      "New best validation epoch 2400 loss=11.569602012634277\n",
      "epoch 2600 loss=215.09361267089844\n",
      "New best validation epoch 2600 loss=11.562578201293945\n",
      "epoch 2800 loss=214.9581756591797\n",
      "epoch 3000 loss=214.2506103515625\n",
      "epoch 3200 loss=212.15142822265625\n",
      "epoch 3400 loss=215.17263793945312\n",
      "epoch 3600 loss=206.2509765625\n",
      "epoch 3800 loss=210.5321502685547\n",
      "epoch 4000 loss=210.193359375\n",
      "epoch 4200 loss=221.88320922851562\n",
      "epoch 4400 loss=208.5854949951172\n",
      "epoch 4600 loss=215.38401794433594\n",
      "epoch 4800 loss=211.00674438476562\n",
      "epoch 4999 loss=209.6625213623047\n",
      "Finished training\n",
      "FINISHED 0 4 13.1993408203125\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136, 15310, 22013, 10963, 24420, 23469])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147, 21662, 14603, 27947, 30371, 10871])\n",
      "epoch 0 loss=267.19403076171875\n",
      "New best validation epoch 0 loss=12.04549789428711\n",
      "epoch 200 loss=249.7509765625\n",
      "New best validation epoch 200 loss=11.862466812133789\n",
      "epoch 400 loss=217.12722778320312\n",
      "New best validation epoch 400 loss=11.593795776367188\n",
      "epoch 600 loss=211.08642578125\n",
      "New best validation epoch 600 loss=11.551766395568848\n",
      "epoch 800 loss=212.218017578125\n",
      "New best validation epoch 800 loss=11.543424606323242\n",
      "epoch 1000 loss=205.7552490234375\n",
      "New best validation epoch 1000 loss=11.531784057617188\n",
      "epoch 1200 loss=206.53359985351562\n",
      "New best validation epoch 1200 loss=11.514423370361328\n",
      "epoch 1400 loss=209.21707153320312\n",
      "New best validation epoch 1400 loss=11.510557174682617\n",
      "epoch 1600 loss=205.044921875\n",
      "New best validation epoch 1600 loss=11.507596969604492\n",
      "epoch 1800 loss=204.38818359375\n",
      "New best validation epoch 1800 loss=11.505437850952148\n",
      "epoch 2000 loss=210.17210388183594\n",
      "New best validation epoch 2000 loss=11.501138687133789\n",
      "epoch 2200 loss=212.62413024902344\n",
      "New best validation epoch 2200 loss=11.500009536743164\n",
      "epoch 2400 loss=192.305419921875\n",
      "New best validation epoch 2400 loss=11.492281913757324\n",
      "epoch 2600 loss=198.4210662841797\n",
      "epoch 2800 loss=210.91555786132812\n",
      "epoch 3000 loss=206.74241638183594\n",
      "New best validation epoch 3000 loss=11.492069244384766\n",
      "epoch 3200 loss=216.93736267089844\n",
      "epoch 3400 loss=204.14202880859375\n",
      "epoch 3600 loss=213.6959228515625\n",
      "epoch 3800 loss=207.27059936523438\n",
      "epoch 4000 loss=204.816162109375\n",
      "New best validation epoch 4000 loss=11.4909086227417\n",
      "epoch 4200 loss=198.66477966308594\n",
      "epoch 4400 loss=211.49473571777344\n",
      "epoch 4600 loss=209.7748260498047\n",
      "epoch 4800 loss=207.81796264648438\n",
      "New best validation epoch 4800 loss=11.485849380493164\n",
      "epoch 4999 loss=211.59823608398438\n",
      "Finished training\n",
      "FINISHED 0 5 12.928250312805176\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136, 15310, 22013, 10963, 24420, 23469,\n",
      "        15005, 11517,  8028,  5091,  7816])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147, 21662, 14603, 27947, 30371, 10871,\n",
      "        17437, 20642, 19089, 22193, 24199])\n",
      "epoch 0 loss=267.8262939453125\n",
      "New best validation epoch 0 loss=12.045496940612793\n",
      "epoch 200 loss=246.3163299560547\n",
      "New best validation epoch 200 loss=11.785965919494629\n",
      "epoch 400 loss=214.4598846435547\n",
      "New best validation epoch 400 loss=11.402948379516602\n",
      "epoch 600 loss=202.71835327148438\n",
      "New best validation epoch 600 loss=11.316094398498535\n",
      "epoch 800 loss=200.70599365234375\n",
      "New best validation epoch 800 loss=11.304157257080078\n",
      "epoch 1000 loss=203.70358276367188\n",
      "New best validation epoch 1000 loss=11.278129577636719\n",
      "epoch 1200 loss=201.92527770996094\n",
      "New best validation epoch 1200 loss=11.263837814331055\n",
      "epoch 1400 loss=205.88925170898438\n",
      "epoch 1600 loss=197.691162109375\n",
      "epoch 1800 loss=198.45040893554688\n",
      "New best validation epoch 1800 loss=11.254430770874023\n",
      "epoch 2000 loss=198.78341674804688\n",
      "epoch 2200 loss=204.7494354248047\n",
      "New best validation epoch 2200 loss=11.25385570526123\n",
      "epoch 2400 loss=196.1964569091797\n",
      "epoch 2600 loss=201.1848602294922\n",
      "epoch 2800 loss=193.79080200195312\n",
      "epoch 3000 loss=188.99819946289062\n",
      "epoch 3200 loss=196.70895385742188\n",
      "epoch 3400 loss=197.77410888671875\n",
      "epoch 3600 loss=197.2626190185547\n",
      "epoch 3800 loss=203.670654296875\n",
      "epoch 4000 loss=188.590576171875\n",
      "epoch 4200 loss=202.7756805419922\n",
      "epoch 4400 loss=193.45089721679688\n",
      "epoch 4600 loss=216.7926483154297\n",
      "epoch 4800 loss=200.5859375\n",
      "epoch 4999 loss=197.93658447265625\n",
      "Finished training\n",
      "FINISHED 0 6 13.174009323120117\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136, 15310, 22013, 10963, 24420, 23469,\n",
      "        15005, 11517,  8028,  5091,  7816,  4228,  1155,  7813,  1162, 31242])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147, 21662, 14603, 27947, 30371, 10871,\n",
      "        17437, 20642, 19089, 22193, 24199, 25344, 25500, 12212, 25366, 13257])\n",
      "epoch 0 loss=268.19720458984375\n",
      "New best validation epoch 0 loss=12.045493125915527\n",
      "epoch 200 loss=242.41586303710938\n",
      "New best validation epoch 200 loss=11.735906600952148\n",
      "epoch 400 loss=211.098388671875\n",
      "New best validation epoch 400 loss=11.409902572631836\n",
      "epoch 600 loss=209.9993896484375\n",
      "New best validation epoch 600 loss=11.336793899536133\n",
      "epoch 800 loss=198.9163818359375\n",
      "New best validation epoch 800 loss=11.32298469543457\n",
      "epoch 1000 loss=189.23361206054688\n",
      "New best validation epoch 1000 loss=11.291983604431152\n",
      "epoch 1200 loss=194.37106323242188\n",
      "New best validation epoch 1200 loss=11.283841133117676\n",
      "epoch 1400 loss=197.706787109375\n",
      "New best validation epoch 1400 loss=11.278322219848633\n",
      "epoch 1600 loss=197.44583129882812\n",
      "New best validation epoch 1600 loss=11.272869110107422\n",
      "epoch 1800 loss=202.3365936279297\n",
      "epoch 2000 loss=191.71507263183594\n",
      "epoch 2200 loss=191.81546020507812\n",
      "epoch 2400 loss=198.3959197998047\n",
      "epoch 2600 loss=197.53805541992188\n",
      "epoch 2800 loss=197.1948699951172\n",
      "epoch 3000 loss=204.26861572265625\n",
      "epoch 3200 loss=198.61068725585938\n",
      "epoch 3400 loss=188.86851501464844\n",
      "New best validation epoch 3400 loss=11.269449234008789\n",
      "epoch 3600 loss=189.95518493652344\n",
      "epoch 3800 loss=198.82908630371094\n",
      "epoch 4000 loss=195.33868408203125\n",
      "epoch 4200 loss=191.509033203125\n",
      "epoch 4400 loss=190.1307373046875\n",
      "epoch 4600 loss=195.04747009277344\n",
      "New best validation epoch 4600 loss=11.264102935791016\n",
      "epoch 4800 loss=198.51242065429688\n",
      "epoch 4999 loss=193.2159881591797\n",
      "Finished training\n",
      "FINISHED 0 7 13.347541809082031\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136, 15310, 22013, 10963, 24420, 23469,\n",
      "        15005, 11517,  8028,  5091,  7816,  4228,  1155,  7813,  1162, 31242,\n",
      "         8274,  5929,  7813,  5689,  7348])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147, 21662, 14603, 27947, 30371, 10871,\n",
      "        17437, 20642, 19089, 22193, 24199, 25344, 25500, 12212, 25366, 13257,\n",
      "        17054, 16607, 32996, 34591, 21333])\n",
      "epoch 0 loss=267.49371337890625\n",
      "New best validation epoch 0 loss=12.045493125915527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=237.9320526123047\n",
      "New best validation epoch 200 loss=11.70034122467041\n",
      "epoch 400 loss=197.3648681640625\n",
      "New best validation epoch 400 loss=11.341662406921387\n",
      "epoch 600 loss=200.66671752929688\n",
      "New best validation epoch 600 loss=11.278593063354492\n",
      "epoch 800 loss=196.87710571289062\n",
      "New best validation epoch 800 loss=11.249105453491211\n",
      "epoch 1000 loss=184.5695037841797\n",
      "New best validation epoch 1000 loss=11.226960182189941\n",
      "epoch 1200 loss=190.037353515625\n",
      "New best validation epoch 1200 loss=11.215490341186523\n",
      "epoch 1400 loss=196.53170776367188\n",
      "New best validation epoch 1400 loss=11.204118728637695\n",
      "epoch 1600 loss=182.03579711914062\n",
      "New best validation epoch 1600 loss=11.200990676879883\n",
      "epoch 1800 loss=189.27297973632812\n",
      "epoch 2000 loss=189.85508728027344\n",
      "New best validation epoch 2000 loss=11.188817024230957\n",
      "epoch 2200 loss=191.69224548339844\n",
      "epoch 2400 loss=188.45458984375\n",
      "epoch 2600 loss=193.55224609375\n",
      "epoch 2800 loss=199.261474609375\n",
      "epoch 3000 loss=193.45709228515625\n",
      "epoch 3200 loss=190.577392578125\n",
      "epoch 3400 loss=195.98922729492188\n",
      "epoch 3600 loss=194.6389617919922\n",
      "epoch 3800 loss=186.18441772460938\n",
      "epoch 4000 loss=194.3037567138672\n",
      "epoch 4200 loss=187.7935791015625\n",
      "epoch 4400 loss=189.69540405273438\n",
      "epoch 4600 loss=200.78012084960938\n",
      "epoch 4800 loss=195.6041717529297\n",
      "epoch 4999 loss=188.70590209960938\n",
      "Finished training\n",
      "FINISHED 0 8 13.393410682678223\n",
      "tensor([ 6715,  9030,  9025,  6708,  6712, 22779, 29047, 29119, 21858, 19220,\n",
      "        24294, 22790, 22798, 31155,  3256,  7166, 20022, 30440,  5321,   751,\n",
      "        25986, 24047, 27310,   991, 29136, 15310, 22013, 10963, 24420, 23469,\n",
      "        15005, 11517,  8028,  5091,  7816,  4228,  1155,  7813,  1162, 31242,\n",
      "         8274,  5929,  7813,  5689,  7348, 20699,  8396, 16039, 33795, 25577])\n",
      "tensor([ 1160,  9030,  5292, 10626,  1167,  3192, 26586, 13803, 13702, 29358,\n",
      "        28360, 12157, 27667, 29228, 12980, 20938, 23458, 18257,  1244, 28621,\n",
      "        16829, 16831,  6591, 16837, 31147, 21662, 14603, 27947, 30371, 10871,\n",
      "        17437, 20642, 19089, 22193, 24199, 25344, 25500, 12212, 25366, 13257,\n",
      "        17054, 16607, 32996, 34591, 21333, 21540, 27439, 24352, 29772,  6735])\n",
      "epoch 0 loss=267.449462890625\n",
      "New best validation epoch 0 loss=12.04549503326416\n",
      "epoch 200 loss=231.164306640625\n",
      "New best validation epoch 200 loss=11.672491073608398\n",
      "epoch 400 loss=203.7319793701172\n",
      "New best validation epoch 400 loss=11.377110481262207\n",
      "epoch 600 loss=196.07952880859375\n",
      "New best validation epoch 600 loss=11.345589637756348\n",
      "epoch 800 loss=188.16891479492188\n",
      "New best validation epoch 800 loss=11.321338653564453\n",
      "epoch 1000 loss=186.41757202148438\n",
      "New best validation epoch 1000 loss=11.303080558776855\n",
      "epoch 1200 loss=184.1614227294922\n",
      "New best validation epoch 1200 loss=11.292328834533691\n",
      "epoch 1400 loss=200.5824432373047\n",
      "New best validation epoch 1400 loss=11.290494918823242\n",
      "epoch 1600 loss=182.3530731201172\n",
      "epoch 1800 loss=180.0098876953125\n",
      "epoch 2000 loss=184.10276794433594\n",
      "epoch 2200 loss=185.02569580078125\n",
      "epoch 2400 loss=185.73419189453125\n",
      "epoch 2600 loss=187.34715270996094\n",
      "New best validation epoch 2600 loss=11.278944969177246\n",
      "epoch 2800 loss=193.3909149169922\n",
      "epoch 3000 loss=187.44198608398438\n",
      "epoch 3200 loss=191.54190063476562\n",
      "epoch 3400 loss=179.0466766357422\n",
      "New best validation epoch 3400 loss=11.277749061584473\n",
      "epoch 3600 loss=194.41619873046875\n",
      "New best validation epoch 3600 loss=11.277421951293945\n",
      "epoch 3800 loss=194.34967041015625\n",
      "New best validation epoch 3800 loss=11.273595809936523\n",
      "epoch 4000 loss=192.8590545654297\n",
      "epoch 4200 loss=188.462890625\n",
      "epoch 4400 loss=195.40914916992188\n",
      "epoch 4600 loss=187.96774291992188\n",
      "epoch 4800 loss=185.75144958496094\n",
      "epoch 4999 loss=186.6301727294922\n",
      "Finished training\n",
      "FINISHED 0 9 13.4033842086792\n",
      "tensor([23469, 21858, 21035, 10925,  6715])\n",
      "tensor([10626,  2171,  9030, 11046, 34516])\n",
      "epoch 0 loss=260.98736572265625\n",
      "New best validation epoch 0 loss=11.89256477355957\n",
      "epoch 200 loss=259.7275390625\n",
      "New best validation epoch 200 loss=11.86898136138916\n",
      "epoch 400 loss=257.96240234375\n",
      "New best validation epoch 400 loss=11.794360160827637\n",
      "epoch 600 loss=251.9078826904297\n",
      "New best validation epoch 600 loss=11.680036544799805\n",
      "epoch 800 loss=251.280029296875\n",
      "New best validation epoch 800 loss=11.556805610656738\n",
      "epoch 1000 loss=245.27957153320312\n",
      "New best validation epoch 1000 loss=11.462262153625488\n",
      "epoch 1200 loss=250.54150390625\n",
      "New best validation epoch 1200 loss=11.405689239501953\n",
      "epoch 1400 loss=252.32394409179688\n",
      "New best validation epoch 1400 loss=11.380902290344238\n",
      "epoch 1600 loss=242.0582275390625\n",
      "New best validation epoch 1600 loss=11.375482559204102\n",
      "epoch 1800 loss=249.94029235839844\n",
      "New best validation epoch 1800 loss=11.364692687988281\n",
      "epoch 2000 loss=246.6005096435547\n",
      "New best validation epoch 2000 loss=11.360179901123047\n",
      "epoch 2200 loss=245.9679412841797\n",
      "New best validation epoch 2200 loss=11.35819149017334\n",
      "epoch 2400 loss=243.36021423339844\n",
      "epoch 2600 loss=251.71963500976562\n",
      "epoch 2800 loss=245.6899871826172\n",
      "epoch 3000 loss=246.57347106933594\n",
      "epoch 3200 loss=250.5394744873047\n",
      "epoch 3400 loss=242.3467254638672\n",
      "epoch 3600 loss=250.3361053466797\n",
      "epoch 3800 loss=245.91787719726562\n",
      "epoch 4000 loss=236.38633728027344\n",
      "epoch 4200 loss=249.47372436523438\n",
      "epoch 4400 loss=243.76402282714844\n",
      "epoch 4600 loss=251.38186645507812\n",
      "epoch 4800 loss=246.9032440185547\n",
      "epoch 4999 loss=247.1356658935547\n",
      "Finished training\n",
      "FINISHED 1 0 15.723607063293457\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214])\n",
      "epoch 0 loss=260.4195251464844\n",
      "New best validation epoch 0 loss=11.892565727233887\n",
      "epoch 200 loss=258.5976257324219\n",
      "New best validation epoch 200 loss=11.869194984436035\n",
      "epoch 400 loss=250.2422332763672\n",
      "New best validation epoch 400 loss=11.802058219909668\n",
      "epoch 600 loss=242.1219940185547\n",
      "New best validation epoch 600 loss=11.719592094421387\n",
      "epoch 800 loss=234.4228515625\n",
      "New best validation epoch 800 loss=11.645700454711914\n",
      "epoch 1000 loss=234.536376953125\n",
      "New best validation epoch 1000 loss=11.584057807922363\n",
      "epoch 1200 loss=230.5276336669922\n",
      "New best validation epoch 1200 loss=11.534937858581543\n",
      "epoch 1400 loss=234.11422729492188\n",
      "New best validation epoch 1400 loss=11.498230934143066\n",
      "epoch 1600 loss=231.6608123779297\n",
      "New best validation epoch 1600 loss=11.478270530700684\n",
      "epoch 1800 loss=233.16156005859375\n",
      "New best validation epoch 1800 loss=11.472021102905273\n",
      "epoch 2000 loss=231.4957275390625\n",
      "New best validation epoch 2000 loss=11.460792541503906\n",
      "epoch 2200 loss=236.28695678710938\n",
      "epoch 2400 loss=229.8937225341797\n",
      "epoch 2600 loss=231.25889587402344\n",
      "epoch 2800 loss=233.11587524414062\n",
      "epoch 3000 loss=231.85775756835938\n",
      "epoch 3200 loss=241.42111206054688\n",
      "New best validation epoch 3200 loss=11.456249237060547\n",
      "epoch 3400 loss=228.1783905029297\n",
      "New best validation epoch 3400 loss=11.454442977905273\n",
      "epoch 3600 loss=236.71612548828125\n",
      "epoch 3800 loss=232.85861206054688\n",
      "epoch 4000 loss=231.55593872070312\n",
      "epoch 4200 loss=226.9647216796875\n",
      "epoch 4400 loss=231.207763671875\n",
      "epoch 4600 loss=228.5438232421875\n",
      "epoch 4800 loss=227.83824157714844\n",
      "epoch 4999 loss=233.80270385742188\n",
      "Finished training\n",
      "FINISHED 1 1 15.582716941833496\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940])\n",
      "epoch 0 loss=260.24249267578125\n",
      "New best validation epoch 0 loss=11.89256477355957\n",
      "epoch 200 loss=254.43295288085938\n",
      "New best validation epoch 200 loss=11.83519458770752\n",
      "epoch 400 loss=238.24191284179688\n",
      "New best validation epoch 400 loss=11.686062812805176\n",
      "epoch 600 loss=223.9026336669922\n",
      "New best validation epoch 600 loss=11.567485809326172\n",
      "epoch 800 loss=219.8525390625\n",
      "New best validation epoch 800 loss=11.528447151184082\n",
      "epoch 1000 loss=218.7728271484375\n",
      "New best validation epoch 1000 loss=11.522494316101074\n",
      "epoch 1200 loss=219.63682556152344\n",
      "epoch 1400 loss=219.59136962890625\n",
      "epoch 1600 loss=212.40676879882812\n",
      "epoch 1800 loss=225.31661987304688\n",
      "epoch 2000 loss=213.22335815429688\n",
      "epoch 2200 loss=217.32510375976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2400 loss=219.65562438964844\n",
      "epoch 2600 loss=218.62893676757812\n",
      "epoch 2800 loss=219.93064880371094\n",
      "epoch 3000 loss=214.70201110839844\n",
      "epoch 3200 loss=214.1728515625\n",
      "epoch 3400 loss=220.0693817138672\n",
      "epoch 3600 loss=222.50830078125\n",
      "epoch 3800 loss=216.602783203125\n",
      "epoch 4000 loss=226.10064697265625\n",
      "epoch 4200 loss=220.02053833007812\n",
      "epoch 4400 loss=222.73545837402344\n",
      "epoch 4600 loss=222.76316833496094\n",
      "epoch 4800 loss=224.15805053710938\n",
      "epoch 4999 loss=209.68447875976562\n",
      "Finished training\n",
      "FINISHED 1 2 15.47389030456543\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977])\n",
      "epoch 0 loss=260.9819030761719\n",
      "New best validation epoch 0 loss=11.892561912536621\n",
      "epoch 200 loss=250.132568359375\n",
      "New best validation epoch 200 loss=11.800525665283203\n",
      "epoch 400 loss=230.64520263671875\n",
      "New best validation epoch 400 loss=11.569972038269043\n",
      "epoch 600 loss=214.6102752685547\n",
      "New best validation epoch 600 loss=11.388455390930176\n",
      "epoch 800 loss=217.1697998046875\n",
      "New best validation epoch 800 loss=11.316929817199707\n",
      "epoch 1000 loss=211.39329528808594\n",
      "New best validation epoch 1000 loss=11.297820091247559\n",
      "epoch 1200 loss=210.6942138671875\n",
      "New best validation epoch 1200 loss=11.286544799804688\n",
      "epoch 1400 loss=210.65049743652344\n",
      "New best validation epoch 1400 loss=11.277327537536621\n",
      "epoch 1600 loss=214.0964813232422\n",
      "New best validation epoch 1600 loss=11.265238761901855\n",
      "epoch 1800 loss=211.3356170654297\n",
      "New best validation epoch 1800 loss=11.26419734954834\n",
      "epoch 2000 loss=215.06130981445312\n",
      "New best validation epoch 2000 loss=11.253376007080078\n",
      "epoch 2200 loss=202.84619140625\n",
      "epoch 2400 loss=215.63897705078125\n",
      "New best validation epoch 2400 loss=11.25190544128418\n",
      "epoch 2600 loss=207.4847869873047\n",
      "New best validation epoch 2600 loss=11.251617431640625\n",
      "epoch 2800 loss=212.7422332763672\n",
      "epoch 3000 loss=214.73916625976562\n",
      "epoch 3200 loss=211.79376220703125\n",
      "New best validation epoch 3200 loss=11.248438835144043\n",
      "epoch 3400 loss=206.40365600585938\n",
      "New best validation epoch 3400 loss=11.238719940185547\n",
      "epoch 3600 loss=212.5227508544922\n",
      "epoch 3800 loss=216.95672607421875\n",
      "epoch 4000 loss=215.4331512451172\n",
      "epoch 4200 loss=206.56504821777344\n",
      "epoch 4400 loss=216.82205200195312\n",
      "epoch 4600 loss=205.02914428710938\n",
      "epoch 4800 loss=205.84967041015625\n",
      "epoch 4999 loss=207.59390258789062\n",
      "Finished training\n",
      "FINISHED 1 3 15.675097465515137\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211])\n",
      "epoch 0 loss=260.4366760253906\n",
      "New best validation epoch 0 loss=11.892562866210938\n",
      "epoch 200 loss=243.61349487304688\n",
      "New best validation epoch 200 loss=11.725972175598145\n",
      "epoch 400 loss=222.72201538085938\n",
      "New best validation epoch 400 loss=11.372076988220215\n",
      "epoch 600 loss=209.7762908935547\n",
      "New best validation epoch 600 loss=11.191319465637207\n",
      "epoch 800 loss=205.708740234375\n",
      "New best validation epoch 800 loss=11.136199951171875\n",
      "epoch 1000 loss=207.9320068359375\n",
      "New best validation epoch 1000 loss=11.120061874389648\n",
      "epoch 1200 loss=201.73411560058594\n",
      "New best validation epoch 1200 loss=11.106558799743652\n",
      "epoch 1400 loss=209.6488037109375\n",
      "epoch 1600 loss=200.54470825195312\n",
      "New best validation epoch 1600 loss=11.098286628723145\n",
      "epoch 1800 loss=198.32431030273438\n",
      "epoch 2000 loss=212.5653533935547\n",
      "epoch 2200 loss=205.60289001464844\n",
      "epoch 2400 loss=201.91232299804688\n",
      "New best validation epoch 2400 loss=11.09743881225586\n",
      "epoch 2600 loss=198.76034545898438\n",
      "epoch 2800 loss=194.3778076171875\n",
      "epoch 3000 loss=210.58523559570312\n",
      "epoch 3200 loss=204.6313934326172\n",
      "epoch 3400 loss=213.16842651367188\n",
      "epoch 3600 loss=207.8794403076172\n",
      "epoch 3800 loss=197.0269012451172\n",
      "epoch 4000 loss=205.83148193359375\n",
      "New best validation epoch 4000 loss=11.096429824829102\n",
      "epoch 4200 loss=198.016845703125\n",
      "New best validation epoch 4200 loss=11.093684196472168\n",
      "epoch 4400 loss=197.91720581054688\n",
      "epoch 4600 loss=206.22947692871094\n",
      "epoch 4800 loss=196.9641571044922\n",
      "epoch 4999 loss=211.35177612304688\n",
      "Finished training\n",
      "FINISHED 1 4 15.622864723205566\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268, 21270, 27310, 21035, 17453, 21219])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211, 20619, 15885, 28601, 29855, 28178])\n",
      "epoch 0 loss=260.75872802734375\n",
      "New best validation epoch 0 loss=11.892561912536621\n",
      "epoch 200 loss=241.1945037841797\n",
      "New best validation epoch 200 loss=11.665620803833008\n",
      "epoch 400 loss=214.51577758789062\n",
      "New best validation epoch 400 loss=11.277459144592285\n",
      "epoch 600 loss=201.4893035888672\n",
      "New best validation epoch 600 loss=11.159022331237793\n",
      "epoch 800 loss=198.515380859375\n",
      "New best validation epoch 800 loss=11.143081665039062\n",
      "epoch 1000 loss=201.5549774169922\n",
      "New best validation epoch 1000 loss=11.13938045501709\n",
      "epoch 1200 loss=196.35043334960938\n",
      "epoch 1400 loss=189.85423278808594\n",
      "New best validation epoch 1400 loss=11.13647174835205\n",
      "epoch 1600 loss=200.93258666992188\n",
      "New best validation epoch 1600 loss=11.131597518920898\n",
      "epoch 1800 loss=196.1676025390625\n",
      "New best validation epoch 1800 loss=11.126415252685547\n",
      "epoch 2000 loss=201.04794311523438\n",
      "epoch 2200 loss=197.3452606201172\n",
      "epoch 2400 loss=202.1900634765625\n",
      "epoch 2600 loss=200.28240966796875\n",
      "epoch 2800 loss=203.74705505371094\n",
      "epoch 3000 loss=200.8025360107422\n",
      "New best validation epoch 3000 loss=11.1176118850708\n",
      "epoch 3200 loss=210.45872497558594\n",
      "epoch 3400 loss=202.20994567871094\n",
      "epoch 3600 loss=204.25267028808594\n",
      "epoch 3800 loss=201.75247192382812\n",
      "epoch 4000 loss=198.8866424560547\n",
      "epoch 4200 loss=201.21136474609375\n",
      "epoch 4400 loss=195.30145263671875\n",
      "epoch 4600 loss=190.11863708496094\n",
      "epoch 4800 loss=207.746826171875\n",
      "epoch 4999 loss=203.15335083007812\n",
      "Finished training\n",
      "FINISHED 1 5 15.630929946899414\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268, 21270, 27310, 21035, 17453, 21219,\n",
      "         7816, 11300, 13525, 30440, 29136])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211, 20619, 15885, 28601, 29855, 28178,\n",
      "         7490, 30470,  5811, 28661, 29650])\n",
      "epoch 0 loss=260.69476318359375\n",
      "New best validation epoch 0 loss=11.892562866210938\n",
      "epoch 200 loss=238.85665893554688\n",
      "New best validation epoch 200 loss=11.6194486618042\n",
      "epoch 400 loss=207.3541717529297\n",
      "New best validation epoch 400 loss=11.139530181884766\n",
      "epoch 600 loss=192.41796875\n",
      "New best validation epoch 600 loss=10.96674633026123\n",
      "epoch 800 loss=198.24496459960938\n",
      "New best validation epoch 800 loss=10.943731307983398\n",
      "epoch 1000 loss=195.516357421875\n",
      "New best validation epoch 1000 loss=10.930936813354492\n",
      "epoch 1200 loss=182.0474090576172\n",
      "epoch 1400 loss=191.5076904296875\n",
      "New best validation epoch 1400 loss=10.92645263671875\n",
      "epoch 1600 loss=188.45567321777344\n",
      "New best validation epoch 1600 loss=10.926177024841309\n",
      "epoch 1800 loss=201.16531372070312\n",
      "epoch 2000 loss=189.69064331054688\n",
      "New best validation epoch 2000 loss=10.921709060668945\n",
      "epoch 2200 loss=196.3743896484375\n",
      "epoch 2400 loss=193.119873046875\n",
      "New best validation epoch 2400 loss=10.906424522399902\n",
      "epoch 2600 loss=190.95181274414062\n",
      "epoch 2800 loss=185.14236450195312\n",
      "epoch 3000 loss=199.23446655273438\n",
      "epoch 3200 loss=185.51156616210938\n",
      "epoch 3400 loss=194.37887573242188\n",
      "epoch 3600 loss=189.79818725585938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3800 loss=188.20321655273438\n",
      "epoch 4000 loss=189.58395385742188\n",
      "epoch 4200 loss=186.31710815429688\n",
      "epoch 4400 loss=187.6835174560547\n",
      "epoch 4600 loss=191.62237548828125\n",
      "epoch 4800 loss=184.3370361328125\n",
      "epoch 4999 loss=186.7119598388672\n",
      "New best validation epoch 4999 loss=10.89992904663086\n",
      "Finished training\n",
      "FINISHED 1 6 15.373400688171387\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268, 21270, 27310, 21035, 17453, 21219,\n",
      "         7816, 11300, 13525, 30440, 29136,  5613, 32341, 16890, 31705,   508])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211, 20619, 15885, 28601, 29855, 28178,\n",
      "         7490, 30470,  5811, 28661, 29650, 18095, 32203, 12505, 29271, 19730])\n",
      "epoch 0 loss=260.81756591796875\n",
      "New best validation epoch 0 loss=11.892560005187988\n",
      "epoch 200 loss=237.48095703125\n",
      "New best validation epoch 200 loss=11.560065269470215\n",
      "epoch 400 loss=202.62913513183594\n",
      "New best validation epoch 400 loss=10.975162506103516\n",
      "epoch 600 loss=194.49180603027344\n",
      "New best validation epoch 600 loss=10.776287078857422\n",
      "epoch 800 loss=195.6261444091797\n",
      "New best validation epoch 800 loss=10.741547584533691\n",
      "epoch 1000 loss=190.53591918945312\n",
      "New best validation epoch 1000 loss=10.728949546813965\n",
      "epoch 1200 loss=190.29244995117188\n",
      "epoch 1400 loss=181.47055053710938\n",
      "epoch 1600 loss=191.0991668701172\n",
      "epoch 1800 loss=185.90814208984375\n",
      "New best validation epoch 1800 loss=10.728281021118164\n",
      "epoch 2000 loss=189.81747436523438\n",
      "epoch 2200 loss=187.6368865966797\n",
      "New best validation epoch 2200 loss=10.726778030395508\n",
      "epoch 2400 loss=195.18600463867188\n",
      "New best validation epoch 2400 loss=10.72500228881836\n",
      "epoch 2600 loss=187.5777587890625\n",
      "epoch 2800 loss=187.73812866210938\n",
      "epoch 3000 loss=189.8726043701172\n",
      "New best validation epoch 3000 loss=10.720441818237305\n",
      "epoch 3200 loss=189.3092498779297\n",
      "New best validation epoch 3200 loss=10.71721363067627\n",
      "epoch 3400 loss=175.66146850585938\n",
      "epoch 3600 loss=184.18051147460938\n",
      "epoch 3800 loss=190.53729248046875\n",
      "epoch 4000 loss=187.59475708007812\n",
      "epoch 4200 loss=180.3331298828125\n",
      "epoch 4400 loss=186.25985717773438\n",
      "epoch 4600 loss=180.50228881835938\n",
      "epoch 4800 loss=188.27825927734375\n",
      "epoch 4999 loss=189.4936981201172\n",
      "Finished training\n",
      "FINISHED 1 7 15.397788047790527\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268, 21270, 27310, 21035, 17453, 21219,\n",
      "         7816, 11300, 13525, 30440, 29136,  5613, 32341, 16890, 31705,   508,\n",
      "        11046, 11840,  1078, 16171, 23795])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211, 20619, 15885, 28601, 29855, 28178,\n",
      "         7490, 30470,  5811, 28661, 29650, 18095, 32203, 12505, 29271, 19730,\n",
      "         2502, 18227, 33901, 22809,   548])\n",
      "epoch 0 loss=260.6593017578125\n",
      "New best validation epoch 0 loss=11.892559051513672\n",
      "epoch 200 loss=230.78662109375\n",
      "New best validation epoch 200 loss=11.427833557128906\n",
      "epoch 400 loss=198.23887634277344\n",
      "New best validation epoch 400 loss=10.746063232421875\n",
      "epoch 600 loss=184.300048828125\n",
      "New best validation epoch 600 loss=10.562515258789062\n",
      "epoch 800 loss=179.35107421875\n",
      "New best validation epoch 800 loss=10.53377628326416\n",
      "epoch 1000 loss=184.6212158203125\n",
      "epoch 1200 loss=185.9916229248047\n",
      "New best validation epoch 1200 loss=10.533628463745117\n",
      "epoch 1400 loss=171.7028045654297\n",
      "epoch 1600 loss=185.19932556152344\n",
      "epoch 1800 loss=188.54879760742188\n",
      "epoch 2000 loss=166.22251892089844\n",
      "epoch 2200 loss=181.25584411621094\n",
      "epoch 2400 loss=191.2506561279297\n",
      "epoch 2600 loss=183.3656005859375\n",
      "epoch 2800 loss=181.58372497558594\n",
      "epoch 3000 loss=181.78575134277344\n",
      "epoch 3200 loss=187.9659423828125\n",
      "epoch 3400 loss=185.782470703125\n",
      "epoch 3600 loss=189.5150604248047\n",
      "epoch 3800 loss=182.14903259277344\n",
      "epoch 4000 loss=176.318603515625\n",
      "epoch 4200 loss=173.74777221679688\n",
      "epoch 4400 loss=185.1316375732422\n",
      "epoch 4600 loss=188.54345703125\n",
      "epoch 4800 loss=183.1133270263672\n",
      "epoch 4999 loss=182.10134887695312\n",
      "Finished training\n",
      "FINISHED 1 8 15.34245777130127\n",
      "tensor([23469, 21858, 21035, 10925,  6715, 22205, 21728, 29047, 22196, 25522,\n",
      "        26016, 18995,  7166, 23557, 31155,   991,   920, 31120, 11512,  7574,\n",
      "        22790, 21615, 24720, 22798, 18268, 21270, 27310, 21035, 17453, 21219,\n",
      "         7816, 11300, 13525, 30440, 29136,  5613, 32341, 16890, 31705,   508,\n",
      "        11046, 11840,  1078, 16171, 23795,  7584, 18807,  1972, 20618, 22776])\n",
      "tensor([10626,  2171,  9030, 11046, 34516, 26322, 18053, 33856, 26213, 26214,\n",
      "          148, 31367,  1167, 32283, 26940,  6894, 12322, 28895,  5222, 10977,\n",
      "        27555, 29358,  7253, 25405, 15211, 20619, 15885, 28601, 29855, 28178,\n",
      "         7490, 30470,  5811, 28661, 29650, 18095, 32203, 12505, 29271, 19730,\n",
      "         2502, 18227, 33901, 22809,   548, 11811, 28886,   308, 32834,  7193])\n",
      "epoch 0 loss=260.849853515625\n",
      "New best validation epoch 0 loss=11.892557144165039\n",
      "epoch 200 loss=225.28466796875\n",
      "New best validation epoch 200 loss=11.386873245239258\n",
      "epoch 400 loss=183.46014404296875\n",
      "New best validation epoch 400 loss=10.783923149108887\n",
      "epoch 600 loss=183.28720092773438\n",
      "New best validation epoch 600 loss=10.656354904174805\n",
      "epoch 800 loss=180.68099975585938\n",
      "New best validation epoch 800 loss=10.633523941040039\n",
      "epoch 1000 loss=178.46566772460938\n",
      "epoch 1200 loss=174.29595947265625\n",
      "New best validation epoch 1200 loss=10.625191688537598\n",
      "epoch 1400 loss=183.9371337890625\n",
      "New best validation epoch 1400 loss=10.620922088623047\n",
      "epoch 1600 loss=182.64622497558594\n",
      "epoch 1800 loss=181.1633758544922\n",
      "epoch 2000 loss=181.659912109375\n",
      "epoch 2200 loss=179.0321044921875\n",
      "epoch 2400 loss=176.3928680419922\n",
      "epoch 2600 loss=176.4652557373047\n",
      "epoch 2800 loss=175.09564208984375\n",
      "epoch 3000 loss=182.38674926757812\n",
      "epoch 3200 loss=183.2135467529297\n",
      "epoch 3400 loss=178.4552459716797\n",
      "epoch 3600 loss=184.6004638671875\n",
      "epoch 3800 loss=171.295166015625\n",
      "epoch 4000 loss=174.56838989257812\n",
      "epoch 4200 loss=183.6708221435547\n",
      "New best validation epoch 4200 loss=10.608542442321777\n",
      "epoch 4400 loss=186.314453125\n",
      "epoch 4600 loss=174.62835693359375\n",
      "epoch 4800 loss=182.40867614746094\n",
      "epoch 4999 loss=178.8323974609375\n",
      "Finished training\n",
      "FINISHED 1 9 15.456366539001465\n",
      "tensor([10626, 31155,   139, 23469, 30976])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160])\n",
      "epoch 0 loss=270.3739929199219\n",
      "New best validation epoch 0 loss=12.067731857299805\n",
      "epoch 200 loss=268.48126220703125\n",
      "New best validation epoch 200 loss=12.048277854919434\n",
      "epoch 400 loss=266.5220031738281\n",
      "New best validation epoch 400 loss=11.987112045288086\n",
      "epoch 600 loss=259.47528076171875\n",
      "New best validation epoch 600 loss=11.89992618560791\n",
      "epoch 800 loss=259.82086181640625\n",
      "New best validation epoch 800 loss=11.814126014709473\n",
      "epoch 1000 loss=255.66268920898438\n",
      "New best validation epoch 1000 loss=11.757994651794434\n",
      "epoch 1200 loss=251.0491180419922\n",
      "New best validation epoch 1200 loss=11.731863021850586\n",
      "epoch 1400 loss=256.0163269042969\n",
      "New best validation epoch 1400 loss=11.724996566772461\n",
      "epoch 1600 loss=255.93814086914062\n",
      "New best validation epoch 1600 loss=11.715982437133789\n",
      "epoch 1800 loss=251.7418212890625\n",
      "epoch 2000 loss=255.96392822265625\n",
      "epoch 2200 loss=255.66064453125\n",
      "New best validation epoch 2200 loss=11.713028907775879\n",
      "epoch 2400 loss=250.69326782226562\n",
      "New best validation epoch 2400 loss=11.709892272949219\n",
      "epoch 2600 loss=254.57373046875\n",
      "New best validation epoch 2600 loss=11.708706855773926\n",
      "epoch 2800 loss=254.23863220214844\n",
      "epoch 3000 loss=253.92388916015625\n",
      "epoch 3200 loss=244.32180786132812\n",
      "New best validation epoch 3200 loss=11.707138061523438\n",
      "epoch 3400 loss=252.99801635742188\n",
      "epoch 3600 loss=252.8610076904297\n",
      "epoch 3800 loss=255.43002319335938\n",
      "New best validation epoch 3800 loss=11.705268859863281\n",
      "epoch 4000 loss=254.56297302246094\n",
      "New best validation epoch 4000 loss=11.70468807220459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4200 loss=258.4997253417969\n",
      "epoch 4400 loss=256.8087158203125\n",
      "epoch 4600 loss=251.88372802734375\n",
      "New best validation epoch 4600 loss=11.704629898071289\n",
      "epoch 4800 loss=249.3871307373047\n",
      "epoch 4999 loss=253.11557006835938\n",
      "New best validation epoch 4999 loss=11.703339576721191\n",
      "Finished training\n",
      "FINISHED 2 0 13.231029510498047\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192])\n",
      "epoch 0 loss=270.06298828125\n",
      "New best validation epoch 0 loss=12.067733764648438\n",
      "epoch 200 loss=266.56060791015625\n",
      "New best validation epoch 200 loss=12.03956127166748\n",
      "epoch 400 loss=258.9329528808594\n",
      "New best validation epoch 400 loss=11.96567440032959\n",
      "epoch 600 loss=243.08468627929688\n",
      "New best validation epoch 600 loss=11.899320602416992\n",
      "epoch 800 loss=235.9285125732422\n",
      "New best validation epoch 800 loss=11.856619834899902\n",
      "epoch 1000 loss=245.8970947265625\n",
      "New best validation epoch 1000 loss=11.815571784973145\n",
      "epoch 1200 loss=240.0391082763672\n",
      "New best validation epoch 1200 loss=11.780067443847656\n",
      "epoch 1400 loss=239.38784790039062\n",
      "New best validation epoch 1400 loss=11.755143165588379\n",
      "epoch 1600 loss=244.9664764404297\n",
      "New best validation epoch 1600 loss=11.736519813537598\n",
      "epoch 1800 loss=237.83152770996094\n",
      "New best validation epoch 1800 loss=11.725058555603027\n",
      "epoch 2000 loss=239.420166015625\n",
      "New best validation epoch 2000 loss=11.717437744140625\n",
      "epoch 2200 loss=244.17120361328125\n",
      "New best validation epoch 2200 loss=11.714045524597168\n",
      "epoch 2400 loss=237.2501678466797\n",
      "New best validation epoch 2400 loss=11.708914756774902\n",
      "epoch 2600 loss=237.03050231933594\n",
      "New best validation epoch 2600 loss=11.704087257385254\n",
      "epoch 2800 loss=244.89718627929688\n",
      "New best validation epoch 2800 loss=11.700417518615723\n",
      "epoch 3000 loss=241.81109619140625\n",
      "New best validation epoch 3000 loss=11.699803352355957\n",
      "epoch 3200 loss=241.92771911621094\n",
      "epoch 3400 loss=242.80569458007812\n",
      "New best validation epoch 3400 loss=11.697136878967285\n",
      "epoch 3600 loss=229.3787841796875\n",
      "epoch 3800 loss=242.76907348632812\n",
      "epoch 4000 loss=238.41744995117188\n",
      "New best validation epoch 4000 loss=11.69427490234375\n",
      "epoch 4200 loss=235.9644012451172\n",
      "epoch 4400 loss=246.3379364013672\n",
      "epoch 4600 loss=241.87713623046875\n",
      "epoch 4800 loss=231.94552612304688\n",
      "epoch 4999 loss=231.92733764648438\n",
      "Finished training\n",
      "FINISHED 2 1 13.664268493652344\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064])\n",
      "epoch 0 loss=269.5016784667969\n",
      "New best validation epoch 0 loss=12.067732810974121\n",
      "epoch 200 loss=263.0823974609375\n",
      "New best validation epoch 200 loss=11.97895622253418\n",
      "epoch 400 loss=246.5297088623047\n",
      "New best validation epoch 400 loss=11.746369361877441\n",
      "epoch 600 loss=235.0983123779297\n",
      "New best validation epoch 600 loss=11.55305004119873\n",
      "epoch 800 loss=234.6376190185547\n",
      "New best validation epoch 800 loss=11.496963500976562\n",
      "epoch 1000 loss=231.00314331054688\n",
      "New best validation epoch 1000 loss=11.484806060791016\n",
      "epoch 1200 loss=224.73699951171875\n",
      "New best validation epoch 1200 loss=11.479424476623535\n",
      "epoch 1400 loss=223.47900390625\n",
      "New best validation epoch 1400 loss=11.468095779418945\n",
      "epoch 1600 loss=233.99998474121094\n",
      "New best validation epoch 1600 loss=11.465058326721191\n",
      "epoch 1800 loss=225.68954467773438\n",
      "New best validation epoch 1800 loss=11.456303596496582\n",
      "epoch 2000 loss=229.277099609375\n",
      "New best validation epoch 2000 loss=11.450016975402832\n",
      "epoch 2200 loss=227.87596130371094\n",
      "New best validation epoch 2200 loss=11.44910717010498\n",
      "epoch 2400 loss=230.6030731201172\n",
      "New best validation epoch 2400 loss=11.44433307647705\n",
      "epoch 2600 loss=231.07861328125\n",
      "epoch 2800 loss=229.69580078125\n",
      "epoch 3000 loss=226.9039306640625\n",
      "epoch 3200 loss=229.297607421875\n",
      "epoch 3400 loss=232.7933807373047\n",
      "epoch 3600 loss=226.06790161132812\n",
      "epoch 3800 loss=230.41970825195312\n",
      "epoch 4000 loss=227.85679626464844\n",
      "epoch 4200 loss=227.3143310546875\n",
      "New best validation epoch 4200 loss=11.44389820098877\n",
      "epoch 4400 loss=229.3417205810547\n",
      "epoch 4600 loss=219.9741668701172\n",
      "epoch 4800 loss=227.95396423339844\n",
      "epoch 4999 loss=232.4716033935547\n",
      "New best validation epoch 4999 loss=11.442671775817871\n",
      "Finished training\n",
      "FINISHED 2 2 13.394472122192383\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793])\n",
      "epoch 0 loss=270.28692626953125\n",
      "New best validation epoch 0 loss=12.067731857299805\n",
      "epoch 200 loss=258.7231750488281\n",
      "New best validation epoch 200 loss=11.939220428466797\n",
      "epoch 400 loss=239.38084411621094\n",
      "New best validation epoch 400 loss=11.646174430847168\n",
      "epoch 600 loss=222.77978515625\n",
      "New best validation epoch 600 loss=11.504186630249023\n",
      "epoch 800 loss=230.0146484375\n",
      "New best validation epoch 800 loss=11.498903274536133\n",
      "epoch 1000 loss=230.51022338867188\n",
      "epoch 1200 loss=227.21055603027344\n",
      "epoch 1400 loss=222.223388671875\n",
      "epoch 1600 loss=218.6595916748047\n",
      "epoch 1800 loss=219.5697021484375\n",
      "epoch 2000 loss=229.83311462402344\n",
      "epoch 2200 loss=213.9908447265625\n",
      "epoch 2400 loss=212.57269287109375\n",
      "epoch 2600 loss=215.8306121826172\n",
      "epoch 2800 loss=219.81089782714844\n",
      "epoch 3000 loss=228.53781127929688\n",
      "epoch 3200 loss=216.6077117919922\n",
      "epoch 3400 loss=217.83053588867188\n",
      "epoch 3600 loss=226.93576049804688\n",
      "epoch 3800 loss=221.8009033203125\n",
      "epoch 4000 loss=224.2613983154297\n",
      "epoch 4200 loss=213.46762084960938\n",
      "epoch 4400 loss=220.15673828125\n",
      "epoch 4600 loss=225.4684295654297\n",
      "epoch 4800 loss=215.63790893554688\n",
      "epoch 4999 loss=217.53424072265625\n",
      "Finished training\n",
      "FINISHED 2 3 12.984502792358398\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463])\n",
      "epoch 0 loss=269.28619384765625\n",
      "New best validation epoch 0 loss=12.067730903625488\n",
      "epoch 200 loss=253.39303588867188\n",
      "New best validation epoch 200 loss=11.890997886657715\n",
      "epoch 400 loss=229.34727478027344\n",
      "New best validation epoch 400 loss=11.510017395019531\n",
      "epoch 600 loss=212.92984008789062\n",
      "New best validation epoch 600 loss=11.337822914123535\n",
      "epoch 800 loss=221.17111206054688\n",
      "New best validation epoch 800 loss=11.304807662963867\n",
      "epoch 1000 loss=211.54241943359375\n",
      "New best validation epoch 1000 loss=11.29841423034668\n",
      "epoch 1200 loss=218.42697143554688\n",
      "New best validation epoch 1200 loss=11.285700798034668\n",
      "epoch 1400 loss=211.81866455078125\n",
      "New best validation epoch 1400 loss=11.279557228088379\n",
      "epoch 1600 loss=216.56280517578125\n",
      "New best validation epoch 1600 loss=11.275392532348633\n",
      "epoch 1800 loss=219.02029418945312\n",
      "New best validation epoch 1800 loss=11.265183448791504\n",
      "epoch 2000 loss=212.20343017578125\n",
      "New best validation epoch 2000 loss=11.264431953430176\n",
      "epoch 2200 loss=197.13308715820312\n",
      "New best validation epoch 2200 loss=11.251766204833984\n",
      "epoch 2400 loss=210.61367797851562\n",
      "New best validation epoch 2400 loss=11.248440742492676\n",
      "epoch 2600 loss=212.04281616210938\n",
      "New best validation epoch 2600 loss=11.246203422546387\n",
      "epoch 2800 loss=202.0277099609375\n",
      "epoch 3000 loss=212.2823486328125\n",
      "epoch 3200 loss=214.46080017089844\n",
      "epoch 3400 loss=216.39700317382812\n",
      "New best validation epoch 3400 loss=11.240418434143066\n",
      "epoch 3600 loss=205.11590576171875\n",
      "epoch 3800 loss=217.017333984375\n",
      "epoch 4000 loss=212.57528686523438\n",
      "epoch 4200 loss=204.7297821044922\n",
      "epoch 4400 loss=208.1367645263672\n",
      "epoch 4600 loss=214.48388671875\n",
      "New best validation epoch 4600 loss=11.238779067993164\n",
      "epoch 4800 loss=213.7757568359375\n",
      "New best validation epoch 4800 loss=11.237093925476074\n",
      "epoch 4999 loss=208.66915893554688\n",
      "Finished training\n",
      "FINISHED 2 4 12.873269081115723\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498, 11084,   991, 20022,  7816,  8825])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463, 23701, 18257, 20619, 34565, 25322])\n",
      "epoch 0 loss=268.886962890625\n",
      "New best validation epoch 0 loss=12.067733764648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=253.2611541748047\n",
      "New best validation epoch 200 loss=11.853787422180176\n",
      "epoch 400 loss=209.08018493652344\n",
      "New best validation epoch 400 loss=11.439409255981445\n",
      "epoch 600 loss=205.45980834960938\n",
      "New best validation epoch 600 loss=11.319504737854004\n",
      "epoch 800 loss=205.78927612304688\n",
      "epoch 1000 loss=197.94943237304688\n",
      "New best validation epoch 1000 loss=11.317416191101074\n",
      "epoch 1200 loss=202.41905212402344\n",
      "epoch 1400 loss=202.67428588867188\n",
      "epoch 1600 loss=205.83163452148438\n",
      "epoch 1800 loss=192.45591735839844\n",
      "epoch 2000 loss=202.820068359375\n",
      "epoch 2200 loss=197.24212646484375\n",
      "epoch 2400 loss=203.7411346435547\n",
      "New best validation epoch 2400 loss=11.31435775756836\n",
      "epoch 2600 loss=200.19015502929688\n",
      "epoch 2800 loss=214.25115966796875\n",
      "epoch 3000 loss=191.12295532226562\n",
      "epoch 3200 loss=195.675537109375\n",
      "epoch 3400 loss=202.024169921875\n",
      "epoch 3600 loss=203.14541625976562\n",
      "epoch 3800 loss=205.61236572265625\n",
      "epoch 4000 loss=203.52011108398438\n",
      "epoch 4200 loss=210.8107147216797\n",
      "epoch 4400 loss=209.10374450683594\n",
      "epoch 4600 loss=204.77513122558594\n",
      "epoch 4800 loss=206.09530639648438\n",
      "epoch 4999 loss=210.66390991210938\n",
      "Finished training\n",
      "FINISHED 2 5 12.786083221435547\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498, 11084,   991, 20022,  7816,  8825,\n",
      "        22790, 16709, 16653,  1715,  3263])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463, 23701, 18257, 20619, 34565, 25322,\n",
      "        20700, 22637, 22326, 20697, 30191])\n",
      "epoch 0 loss=269.7949523925781\n",
      "New best validation epoch 0 loss=12.067730903625488\n",
      "epoch 200 loss=249.49569702148438\n",
      "New best validation epoch 200 loss=11.832422256469727\n",
      "epoch 400 loss=209.9258270263672\n",
      "New best validation epoch 400 loss=11.418867111206055\n",
      "epoch 600 loss=196.5128173828125\n",
      "New best validation epoch 600 loss=11.328595161437988\n",
      "epoch 800 loss=195.61471557617188\n",
      "New best validation epoch 800 loss=11.32420825958252\n",
      "epoch 1000 loss=197.28024291992188\n",
      "epoch 1200 loss=207.52037048339844\n",
      "epoch 1400 loss=196.77548217773438\n",
      "New best validation epoch 1400 loss=11.322392463684082\n",
      "epoch 1600 loss=201.6451873779297\n",
      "epoch 1800 loss=195.77537536621094\n",
      "epoch 2000 loss=187.5360107421875\n",
      "New best validation epoch 2000 loss=11.322009086608887\n",
      "epoch 2200 loss=200.04820251464844\n",
      "epoch 2400 loss=196.97665405273438\n",
      "New best validation epoch 2400 loss=11.313591957092285\n",
      "epoch 2600 loss=194.42962646484375\n",
      "epoch 2800 loss=195.77713012695312\n",
      "New best validation epoch 2800 loss=11.306628227233887\n",
      "epoch 3000 loss=198.64813232421875\n",
      "epoch 3200 loss=198.60052490234375\n",
      "epoch 3400 loss=189.1939697265625\n",
      "New best validation epoch 3400 loss=11.301960945129395\n",
      "epoch 3600 loss=201.97203063964844\n",
      "epoch 3800 loss=204.57382202148438\n",
      "New best validation epoch 3800 loss=11.300870895385742\n",
      "epoch 4000 loss=206.08274841308594\n",
      "epoch 4200 loss=201.179443359375\n",
      "epoch 4400 loss=196.818359375\n",
      "epoch 4600 loss=197.59213256835938\n",
      "epoch 4800 loss=195.79421997070312\n",
      "epoch 4999 loss=196.78445434570312\n",
      "Finished training\n",
      "FINISHED 2 6 12.765907287597656\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498, 11084,   991, 20022,  7816,  8825,\n",
      "        22790, 16709, 16653,  1715,  3263, 30330, 27798, 28813, 29136, 28815])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463, 23701, 18257, 20619, 34565, 25322,\n",
      "        20700, 22637, 22326, 20697, 30191, 29238, 28904,  5368, 27497, 34575])\n",
      "epoch 0 loss=269.67041015625\n",
      "New best validation epoch 0 loss=12.067728042602539\n",
      "epoch 200 loss=242.4197235107422\n",
      "New best validation epoch 200 loss=11.775870323181152\n",
      "epoch 400 loss=203.72100830078125\n",
      "New best validation epoch 400 loss=11.388440132141113\n",
      "epoch 600 loss=194.79653930664062\n",
      "New best validation epoch 600 loss=11.353708267211914\n",
      "epoch 800 loss=192.5601806640625\n",
      "New best validation epoch 800 loss=11.340938568115234\n",
      "epoch 1000 loss=193.49874877929688\n",
      "New best validation epoch 1000 loss=11.33734130859375\n",
      "epoch 1200 loss=192.4249267578125\n",
      "epoch 1400 loss=195.69114685058594\n",
      "epoch 1600 loss=189.07028198242188\n",
      "New best validation epoch 1600 loss=11.324108123779297\n",
      "epoch 1800 loss=196.75460815429688\n",
      "epoch 2000 loss=192.0352020263672\n",
      "epoch 2200 loss=194.37307739257812\n",
      "epoch 2400 loss=196.8925018310547\n",
      "epoch 2600 loss=180.99029541015625\n",
      "epoch 2800 loss=191.5719451904297\n",
      "epoch 3000 loss=195.17474365234375\n",
      "epoch 3200 loss=194.5908203125\n",
      "epoch 3400 loss=197.0418701171875\n",
      "epoch 3600 loss=189.46458435058594\n",
      "epoch 3800 loss=197.1756591796875\n",
      "epoch 4000 loss=190.18673706054688\n",
      "epoch 4200 loss=183.20785522460938\n",
      "epoch 4400 loss=184.9633026123047\n",
      "epoch 4600 loss=195.89952087402344\n",
      "epoch 4800 loss=187.36692810058594\n",
      "epoch 4999 loss=192.4052734375\n",
      "Finished training\n",
      "FINISHED 2 7 12.728092193603516\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498, 11084,   991, 20022,  7816,  8825,\n",
      "        22790, 16709, 16653,  1715,  3263, 30330, 27798, 28813, 29136, 28815,\n",
      "         6092, 22053,  7737,  5613, 18231])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463, 23701, 18257, 20619, 34565, 25322,\n",
      "        20700, 22637, 22326, 20697, 30191, 29238, 28904,  5368, 27497, 34575,\n",
      "        18982, 27210, 28378, 31154, 22387])\n",
      "epoch 0 loss=269.4590759277344\n",
      "New best validation epoch 0 loss=12.067727088928223\n",
      "epoch 200 loss=238.68833923339844\n",
      "New best validation epoch 200 loss=11.726786613464355\n",
      "epoch 400 loss=201.12815856933594\n",
      "New best validation epoch 400 loss=11.35845947265625\n",
      "epoch 600 loss=189.3009033203125\n",
      "New best validation epoch 600 loss=11.353206634521484\n",
      "epoch 800 loss=176.56753540039062\n",
      "epoch 1000 loss=185.1272430419922\n",
      "New best validation epoch 1000 loss=11.351197242736816\n",
      "epoch 1200 loss=187.004150390625\n",
      "New best validation epoch 1200 loss=11.34394359588623\n",
      "epoch 1400 loss=191.56173706054688\n",
      "epoch 1600 loss=189.7366180419922\n",
      "New best validation epoch 1600 loss=11.325578689575195\n",
      "epoch 1800 loss=180.093994140625\n",
      "New best validation epoch 1800 loss=11.31777572631836\n",
      "epoch 2000 loss=187.67919921875\n",
      "New best validation epoch 2000 loss=11.31031322479248\n",
      "epoch 2200 loss=184.0902099609375\n",
      "epoch 2400 loss=194.04367065429688\n",
      "epoch 2600 loss=188.1244354248047\n",
      "epoch 2800 loss=188.4705047607422\n",
      "New best validation epoch 2800 loss=11.30803394317627\n",
      "epoch 3000 loss=197.52349853515625\n",
      "epoch 3200 loss=178.2290496826172\n",
      "epoch 3400 loss=190.44696044921875\n",
      "epoch 3600 loss=187.88072204589844\n",
      "epoch 3800 loss=193.82395935058594\n",
      "epoch 4000 loss=184.8323974609375\n",
      "New best validation epoch 4000 loss=11.305253982543945\n",
      "epoch 4200 loss=184.97195434570312\n",
      "epoch 4400 loss=189.38412475585938\n",
      "New best validation epoch 4400 loss=11.299765586853027\n",
      "epoch 4600 loss=194.1935272216797\n",
      "epoch 4800 loss=191.5713348388672\n",
      "epoch 4999 loss=186.65121459960938\n",
      "Finished training\n",
      "FINISHED 2 8 12.83489990234375\n",
      "tensor([10626, 31155,   139, 23469, 30976, 19651, 22141, 22779, 16607, 19220,\n",
      "        22798, 15282,  7166, 15284, 11032, 21858, 20608,  8491, 24047,  3300,\n",
      "        21615, 19069, 29119, 29120, 30498, 11084,   991, 20022,  7816,  8825,\n",
      "        22790, 16709, 16653,  1715,  3263, 30330, 27798, 28813, 29136, 28815,\n",
      "         6092, 22053,  7737,  5613, 18231, 30348, 17453, 26230, 11537, 10659])\n",
      "tensor([ 1167, 10626, 31147, 25274,  1160, 20391, 19550, 29958, 20506,  3192,\n",
      "         8610,  8850, 28390,  6403,  9064,  2254, 12980,  1735, 32283, 22793,\n",
      "        13803, 23237, 33488, 31143, 30463, 23701, 18257, 20619, 34565, 25322,\n",
      "        20700, 22637, 22326, 20697, 30191, 29238, 28904,  5368, 27497, 34575,\n",
      "        18982, 27210, 28378, 31154, 22387,  7451, 17054,   605,  3584,  2831])\n",
      "epoch 0 loss=269.5958251953125\n",
      "New best validation epoch 0 loss=12.067728042602539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=234.72726440429688\n",
      "New best validation epoch 200 loss=11.730265617370605\n",
      "epoch 400 loss=195.51608276367188\n",
      "New best validation epoch 400 loss=11.401272773742676\n",
      "epoch 600 loss=193.747314453125\n",
      "New best validation epoch 600 loss=11.401183128356934\n",
      "epoch 800 loss=175.84963989257812\n",
      "epoch 1000 loss=181.0313720703125\n",
      "New best validation epoch 1000 loss=11.387483596801758\n",
      "epoch 1200 loss=180.46878051757812\n",
      "New best validation epoch 1200 loss=11.38001823425293\n",
      "epoch 1400 loss=186.34555053710938\n",
      "epoch 1600 loss=181.2662811279297\n",
      "epoch 1800 loss=182.44139099121094\n",
      "New best validation epoch 1800 loss=11.379240989685059\n",
      "epoch 2000 loss=185.43869018554688\n",
      "New best validation epoch 2000 loss=11.360414505004883\n",
      "epoch 2200 loss=176.10415649414062\n",
      "New best validation epoch 2200 loss=11.357413291931152\n",
      "epoch 2400 loss=180.41162109375\n",
      "epoch 2600 loss=178.3839111328125\n",
      "epoch 2800 loss=189.90782165527344\n",
      "New best validation epoch 2800 loss=11.354792594909668\n",
      "epoch 3000 loss=184.7593231201172\n",
      "epoch 3200 loss=182.01914978027344\n",
      "epoch 3400 loss=190.51766967773438\n",
      "epoch 3600 loss=186.79173278808594\n",
      "epoch 3800 loss=190.75657653808594\n",
      "epoch 4000 loss=175.62399291992188\n",
      "New best validation epoch 4000 loss=11.34940242767334\n",
      "epoch 4200 loss=182.28265380859375\n",
      "epoch 4400 loss=178.68609619140625\n",
      "epoch 4600 loss=187.26904296875\n",
      "epoch 4800 loss=188.004150390625\n",
      "epoch 4999 loss=191.6715545654297\n",
      "Finished training\n",
      "FINISHED 2 9 12.855852127075195\n",
      "tensor([23469, 22006, 24276,  6715, 23702])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006])\n",
      "epoch 0 loss=256.54949951171875\n",
      "New best validation epoch 0 loss=11.804819107055664\n",
      "epoch 200 loss=256.5269470214844\n",
      "New best validation epoch 200 loss=11.783473014831543\n",
      "epoch 400 loss=253.4269256591797\n",
      "New best validation epoch 400 loss=11.715188026428223\n",
      "epoch 600 loss=250.51910400390625\n",
      "New best validation epoch 600 loss=11.611503601074219\n",
      "epoch 800 loss=248.39749145507812\n",
      "New best validation epoch 800 loss=11.501012802124023\n",
      "epoch 1000 loss=248.71214294433594\n",
      "New best validation epoch 1000 loss=11.427734375\n",
      "epoch 1200 loss=243.01499938964844\n",
      "New best validation epoch 1200 loss=11.389620780944824\n",
      "epoch 1400 loss=244.89695739746094\n",
      "New best validation epoch 1400 loss=11.373281478881836\n",
      "epoch 1600 loss=243.58189392089844\n",
      "epoch 1800 loss=244.56248474121094\n",
      "New best validation epoch 1800 loss=11.368210792541504\n",
      "epoch 2000 loss=240.91299438476562\n",
      "New best validation epoch 2000 loss=11.365363121032715\n",
      "epoch 2200 loss=244.145263671875\n",
      "New best validation epoch 2200 loss=11.363118171691895\n",
      "epoch 2400 loss=243.55010986328125\n",
      "New best validation epoch 2400 loss=11.355286598205566\n",
      "epoch 2600 loss=245.4810791015625\n",
      "epoch 2800 loss=246.4927978515625\n",
      "New best validation epoch 2800 loss=11.350391387939453\n",
      "epoch 3000 loss=233.45050048828125\n",
      "epoch 3200 loss=242.59825134277344\n",
      "New best validation epoch 3200 loss=11.346335411071777\n",
      "epoch 3400 loss=242.9158172607422\n",
      "epoch 3600 loss=240.3451385498047\n",
      "epoch 3800 loss=239.81663513183594\n",
      "New best validation epoch 3800 loss=11.346196174621582\n",
      "epoch 4000 loss=242.234375\n",
      "New best validation epoch 4000 loss=11.345491409301758\n",
      "epoch 4200 loss=242.82725524902344\n",
      "New best validation epoch 4200 loss=11.339483261108398\n",
      "epoch 4400 loss=242.16050720214844\n",
      "epoch 4600 loss=248.11141967773438\n",
      "New best validation epoch 4600 loss=11.337105751037598\n",
      "epoch 4800 loss=241.26292419433594\n",
      "epoch 4999 loss=245.318115234375\n",
      "Finished training\n",
      "FINISHED 3 0 16.673933029174805\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483])\n",
      "epoch 0 loss=257.0829772949219\n",
      "New best validation epoch 0 loss=11.804819107055664\n",
      "epoch 200 loss=253.98533630371094\n",
      "New best validation epoch 200 loss=11.76909065246582\n",
      "epoch 400 loss=242.5863494873047\n",
      "New best validation epoch 400 loss=11.669207572937012\n",
      "epoch 600 loss=231.60311889648438\n",
      "New best validation epoch 600 loss=11.561161041259766\n",
      "epoch 800 loss=231.68260192871094\n",
      "New best validation epoch 800 loss=11.481585502624512\n",
      "epoch 1000 loss=228.67352294921875\n",
      "New best validation epoch 1000 loss=11.427034378051758\n",
      "epoch 1200 loss=232.1449432373047\n",
      "New best validation epoch 1200 loss=11.389241218566895\n",
      "epoch 1400 loss=225.24070739746094\n",
      "New best validation epoch 1400 loss=11.36284351348877\n",
      "epoch 1600 loss=226.15325927734375\n",
      "New best validation epoch 1600 loss=11.344569206237793\n",
      "epoch 1800 loss=225.66403198242188\n",
      "New best validation epoch 1800 loss=11.334349632263184\n",
      "epoch 2000 loss=222.01864624023438\n",
      "New best validation epoch 2000 loss=11.3241605758667\n",
      "epoch 2200 loss=231.6903076171875\n",
      "New best validation epoch 2200 loss=11.317079544067383\n",
      "epoch 2400 loss=229.59103393554688\n",
      "New best validation epoch 2400 loss=11.312847137451172\n",
      "epoch 2600 loss=223.6180877685547\n",
      "New best validation epoch 2600 loss=11.309957504272461\n",
      "epoch 2800 loss=223.68711853027344\n",
      "New best validation epoch 2800 loss=11.309412002563477\n",
      "epoch 3000 loss=222.33731079101562\n",
      "New best validation epoch 3000 loss=11.307927131652832\n",
      "epoch 3200 loss=226.96759033203125\n",
      "New best validation epoch 3200 loss=11.304651260375977\n",
      "epoch 3400 loss=236.83006286621094\n",
      "New best validation epoch 3400 loss=11.301329612731934\n",
      "epoch 3600 loss=229.7933349609375\n",
      "epoch 3800 loss=227.8377685546875\n",
      "New best validation epoch 3800 loss=11.299506187438965\n",
      "epoch 4000 loss=231.79269409179688\n",
      "New best validation epoch 4000 loss=11.294340133666992\n",
      "epoch 4200 loss=225.72674560546875\n",
      "epoch 4400 loss=227.54608154296875\n",
      "epoch 4600 loss=229.3350067138672\n",
      "epoch 4800 loss=232.85324096679688\n",
      "epoch 4999 loss=226.19473266601562\n",
      "Finished training\n",
      "FINISHED 3 1 16.850507736206055\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870])\n",
      "epoch 0 loss=256.7093505859375\n",
      "New best validation epoch 0 loss=11.804818153381348\n",
      "epoch 200 loss=251.8696746826172\n",
      "New best validation epoch 200 loss=11.698802947998047\n",
      "epoch 400 loss=238.72434997558594\n",
      "New best validation epoch 400 loss=11.414901733398438\n",
      "epoch 600 loss=222.55596923828125\n",
      "New best validation epoch 600 loss=11.163755416870117\n",
      "epoch 800 loss=220.8887176513672\n",
      "New best validation epoch 800 loss=11.05184555053711\n",
      "epoch 1000 loss=221.23240661621094\n",
      "New best validation epoch 1000 loss=11.01434326171875\n",
      "epoch 1200 loss=230.19821166992188\n",
      "New best validation epoch 1200 loss=11.009674072265625\n",
      "epoch 1400 loss=224.00833129882812\n",
      "New best validation epoch 1400 loss=11.004068374633789\n",
      "epoch 1600 loss=216.3070526123047\n",
      "epoch 1800 loss=221.6893768310547\n",
      "New best validation epoch 1800 loss=11.003822326660156\n",
      "epoch 2000 loss=214.21627807617188\n",
      "epoch 2200 loss=222.95632934570312\n",
      "New best validation epoch 2200 loss=11.003464698791504\n",
      "epoch 2400 loss=218.40841674804688\n",
      "New best validation epoch 2400 loss=11.000377655029297\n",
      "epoch 2600 loss=217.53269958496094\n",
      "epoch 2800 loss=214.26919555664062\n",
      "epoch 3000 loss=220.53326416015625\n",
      "New best validation epoch 3000 loss=10.99489688873291\n",
      "epoch 3200 loss=210.23062133789062\n",
      "New best validation epoch 3200 loss=10.994606971740723\n",
      "epoch 3400 loss=225.72305297851562\n",
      "New best validation epoch 3400 loss=10.989248275756836\n",
      "epoch 3600 loss=218.28953552246094\n",
      "epoch 3800 loss=227.5233154296875\n",
      "epoch 4000 loss=218.78829956054688\n",
      "epoch 4200 loss=216.9384002685547\n",
      "epoch 4400 loss=225.30999755859375\n",
      "epoch 4600 loss=225.1027069091797\n",
      "epoch 4800 loss=226.29135131835938\n",
      "epoch 4999 loss=224.28147888183594\n",
      "Finished training\n",
      "FINISHED 3 2 16.40840721130371\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952])\n",
      "epoch 0 loss=256.736572265625\n",
      "New best validation epoch 0 loss=11.804815292358398\n",
      "epoch 200 loss=247.2029266357422\n",
      "New best validation epoch 200 loss=11.609705924987793\n",
      "epoch 400 loss=227.71299743652344\n",
      "New best validation epoch 400 loss=11.152843475341797\n",
      "epoch 600 loss=218.8121337890625\n",
      "New best validation epoch 600 loss=10.860272407531738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 800 loss=219.06005859375\n",
      "New best validation epoch 800 loss=10.776251792907715\n",
      "epoch 1000 loss=207.61941528320312\n",
      "New best validation epoch 1000 loss=10.751704216003418\n",
      "epoch 1200 loss=210.15982055664062\n",
      "New best validation epoch 1200 loss=10.742693901062012\n",
      "epoch 1400 loss=214.041015625\n",
      "New best validation epoch 1400 loss=10.734418869018555\n",
      "epoch 1600 loss=217.41734313964844\n",
      "New best validation epoch 1600 loss=10.730623245239258\n",
      "epoch 1800 loss=217.5375213623047\n",
      "New best validation epoch 1800 loss=10.718969345092773\n",
      "epoch 2000 loss=216.36880493164062\n",
      "epoch 2200 loss=209.51083374023438\n",
      "epoch 2400 loss=211.32101440429688\n",
      "epoch 2600 loss=215.1220703125\n",
      "epoch 2800 loss=207.04293823242188\n",
      "epoch 3000 loss=211.3331298828125\n",
      "epoch 3200 loss=210.1439208984375\n",
      "epoch 3400 loss=207.26693725585938\n",
      "epoch 3600 loss=206.73629760742188\n",
      "New best validation epoch 3600 loss=10.712224960327148\n",
      "epoch 3800 loss=206.56309509277344\n",
      "epoch 4000 loss=202.65048217773438\n",
      "epoch 4200 loss=204.75906372070312\n",
      "epoch 4400 loss=207.6485595703125\n",
      "epoch 4600 loss=207.53683471679688\n",
      "epoch 4800 loss=203.56777954101562\n",
      "New best validation epoch 4800 loss=10.711051940917969\n",
      "epoch 4999 loss=211.9107666015625\n",
      "New best validation epoch 4999 loss=10.710993766784668\n",
      "Finished training\n",
      "FINISHED 3 3 16.205360412597656\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285])\n",
      "epoch 0 loss=257.29010009765625\n",
      "New best validation epoch 0 loss=11.804815292358398\n",
      "epoch 200 loss=243.7398681640625\n",
      "New best validation epoch 200 loss=11.539406776428223\n",
      "epoch 400 loss=221.52853393554688\n",
      "New best validation epoch 400 loss=11.029441833496094\n",
      "epoch 600 loss=212.7699432373047\n",
      "New best validation epoch 600 loss=10.843549728393555\n",
      "epoch 800 loss=207.8426513671875\n",
      "New best validation epoch 800 loss=10.7947998046875\n",
      "epoch 1000 loss=213.39682006835938\n",
      "New best validation epoch 1000 loss=10.786959648132324\n",
      "epoch 1200 loss=206.45030212402344\n",
      "New best validation epoch 1200 loss=10.780559539794922\n",
      "epoch 1400 loss=202.6226806640625\n",
      "New best validation epoch 1400 loss=10.776859283447266\n",
      "epoch 1600 loss=212.8897705078125\n",
      "New best validation epoch 1600 loss=10.773877143859863\n",
      "epoch 1800 loss=204.41964721679688\n",
      "New best validation epoch 1800 loss=10.770675659179688\n",
      "epoch 2000 loss=201.01748657226562\n",
      "epoch 2200 loss=208.6268310546875\n",
      "epoch 2400 loss=201.02975463867188\n",
      "epoch 2600 loss=196.64254760742188\n",
      "New best validation epoch 2600 loss=10.767107963562012\n",
      "epoch 2800 loss=200.67431640625\n",
      "epoch 3000 loss=201.77444458007812\n",
      "New best validation epoch 3000 loss=10.760549545288086\n",
      "epoch 3200 loss=202.96200561523438\n",
      "epoch 3400 loss=203.2571258544922\n",
      "epoch 3600 loss=208.70114135742188\n",
      "epoch 3800 loss=204.5690155029297\n",
      "epoch 4000 loss=202.63302612304688\n",
      "epoch 4200 loss=204.34786987304688\n",
      "epoch 4400 loss=203.43887329101562\n",
      "epoch 4600 loss=204.99627685546875\n",
      "epoch 4800 loss=209.6536865234375\n",
      "epoch 4999 loss=196.20018005371094\n",
      "Finished training\n",
      "FINISHED 3 4 16.301700592041016\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299, 27838,  6092,  9101,  7816, 23450])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285, 15797, 13717, 10037,  7697,  3142])\n",
      "epoch 0 loss=256.7725830078125\n",
      "New best validation epoch 0 loss=11.804816246032715\n",
      "epoch 200 loss=236.7190399169922\n",
      "New best validation epoch 200 loss=11.51539134979248\n",
      "epoch 400 loss=207.92564392089844\n",
      "New best validation epoch 400 loss=11.022342681884766\n",
      "epoch 600 loss=201.7137908935547\n",
      "New best validation epoch 600 loss=10.891451835632324\n",
      "epoch 800 loss=200.94723510742188\n",
      "New best validation epoch 800 loss=10.88271713256836\n",
      "epoch 1000 loss=183.84471130371094\n",
      "New best validation epoch 1000 loss=10.881103515625\n",
      "epoch 1200 loss=195.00582885742188\n",
      "New best validation epoch 1200 loss=10.870484352111816\n",
      "epoch 1400 loss=200.7637939453125\n",
      "New best validation epoch 1400 loss=10.866744995117188\n",
      "epoch 1600 loss=201.8776092529297\n",
      "epoch 1800 loss=196.4105682373047\n",
      "epoch 2000 loss=197.20201110839844\n",
      "New best validation epoch 2000 loss=10.865184783935547\n",
      "epoch 2200 loss=199.7185516357422\n",
      "New best validation epoch 2200 loss=10.863226890563965\n",
      "epoch 2400 loss=203.99417114257812\n",
      "New best validation epoch 2400 loss=10.860858917236328\n",
      "epoch 2600 loss=193.69564819335938\n",
      "epoch 2800 loss=195.6755828857422\n",
      "New best validation epoch 2800 loss=10.856646537780762\n",
      "epoch 3000 loss=190.84503173828125\n",
      "New best validation epoch 3000 loss=10.854951858520508\n",
      "epoch 3200 loss=195.4091796875\n",
      "New best validation epoch 3200 loss=10.854835510253906\n",
      "epoch 3400 loss=198.7972412109375\n",
      "epoch 3600 loss=202.42173767089844\n",
      "epoch 3800 loss=194.22915649414062\n",
      "New best validation epoch 3800 loss=10.851364135742188\n",
      "epoch 4000 loss=198.10006713867188\n",
      "epoch 4200 loss=195.47509765625\n",
      "epoch 4400 loss=215.05674743652344\n",
      "epoch 4600 loss=206.59295654296875\n",
      "epoch 4800 loss=193.72518920898438\n",
      "epoch 4999 loss=201.6279754638672\n",
      "Finished training\n",
      "FINISHED 3 5 16.208799362182617\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299, 27838,  6092,  9101,  7816, 23450,\n",
      "        17446, 22526,  6410, 28163, 17385])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285, 15797, 13717, 10037,  7697,  3142,\n",
      "        12314, 25500,  1638, 30905,  2701])\n",
      "epoch 0 loss=256.7623291015625\n",
      "New best validation epoch 0 loss=11.804814338684082\n",
      "epoch 200 loss=234.29051208496094\n",
      "New best validation epoch 200 loss=11.488059043884277\n",
      "epoch 400 loss=205.0179901123047\n",
      "New best validation epoch 400 loss=10.983034133911133\n",
      "epoch 600 loss=190.7952880859375\n",
      "New best validation epoch 600 loss=10.873406410217285\n",
      "epoch 800 loss=193.867431640625\n",
      "New best validation epoch 800 loss=10.865983009338379\n",
      "epoch 1000 loss=191.174072265625\n",
      "New best validation epoch 1000 loss=10.859506607055664\n",
      "epoch 1200 loss=187.5321044921875\n",
      "epoch 1400 loss=197.58029174804688\n",
      "New best validation epoch 1400 loss=10.855375289916992\n",
      "epoch 1600 loss=189.48309326171875\n",
      "New best validation epoch 1600 loss=10.855347633361816\n",
      "epoch 1800 loss=195.80950927734375\n",
      "New best validation epoch 1800 loss=10.85136604309082\n",
      "epoch 2000 loss=185.00421142578125\n",
      "epoch 2200 loss=192.550048828125\n",
      "epoch 2400 loss=186.42941284179688\n",
      "epoch 2600 loss=193.12191772460938\n",
      "epoch 2800 loss=191.902587890625\n",
      "epoch 3000 loss=199.89280700683594\n",
      "epoch 3200 loss=192.0719451904297\n",
      "epoch 3400 loss=189.1834716796875\n",
      "epoch 3600 loss=192.17697143554688\n",
      "epoch 3800 loss=187.88912963867188\n",
      "epoch 4000 loss=203.148193359375\n",
      "epoch 4200 loss=205.38766479492188\n",
      "epoch 4400 loss=186.2509765625\n",
      "epoch 4600 loss=191.655029296875\n",
      "epoch 4800 loss=200.1464080810547\n",
      "epoch 4999 loss=196.3695526123047\n",
      "New best validation epoch 4999 loss=10.848454475402832\n",
      "Finished training\n",
      "FINISHED 3 6 16.188814163208008\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299, 27838,  6092,  9101,  7816, 23450,\n",
      "        17446, 22526,  6410, 28163, 17385, 20378, 11512,  8385, 17885, 16752])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285, 15797, 13717, 10037,  7697,  3142,\n",
      "        12314, 25500,  1638, 30905,  2701, 30371, 31558, 29098, 27825, 11027])\n",
      "epoch 0 loss=256.3667907714844\n",
      "New best validation epoch 0 loss=11.804811477661133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=231.3610076904297\n",
      "New best validation epoch 200 loss=11.429043769836426\n",
      "epoch 400 loss=190.2110595703125\n",
      "New best validation epoch 400 loss=10.91226863861084\n",
      "epoch 600 loss=189.37863159179688\n",
      "New best validation epoch 600 loss=10.838464736938477\n",
      "epoch 800 loss=189.7554168701172\n",
      "epoch 1000 loss=182.03671264648438\n",
      "epoch 1200 loss=185.86865234375\n",
      "epoch 1400 loss=179.111083984375\n",
      "epoch 1600 loss=186.63894653320312\n",
      "epoch 1800 loss=189.17105102539062\n",
      "epoch 2000 loss=189.61309814453125\n",
      "epoch 2200 loss=190.5345916748047\n",
      "epoch 2400 loss=193.54583740234375\n",
      "epoch 2600 loss=180.51419067382812\n",
      "epoch 2800 loss=181.30075073242188\n",
      "epoch 3000 loss=181.63497924804688\n",
      "epoch 3200 loss=179.48716735839844\n",
      "epoch 3400 loss=178.7379608154297\n",
      "epoch 3600 loss=180.50042724609375\n",
      "epoch 3800 loss=181.01846313476562\n",
      "epoch 4000 loss=184.38031005859375\n",
      "epoch 4200 loss=185.56509399414062\n",
      "epoch 4400 loss=183.366943359375\n",
      "epoch 4600 loss=196.69180297851562\n",
      "epoch 4800 loss=187.611083984375\n",
      "epoch 4999 loss=191.8363494873047\n",
      "Finished training\n",
      "FINISHED 3 7 16.249359130859375\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299, 27838,  6092,  9101,  7816, 23450,\n",
      "        17446, 22526,  6410, 28163, 17385, 20378, 11512,  8385, 17885, 16752,\n",
      "        24720, 25501, 21109, 15910,   751])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285, 15797, 13717, 10037,  7697,  3142,\n",
      "        12314, 25500,  1638, 30905,  2701, 30371, 31558, 29098, 27825, 11027,\n",
      "         1258, 18659, 17449, 31728, 24205])\n",
      "epoch 0 loss=256.3201904296875\n",
      "New best validation epoch 0 loss=11.804811477661133\n",
      "epoch 200 loss=228.0635986328125\n",
      "New best validation epoch 200 loss=11.34218978881836\n",
      "epoch 400 loss=183.213134765625\n",
      "New best validation epoch 400 loss=10.905879020690918\n",
      "epoch 600 loss=182.0214080810547\n",
      "epoch 800 loss=176.69876098632812\n",
      "epoch 1000 loss=180.45123291015625\n",
      "epoch 1200 loss=185.044189453125\n",
      "epoch 1400 loss=185.31686401367188\n",
      "epoch 1600 loss=179.35801696777344\n",
      "epoch 1800 loss=175.5052490234375\n",
      "epoch 2000 loss=183.2750701904297\n",
      "epoch 2200 loss=180.3954315185547\n",
      "epoch 2400 loss=179.67715454101562\n",
      "epoch 2600 loss=184.67425537109375\n",
      "epoch 2800 loss=199.68063354492188\n",
      "epoch 3000 loss=178.5665283203125\n",
      "epoch 3200 loss=173.67938232421875\n",
      "epoch 3400 loss=188.1406707763672\n",
      "epoch 3600 loss=184.523193359375\n",
      "epoch 3800 loss=181.76934814453125\n",
      "epoch 4000 loss=187.63632202148438\n",
      "epoch 4200 loss=181.6470947265625\n",
      "epoch 4400 loss=184.49302673339844\n",
      "epoch 4600 loss=174.71063232421875\n",
      "epoch 4800 loss=182.34295654296875\n",
      "epoch 4999 loss=192.28916931152344\n",
      "Finished training\n",
      "FINISHED 3 8 16.275693893432617\n",
      "tensor([23469, 22006, 24276,  6715, 23702, 22141, 25121, 18231, 19220, 25111,\n",
      "        10869, 10626, 11024, 11027, 10925, 24205, 30439, 20888, 19834, 21858,\n",
      "        24404, 33940,   500, 28525, 14299, 27838,  6092,  9101,  7816, 23450,\n",
      "        17446, 22526,  6410, 28163, 17385, 20378, 11512,  8385, 17885, 16752,\n",
      "        24720, 25501, 21109, 15910,   751, 15064,  2929,  2666, 26631, 25986])\n",
      "tensor([ 1160, 25274,  2162, 10626, 22006, 13803, 29356, 29715, 23237, 34483,\n",
      "        28904,  4973, 28886, 12650, 12870, 21103, 30892, 11046, 31318, 29952,\n",
      "        29862, 31147,   139, 28716,  6285, 15797, 13717, 10037,  7697,  3142,\n",
      "        12314, 25500,  1638, 30905,  2701, 30371, 31558, 29098, 27825, 11027,\n",
      "         1258, 18659, 17449, 31728, 24205,  6608, 11252, 18227, 27870, 30032])\n",
      "epoch 0 loss=256.3935241699219\n",
      "New best validation epoch 0 loss=11.804810523986816\n",
      "epoch 200 loss=226.87742614746094\n",
      "New best validation epoch 200 loss=11.313393592834473\n",
      "epoch 400 loss=189.01461791992188\n",
      "New best validation epoch 400 loss=10.89545726776123\n",
      "epoch 600 loss=175.34335327148438\n",
      "epoch 800 loss=180.42926025390625\n",
      "epoch 1000 loss=178.0840301513672\n",
      "epoch 1200 loss=176.7266082763672\n",
      "epoch 1400 loss=170.3225555419922\n",
      "epoch 1600 loss=178.21188354492188\n",
      "epoch 1800 loss=169.10910034179688\n",
      "epoch 2000 loss=178.70913696289062\n",
      "epoch 2200 loss=180.1448974609375\n",
      "epoch 2400 loss=174.84544372558594\n",
      "epoch 2600 loss=180.0049285888672\n",
      "epoch 2800 loss=173.20339965820312\n",
      "epoch 3000 loss=173.272216796875\n",
      "epoch 3200 loss=186.86607360839844\n",
      "epoch 3400 loss=179.46536254882812\n",
      "epoch 3600 loss=175.82598876953125\n",
      "epoch 3800 loss=174.911376953125\n",
      "epoch 4000 loss=184.7054443359375\n",
      "epoch 4200 loss=179.6551513671875\n",
      "epoch 4400 loss=171.08126831054688\n",
      "epoch 4600 loss=175.47811889648438\n",
      "epoch 4800 loss=176.1324920654297\n",
      "epoch 4999 loss=177.2415008544922\n",
      "Finished training\n",
      "FINISHED 3 9 16.137422561645508\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709])\n",
      "tensor([10626,  1160, 24294,  2162,  1167])\n",
      "epoch 0 loss=265.37908935546875\n",
      "New best validation epoch 0 loss=11.583380699157715\n",
      "epoch 200 loss=265.3026123046875\n",
      "New best validation epoch 200 loss=11.565217018127441\n",
      "epoch 400 loss=261.94854736328125\n",
      "New best validation epoch 400 loss=11.50719165802002\n",
      "epoch 600 loss=256.1239929199219\n",
      "New best validation epoch 600 loss=11.419896125793457\n",
      "epoch 800 loss=256.18817138671875\n",
      "New best validation epoch 800 loss=11.342240333557129\n",
      "epoch 1000 loss=259.75543212890625\n",
      "New best validation epoch 1000 loss=11.308579444885254\n",
      "epoch 1200 loss=256.5693359375\n",
      "New best validation epoch 1200 loss=11.290961265563965\n",
      "epoch 1400 loss=250.8041229248047\n",
      "New best validation epoch 1400 loss=11.286343574523926\n",
      "epoch 1600 loss=253.08566284179688\n",
      "New best validation epoch 1600 loss=11.281878471374512\n",
      "epoch 1800 loss=249.89181518554688\n",
      "epoch 2000 loss=248.33306884765625\n",
      "epoch 2200 loss=247.35302734375\n",
      "epoch 2400 loss=251.39056396484375\n",
      "epoch 2600 loss=252.08935546875\n",
      "New best validation epoch 2600 loss=11.280607223510742\n",
      "epoch 2800 loss=257.4913635253906\n",
      "New best validation epoch 2800 loss=11.279580116271973\n",
      "epoch 3000 loss=252.52984619140625\n",
      "epoch 3200 loss=256.42333984375\n",
      "epoch 3400 loss=252.89248657226562\n",
      "epoch 3600 loss=256.6459045410156\n",
      "epoch 3800 loss=254.8446502685547\n",
      "epoch 4000 loss=251.86935424804688\n",
      "epoch 4200 loss=253.2411346435547\n",
      "epoch 4400 loss=254.626953125\n",
      "epoch 4600 loss=253.99998474121094\n",
      "epoch 4800 loss=252.87127685546875\n",
      "epoch 4999 loss=252.72915649414062\n",
      "Finished training\n",
      "FINISHED 4 0 14.647382736206055\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192])\n",
      "epoch 0 loss=266.33721923828125\n",
      "New best validation epoch 0 loss=11.583380699157715\n",
      "epoch 200 loss=262.32000732421875\n",
      "New best validation epoch 200 loss=11.563931465148926\n",
      "epoch 400 loss=253.393798828125\n",
      "New best validation epoch 400 loss=11.510441780090332\n",
      "epoch 600 loss=245.2629852294922\n",
      "New best validation epoch 600 loss=11.461134910583496\n",
      "epoch 800 loss=237.364990234375\n",
      "New best validation epoch 800 loss=11.434601783752441\n",
      "epoch 1000 loss=232.78158569335938\n",
      "New best validation epoch 1000 loss=11.416069984436035\n",
      "epoch 1200 loss=241.980224609375\n",
      "New best validation epoch 1200 loss=11.40363883972168\n",
      "epoch 1400 loss=235.351318359375\n",
      "New best validation epoch 1400 loss=11.399333953857422\n",
      "epoch 1600 loss=233.2489013671875\n",
      "New best validation epoch 1600 loss=11.393771171569824\n",
      "epoch 1800 loss=237.28204345703125\n",
      "New best validation epoch 1800 loss=11.387735366821289\n",
      "epoch 2000 loss=232.12432861328125\n",
      "New best validation epoch 2000 loss=11.382417678833008\n",
      "epoch 2200 loss=236.826171875\n",
      "New best validation epoch 2200 loss=11.381837844848633\n",
      "epoch 2400 loss=233.753662109375\n",
      "New best validation epoch 2400 loss=11.37856388092041\n",
      "epoch 2600 loss=239.34689331054688\n",
      "New best validation epoch 2600 loss=11.37791919708252\n",
      "epoch 2800 loss=233.5940399169922\n",
      "New best validation epoch 2800 loss=11.377089500427246\n",
      "epoch 3000 loss=238.02911376953125\n",
      "New best validation epoch 3000 loss=11.375543594360352\n",
      "epoch 3200 loss=238.9496307373047\n",
      "New best validation epoch 3200 loss=11.374689102172852\n",
      "epoch 3400 loss=241.4706268310547\n",
      "epoch 3600 loss=237.9544677734375\n",
      "New best validation epoch 3600 loss=11.372452735900879\n",
      "epoch 3800 loss=234.37274169921875\n",
      "epoch 4000 loss=243.45196533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4200 loss=237.5430908203125\n",
      "New best validation epoch 4200 loss=11.3722562789917\n",
      "epoch 4400 loss=234.97021484375\n",
      "epoch 4600 loss=240.3195037841797\n",
      "epoch 4800 loss=238.3922576904297\n",
      "epoch 4999 loss=234.9287109375\n",
      "Finished training\n",
      "FINISHED 4 1 14.433076858520508\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046])\n",
      "epoch 0 loss=266.3573303222656\n",
      "New best validation epoch 0 loss=11.583380699157715\n",
      "epoch 200 loss=257.752197265625\n",
      "New best validation epoch 200 loss=11.482950210571289\n",
      "epoch 400 loss=243.97360229492188\n",
      "New best validation epoch 400 loss=11.22817611694336\n",
      "epoch 600 loss=227.2648162841797\n",
      "New best validation epoch 600 loss=11.029740333557129\n",
      "epoch 800 loss=224.8022918701172\n",
      "New best validation epoch 800 loss=10.957240104675293\n",
      "epoch 1000 loss=223.3096160888672\n",
      "New best validation epoch 1000 loss=10.939766883850098\n",
      "epoch 1200 loss=224.75875854492188\n",
      "New best validation epoch 1200 loss=10.937565803527832\n",
      "epoch 1400 loss=228.7770233154297\n",
      "New best validation epoch 1400 loss=10.92430591583252\n",
      "epoch 1600 loss=225.4681854248047\n",
      "epoch 1800 loss=217.05747985839844\n",
      "epoch 2000 loss=218.1490936279297\n",
      "epoch 2200 loss=226.82339477539062\n",
      "epoch 2400 loss=226.0734100341797\n",
      "epoch 2600 loss=223.18276977539062\n",
      "epoch 2800 loss=226.4871826171875\n",
      "New best validation epoch 2800 loss=10.919225692749023\n",
      "epoch 3000 loss=226.75621032714844\n",
      "epoch 3200 loss=219.13973999023438\n",
      "epoch 3400 loss=222.3075408935547\n",
      "epoch 3600 loss=228.52935791015625\n",
      "epoch 3800 loss=222.0638885498047\n",
      "epoch 4000 loss=217.7117919921875\n",
      "epoch 4200 loss=226.15782165527344\n",
      "epoch 4400 loss=230.54452514648438\n",
      "epoch 4600 loss=224.32843017578125\n",
      "epoch 4800 loss=223.74026489257812\n",
      "epoch 4999 loss=224.26809692382812\n",
      "Finished training\n",
      "FINISHED 4 2 14.588116645812988\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717])\n",
      "epoch 0 loss=266.614501953125\n",
      "New best validation epoch 0 loss=11.583378791809082\n",
      "epoch 200 loss=255.5123291015625\n",
      "New best validation epoch 200 loss=11.452859878540039\n",
      "epoch 400 loss=231.57247924804688\n",
      "New best validation epoch 400 loss=11.141308784484863\n",
      "epoch 600 loss=225.8614501953125\n",
      "New best validation epoch 600 loss=10.928619384765625\n",
      "epoch 800 loss=218.87680053710938\n",
      "New best validation epoch 800 loss=10.842713356018066\n",
      "epoch 1000 loss=219.7881317138672\n",
      "New best validation epoch 1000 loss=10.808079719543457\n",
      "epoch 1200 loss=219.5507049560547\n",
      "New best validation epoch 1200 loss=10.796099662780762\n",
      "epoch 1400 loss=221.67100524902344\n",
      "New best validation epoch 1400 loss=10.783307075500488\n",
      "epoch 1600 loss=209.2734375\n",
      "epoch 1800 loss=218.36154174804688\n",
      "epoch 2000 loss=215.61367797851562\n",
      "New best validation epoch 2000 loss=10.771889686584473\n",
      "epoch 2200 loss=211.92352294921875\n",
      "epoch 2400 loss=219.2416229248047\n",
      "epoch 2600 loss=209.0087890625\n",
      "epoch 2800 loss=218.36990356445312\n",
      "epoch 3000 loss=223.21170043945312\n",
      "epoch 3200 loss=219.3614501953125\n",
      "epoch 3400 loss=215.424072265625\n",
      "epoch 3600 loss=221.8938446044922\n",
      "epoch 3800 loss=213.43930053710938\n",
      "epoch 4000 loss=204.5643310546875\n",
      "epoch 4200 loss=216.6783905029297\n",
      "epoch 4400 loss=204.11456298828125\n",
      "epoch 4600 loss=216.47906494140625\n",
      "New best validation epoch 4600 loss=10.76923656463623\n",
      "epoch 4800 loss=208.37176513671875\n",
      "epoch 4999 loss=211.7429656982422\n",
      "Finished training\n",
      "FINISHED 4 3 14.596542358398438\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301])\n",
      "epoch 0 loss=266.35968017578125\n",
      "New best validation epoch 0 loss=11.583375930786133\n",
      "epoch 200 loss=249.9606170654297\n",
      "New best validation epoch 200 loss=11.375473976135254\n",
      "epoch 400 loss=222.72140502929688\n",
      "New best validation epoch 400 loss=10.899657249450684\n",
      "epoch 600 loss=216.1243133544922\n",
      "New best validation epoch 600 loss=10.644624710083008\n",
      "epoch 800 loss=207.35134887695312\n",
      "New best validation epoch 800 loss=10.584583282470703\n",
      "epoch 1000 loss=214.423828125\n",
      "New best validation epoch 1000 loss=10.562637329101562\n",
      "epoch 1200 loss=202.40573120117188\n",
      "epoch 1400 loss=209.6156463623047\n",
      "New best validation epoch 1400 loss=10.54588794708252\n",
      "epoch 1600 loss=214.590087890625\n",
      "New best validation epoch 1600 loss=10.542232513427734\n",
      "epoch 1800 loss=211.78256225585938\n",
      "epoch 2000 loss=219.0482635498047\n",
      "epoch 2200 loss=206.1924285888672\n",
      "epoch 2400 loss=209.12327575683594\n",
      "epoch 2600 loss=211.52220153808594\n",
      "epoch 2800 loss=207.4068603515625\n",
      "epoch 3000 loss=209.9526824951172\n",
      "epoch 3200 loss=203.615966796875\n",
      "epoch 3400 loss=208.3367156982422\n",
      "epoch 3600 loss=211.14236450195312\n",
      "epoch 3800 loss=209.23287963867188\n",
      "epoch 4000 loss=204.82833862304688\n",
      "epoch 4200 loss=201.00732421875\n",
      "epoch 4400 loss=211.10256958007812\n",
      "epoch 4600 loss=208.49267578125\n",
      "epoch 4800 loss=212.68466186523438\n",
      "epoch 4999 loss=211.24209594726562\n",
      "Finished training\n",
      "FINISHED 4 4 14.597197532653809\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078, 30129, 22798, 20620,   368, 20641])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301, 13713, 27986, 18835, 21106, 30191])\n",
      "epoch 0 loss=266.03631591796875\n",
      "New best validation epoch 0 loss=11.583375930786133\n",
      "epoch 200 loss=250.02328491210938\n",
      "New best validation epoch 200 loss=11.306367874145508\n",
      "epoch 400 loss=214.80233764648438\n",
      "New best validation epoch 400 loss=10.785198211669922\n",
      "epoch 600 loss=208.14805603027344\n",
      "New best validation epoch 600 loss=10.581451416015625\n",
      "epoch 800 loss=208.30186462402344\n",
      "New best validation epoch 800 loss=10.555975914001465\n",
      "epoch 1000 loss=207.4418182373047\n",
      "epoch 1200 loss=198.92263793945312\n",
      "epoch 1400 loss=193.29443359375\n",
      "epoch 1600 loss=200.33935546875\n",
      "epoch 1800 loss=207.36544799804688\n",
      "epoch 2000 loss=204.5695037841797\n",
      "epoch 2200 loss=190.5238494873047\n",
      "epoch 2400 loss=207.5360565185547\n",
      "epoch 2600 loss=201.4823455810547\n",
      "epoch 2800 loss=200.29489135742188\n",
      "epoch 3000 loss=201.51666259765625\n",
      "epoch 3200 loss=204.8624725341797\n",
      "epoch 3400 loss=200.2311248779297\n",
      "epoch 3600 loss=209.82481384277344\n",
      "epoch 3800 loss=208.07864379882812\n",
      "epoch 4000 loss=200.17575073242188\n",
      "epoch 4200 loss=201.32366943359375\n",
      "epoch 4400 loss=199.5990447998047\n",
      "epoch 4600 loss=202.95211791992188\n",
      "epoch 4800 loss=196.02676391601562\n",
      "epoch 4999 loss=199.59835815429688\n",
      "Finished training\n",
      "FINISHED 4 5 14.589829444885254\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078, 30129, 22798, 20620,   368, 20641,\n",
      "         9606,  7816, 25928, 26475, 11685])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301, 13713, 27986, 18835, 21106, 30191,\n",
      "        22286, 28905, 20619, 13286, 28716])\n",
      "epoch 0 loss=266.2923583984375\n",
      "New best validation epoch 0 loss=11.5833740234375\n",
      "epoch 200 loss=243.18124389648438\n",
      "New best validation epoch 200 loss=11.278538703918457\n",
      "epoch 400 loss=204.44235229492188\n",
      "New best validation epoch 400 loss=10.8384428024292\n",
      "epoch 600 loss=190.37107849121094\n",
      "New best validation epoch 600 loss=10.78124713897705\n",
      "epoch 800 loss=197.46226501464844\n",
      "epoch 1000 loss=196.68536376953125\n",
      "epoch 1200 loss=205.58653259277344\n",
      "epoch 1400 loss=194.4388885498047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1600 loss=204.48287963867188\n",
      "epoch 1800 loss=190.47250366210938\n",
      "epoch 2000 loss=195.0983123779297\n",
      "epoch 2200 loss=189.88900756835938\n",
      "epoch 2400 loss=185.84500122070312\n",
      "epoch 2600 loss=188.05453491210938\n",
      "epoch 2800 loss=185.10067749023438\n",
      "epoch 3000 loss=197.3973846435547\n",
      "epoch 3200 loss=185.72134399414062\n",
      "epoch 3400 loss=192.072998046875\n",
      "epoch 3600 loss=196.53250122070312\n",
      "epoch 3800 loss=187.4012451171875\n",
      "epoch 4000 loss=189.4097900390625\n",
      "epoch 4200 loss=188.6112060546875\n",
      "epoch 4400 loss=205.64810180664062\n",
      "epoch 4600 loss=206.7259979248047\n",
      "epoch 4800 loss=193.41073608398438\n",
      "epoch 4999 loss=184.6665496826172\n",
      "Finished training\n",
      "FINISHED 4 6 14.61138916015625\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078, 30129, 22798, 20620,   368, 20641,\n",
      "         9606,  7816, 25928, 26475, 11685, 11923, 10869, 24493, 10885,  3300])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301, 13713, 27986, 18835, 21106, 30191,\n",
      "        22286, 28905, 20619, 13286, 28716,  5407,  4874,  7107, 11300, 10206])\n",
      "epoch 0 loss=265.8670654296875\n",
      "New best validation epoch 0 loss=11.5833740234375\n",
      "epoch 200 loss=232.40658569335938\n",
      "New best validation epoch 200 loss=11.23782730102539\n",
      "epoch 400 loss=201.51531982421875\n",
      "New best validation epoch 400 loss=10.880006790161133\n",
      "epoch 600 loss=177.59275817871094\n",
      "epoch 800 loss=190.93380737304688\n",
      "epoch 1000 loss=183.3599853515625\n",
      "epoch 1200 loss=193.17007446289062\n",
      "epoch 1400 loss=184.76608276367188\n",
      "epoch 1600 loss=193.5943603515625\n",
      "epoch 1800 loss=190.7755584716797\n",
      "epoch 2000 loss=188.83782958984375\n",
      "epoch 2200 loss=185.49490356445312\n",
      "epoch 2400 loss=186.20553588867188\n",
      "epoch 2600 loss=189.59683227539062\n",
      "epoch 2800 loss=183.56146240234375\n",
      "epoch 3000 loss=190.15478515625\n",
      "epoch 3200 loss=189.26834106445312\n",
      "epoch 3400 loss=182.6610107421875\n",
      "epoch 3600 loss=184.64620971679688\n",
      "epoch 3800 loss=191.48934936523438\n",
      "epoch 4000 loss=195.2263641357422\n",
      "epoch 4200 loss=176.3496551513672\n",
      "epoch 4400 loss=193.0250244140625\n",
      "epoch 4600 loss=197.2040252685547\n",
      "epoch 4800 loss=178.94834899902344\n",
      "epoch 4999 loss=187.9290771484375\n",
      "Finished training\n",
      "FINISHED 4 7 14.521621704101562\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078, 30129, 22798, 20620,   368, 20641,\n",
      "         9606,  7816, 25928, 26475, 11685, 11923, 10869, 24493, 10885,  3300,\n",
      "        15282, 23960,  3848,  2568, 24473])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301, 13713, 27986, 18835, 21106, 30191,\n",
      "        22286, 28905, 20619, 13286, 28716,  5407,  4874,  7107, 11300, 10206,\n",
      "        20642, 31380, 12662,  5368,  7461])\n",
      "epoch 0 loss=266.6036376953125\n",
      "New best validation epoch 0 loss=11.5833740234375\n",
      "epoch 200 loss=233.2753143310547\n",
      "New best validation epoch 200 loss=11.161932945251465\n",
      "epoch 400 loss=197.1074676513672\n",
      "New best validation epoch 400 loss=10.842267036437988\n",
      "epoch 600 loss=192.13023376464844\n",
      "epoch 800 loss=178.26585388183594\n",
      "epoch 1000 loss=184.96832275390625\n",
      "epoch 1200 loss=188.73812866210938\n",
      "epoch 1400 loss=179.3560791015625\n",
      "epoch 1600 loss=173.67807006835938\n",
      "epoch 1800 loss=179.57614135742188\n",
      "epoch 2000 loss=188.37310791015625\n",
      "epoch 2200 loss=180.97552490234375\n",
      "epoch 2400 loss=178.83456420898438\n",
      "epoch 2600 loss=185.856689453125\n",
      "epoch 2800 loss=185.7933807373047\n",
      "epoch 3000 loss=184.39495849609375\n",
      "epoch 3200 loss=182.59963989257812\n",
      "epoch 3400 loss=185.854736328125\n",
      "epoch 3600 loss=178.69580078125\n",
      "epoch 3800 loss=176.75347900390625\n",
      "epoch 4000 loss=189.06700134277344\n",
      "epoch 4200 loss=188.3125457763672\n",
      "epoch 4400 loss=191.94883728027344\n",
      "epoch 4600 loss=182.6611785888672\n",
      "epoch 4800 loss=178.23486328125\n",
      "epoch 4999 loss=181.35031127929688\n",
      "Finished training\n",
      "FINISHED 4 8 14.690461158752441\n",
      "tensor([ 6715, 23469,  6712,  7162,  6709, 22141, 29047, 22779, 29070,   402,\n",
      "        21858, 19059, 11024, 18358, 18362, 23445,   172,  5613, 30352,  6803,\n",
      "         2150,  2316, 20022,  9102,  1078, 30129, 22798, 20620,   368, 20641,\n",
      "         9606,  7816, 25928, 26475, 11685, 11923, 10869, 24493, 10885,  3300,\n",
      "        15282, 23960,  3848,  2568, 24473, 20187,   108, 16759, 15800, 12979])\n",
      "tensor([10626,  1160, 24294,  2162,  1167, 26056, 21662, 17694, 20518,  3192,\n",
      "        21515, 11027,  9033,  9022, 11046, 14350, 30594, 17233, 31345,  9717,\n",
      "        13067, 12157, 12980, 18227, 15301, 13713, 27986, 18835, 21106, 30191,\n",
      "        22286, 28905, 20619, 13286, 28716,  5407,  4874,  7107, 11300, 10206,\n",
      "        20642, 31380, 12662,  5368,  7461, 19076, 24225, 24198, 23821,   329])\n",
      "epoch 0 loss=266.1054382324219\n",
      "New best validation epoch 0 loss=11.5833740234375\n",
      "epoch 200 loss=228.2085418701172\n",
      "New best validation epoch 200 loss=11.171426773071289\n",
      "epoch 400 loss=186.11618041992188\n",
      "New best validation epoch 400 loss=11.016454696655273\n",
      "epoch 600 loss=182.73875427246094\n",
      "epoch 800 loss=176.5819091796875\n",
      "epoch 1000 loss=181.1832733154297\n",
      "epoch 1200 loss=172.1133270263672\n",
      "epoch 1400 loss=180.28692626953125\n",
      "epoch 1600 loss=176.7144775390625\n",
      "epoch 1800 loss=184.35757446289062\n",
      "epoch 2000 loss=168.8321533203125\n",
      "epoch 2200 loss=175.58721923828125\n",
      "epoch 2400 loss=183.57904052734375\n",
      "epoch 2600 loss=182.41921997070312\n",
      "epoch 2800 loss=181.98971557617188\n",
      "epoch 3000 loss=176.04209899902344\n",
      "epoch 3200 loss=172.69970703125\n",
      "epoch 3400 loss=182.40777587890625\n",
      "epoch 3600 loss=177.29360961914062\n",
      "epoch 3800 loss=172.8146209716797\n",
      "epoch 4000 loss=181.55825805664062\n",
      "epoch 4200 loss=185.3292236328125\n",
      "epoch 4400 loss=180.2499237060547\n",
      "epoch 4600 loss=185.7543182373047\n",
      "epoch 4800 loss=181.282470703125\n",
      "epoch 4999 loss=174.7447509765625\n",
      "Finished training\n",
      "FINISHED 4 9 14.702917098999023\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006])\n",
      "tensor([18257, 24294,  9030, 10626,  2149])\n",
      "epoch 0 loss=260.13525390625\n",
      "New best validation epoch 0 loss=12.136263847351074\n",
      "epoch 200 loss=259.89129638671875\n",
      "New best validation epoch 200 loss=12.115839004516602\n",
      "epoch 400 loss=256.6256103515625\n",
      "New best validation epoch 400 loss=12.05046272277832\n",
      "epoch 600 loss=250.41250610351562\n",
      "New best validation epoch 600 loss=11.945624351501465\n",
      "epoch 800 loss=246.72457885742188\n",
      "New best validation epoch 800 loss=11.832864761352539\n",
      "epoch 1000 loss=246.34185791015625\n",
      "New best validation epoch 1000 loss=11.758355140686035\n",
      "epoch 1200 loss=245.29345703125\n",
      "New best validation epoch 1200 loss=11.717520713806152\n",
      "epoch 1400 loss=248.3046112060547\n",
      "New best validation epoch 1400 loss=11.690852165222168\n",
      "epoch 1600 loss=245.09298706054688\n",
      "New best validation epoch 1600 loss=11.683473587036133\n",
      "epoch 1800 loss=245.63589477539062\n",
      "New best validation epoch 1800 loss=11.681604385375977\n",
      "epoch 2000 loss=247.5718994140625\n",
      "New best validation epoch 2000 loss=11.674155235290527\n",
      "epoch 2200 loss=247.52508544921875\n",
      "New best validation epoch 2200 loss=11.672037124633789\n",
      "epoch 2400 loss=250.35507202148438\n",
      "New best validation epoch 2400 loss=11.667240142822266\n",
      "epoch 2600 loss=243.42294311523438\n",
      "New best validation epoch 2600 loss=11.665614128112793\n",
      "epoch 2800 loss=245.2867431640625\n",
      "New best validation epoch 2800 loss=11.662792205810547\n",
      "epoch 3000 loss=243.27182006835938\n",
      "epoch 3200 loss=240.68406677246094\n",
      "New best validation epoch 3200 loss=11.658790588378906\n",
      "epoch 3400 loss=243.6783447265625\n",
      "epoch 3600 loss=246.01123046875\n",
      "epoch 3800 loss=248.1849822998047\n",
      "epoch 4000 loss=242.1110076904297\n",
      "epoch 4200 loss=242.73025512695312\n",
      "epoch 4400 loss=249.4775848388672\n",
      "epoch 4600 loss=247.99668884277344\n",
      "epoch 4800 loss=242.99069213867188\n",
      "epoch 4999 loss=247.8673553466797\n",
      "Finished training\n",
      "FINISHED 5 0 15.615032196044922\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954])\n",
      "epoch 0 loss=260.5555114746094\n",
      "New best validation epoch 0 loss=12.136263847351074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=257.9056091308594\n",
      "New best validation epoch 200 loss=12.092960357666016\n",
      "epoch 400 loss=249.3880615234375\n",
      "New best validation epoch 400 loss=11.958664894104004\n",
      "epoch 600 loss=240.06918334960938\n",
      "New best validation epoch 600 loss=11.784468650817871\n",
      "epoch 800 loss=234.3607177734375\n",
      "New best validation epoch 800 loss=11.632088661193848\n",
      "epoch 1000 loss=229.1780242919922\n",
      "New best validation epoch 1000 loss=11.532560348510742\n",
      "epoch 1200 loss=230.40145874023438\n",
      "New best validation epoch 1200 loss=11.47875690460205\n",
      "epoch 1400 loss=219.19464111328125\n",
      "New best validation epoch 1400 loss=11.444382667541504\n",
      "epoch 1600 loss=222.24130249023438\n",
      "New best validation epoch 1600 loss=11.422389030456543\n",
      "epoch 1800 loss=224.5358123779297\n",
      "New best validation epoch 1800 loss=11.415264129638672\n",
      "epoch 2000 loss=228.70614624023438\n",
      "New best validation epoch 2000 loss=11.410571098327637\n",
      "epoch 2200 loss=235.125\n",
      "New best validation epoch 2200 loss=11.404376983642578\n",
      "epoch 2400 loss=232.44158935546875\n",
      "New best validation epoch 2400 loss=11.399904251098633\n",
      "epoch 2600 loss=226.7832489013672\n",
      "New best validation epoch 2600 loss=11.396848678588867\n",
      "epoch 2800 loss=228.30612182617188\n",
      "New best validation epoch 2800 loss=11.3944730758667\n",
      "epoch 3000 loss=231.52243041992188\n",
      "epoch 3200 loss=231.08395385742188\n",
      "epoch 3400 loss=225.41282653808594\n",
      "epoch 3600 loss=218.71263122558594\n",
      "epoch 3800 loss=237.93740844726562\n",
      "New best validation epoch 3800 loss=11.387226104736328\n",
      "epoch 4000 loss=238.25997924804688\n",
      "epoch 4200 loss=224.8106689453125\n",
      "epoch 4400 loss=233.1178436279297\n",
      "epoch 4600 loss=229.83673095703125\n",
      "epoch 4800 loss=229.7457275390625\n",
      "epoch 4999 loss=234.8629150390625\n",
      "Finished training\n",
      "FINISHED 5 1 15.59240436553955\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286])\n",
      "epoch 0 loss=260.01776123046875\n",
      "New best validation epoch 0 loss=12.136263847351074\n",
      "epoch 200 loss=254.70822143554688\n",
      "New best validation epoch 200 loss=12.046822547912598\n",
      "epoch 400 loss=237.687255859375\n",
      "New best validation epoch 400 loss=11.791254043579102\n",
      "epoch 600 loss=221.50576782226562\n",
      "New best validation epoch 600 loss=11.520185470581055\n",
      "epoch 800 loss=224.79135131835938\n",
      "New best validation epoch 800 loss=11.350345611572266\n",
      "epoch 1000 loss=217.9847412109375\n",
      "New best validation epoch 1000 loss=11.268590927124023\n",
      "epoch 1200 loss=213.7232666015625\n",
      "New best validation epoch 1200 loss=11.240008354187012\n",
      "epoch 1400 loss=219.6746063232422\n",
      "New best validation epoch 1400 loss=11.225646018981934\n",
      "epoch 1600 loss=217.0833740234375\n",
      "New best validation epoch 1600 loss=11.225547790527344\n",
      "epoch 1800 loss=229.41268920898438\n",
      "epoch 2000 loss=219.1919403076172\n",
      "epoch 2200 loss=219.23086547851562\n",
      "epoch 2400 loss=218.79733276367188\n",
      "epoch 2600 loss=211.3096160888672\n",
      "epoch 2800 loss=221.17202758789062\n",
      "epoch 3000 loss=222.7863006591797\n",
      "New best validation epoch 3000 loss=11.22531509399414\n",
      "epoch 3200 loss=216.8639373779297\n",
      "New best validation epoch 3200 loss=11.221203804016113\n",
      "epoch 3400 loss=219.3691864013672\n",
      "New best validation epoch 3400 loss=11.219785690307617\n",
      "epoch 3600 loss=218.72247314453125\n",
      "epoch 3800 loss=211.49859619140625\n",
      "epoch 4000 loss=215.84896850585938\n",
      "epoch 4200 loss=218.099609375\n",
      "epoch 4400 loss=218.04046630859375\n",
      "epoch 4600 loss=221.44259643554688\n",
      "epoch 4800 loss=218.10601806640625\n",
      "New best validation epoch 4800 loss=11.216716766357422\n",
      "epoch 4999 loss=214.8099365234375\n",
      "New best validation epoch 4999 loss=11.212950706481934\n",
      "Finished training\n",
      "FINISHED 5 2 15.640119552612305\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413])\n",
      "epoch 0 loss=260.8003845214844\n",
      "New best validation epoch 0 loss=12.136262893676758\n",
      "epoch 200 loss=251.14456176757812\n",
      "New best validation epoch 200 loss=12.036246299743652\n",
      "epoch 400 loss=236.73435974121094\n",
      "New best validation epoch 400 loss=11.766427040100098\n",
      "epoch 600 loss=229.24270629882812\n",
      "New best validation epoch 600 loss=11.502927780151367\n",
      "epoch 800 loss=216.9073486328125\n",
      "New best validation epoch 800 loss=11.338764190673828\n",
      "epoch 1000 loss=210.67166137695312\n",
      "New best validation epoch 1000 loss=11.261874198913574\n",
      "epoch 1200 loss=210.38145446777344\n",
      "New best validation epoch 1200 loss=11.224311828613281\n",
      "epoch 1400 loss=216.4636688232422\n",
      "New best validation epoch 1400 loss=11.216520309448242\n",
      "epoch 1600 loss=216.0491943359375\n",
      "New best validation epoch 1600 loss=11.210546493530273\n",
      "epoch 1800 loss=203.23980712890625\n",
      "New best validation epoch 1800 loss=11.205060005187988\n",
      "epoch 2000 loss=209.13548278808594\n",
      "New best validation epoch 2000 loss=11.204068183898926\n",
      "epoch 2200 loss=212.9747772216797\n",
      "epoch 2400 loss=211.00413513183594\n",
      "New best validation epoch 2400 loss=11.202644348144531\n",
      "epoch 2600 loss=211.4251708984375\n",
      "epoch 2800 loss=209.2333221435547\n",
      "epoch 3000 loss=219.1747283935547\n",
      "New best validation epoch 3000 loss=11.201254844665527\n",
      "epoch 3200 loss=212.82920837402344\n",
      "epoch 3400 loss=209.9097137451172\n",
      "New best validation epoch 3400 loss=11.200602531433105\n",
      "epoch 3600 loss=210.35263061523438\n",
      "New best validation epoch 3600 loss=11.196903228759766\n",
      "epoch 3800 loss=218.32763671875\n",
      "epoch 4000 loss=213.91693115234375\n",
      "epoch 4200 loss=211.9258270263672\n",
      "epoch 4400 loss=205.02761840820312\n",
      "epoch 4600 loss=208.9005126953125\n",
      "epoch 4800 loss=211.4312744140625\n",
      "epoch 4999 loss=211.4139404296875\n",
      "Finished training\n",
      "FINISHED 5 3 15.585281372070312\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237])\n",
      "epoch 0 loss=260.727783203125\n",
      "New best validation epoch 0 loss=12.136263847351074\n",
      "epoch 200 loss=251.27752685546875\n",
      "New best validation epoch 200 loss=12.022675514221191\n",
      "epoch 400 loss=223.56033325195312\n",
      "New best validation epoch 400 loss=11.746725082397461\n",
      "epoch 600 loss=206.19284057617188\n",
      "New best validation epoch 600 loss=11.545214653015137\n",
      "epoch 800 loss=203.40692138671875\n",
      "New best validation epoch 800 loss=11.448241233825684\n",
      "epoch 1000 loss=208.3873748779297\n",
      "New best validation epoch 1000 loss=11.406129837036133\n",
      "epoch 1200 loss=212.12872314453125\n",
      "New best validation epoch 1200 loss=11.38520336151123\n",
      "epoch 1400 loss=199.52438354492188\n",
      "New best validation epoch 1400 loss=11.368077278137207\n",
      "epoch 1600 loss=202.06309509277344\n",
      "epoch 1800 loss=203.83335876464844\n",
      "New best validation epoch 1800 loss=11.35862922668457\n",
      "epoch 2000 loss=201.95086669921875\n",
      "New best validation epoch 2000 loss=11.352953910827637\n",
      "epoch 2200 loss=202.65985107421875\n",
      "epoch 2400 loss=213.10214233398438\n",
      "epoch 2600 loss=204.12197875976562\n",
      "epoch 2800 loss=199.6339111328125\n",
      "epoch 3000 loss=211.50482177734375\n",
      "epoch 3200 loss=198.7799530029297\n",
      "New best validation epoch 3200 loss=11.351110458374023\n",
      "epoch 3400 loss=208.31796264648438\n",
      "epoch 3600 loss=201.38714599609375\n",
      "epoch 3800 loss=207.2571258544922\n",
      "epoch 4000 loss=204.37939453125\n",
      "epoch 4200 loss=210.57017517089844\n",
      "epoch 4400 loss=200.54132080078125\n",
      "epoch 4600 loss=204.23828125\n",
      "epoch 4800 loss=217.62109375\n",
      "epoch 4999 loss=209.98367309570312\n",
      "Finished training\n",
      "FINISHED 5 4 15.482422828674316\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383, 26987,  1078,  5096,  6596,   758])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237, 14330, 11044, 20619, 25273, 20527])\n",
      "epoch 0 loss=261.1568603515625\n",
      "New best validation epoch 0 loss=12.136263847351074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=242.7034149169922\n",
      "New best validation epoch 200 loss=11.988259315490723\n",
      "epoch 400 loss=215.4158172607422\n",
      "New best validation epoch 400 loss=11.692062377929688\n",
      "epoch 600 loss=204.96493530273438\n",
      "New best validation epoch 600 loss=11.542983055114746\n",
      "epoch 800 loss=201.63864135742188\n",
      "New best validation epoch 800 loss=11.497720718383789\n",
      "epoch 1000 loss=199.37460327148438\n",
      "New best validation epoch 1000 loss=11.471295356750488\n",
      "epoch 1200 loss=204.4304656982422\n",
      "epoch 1400 loss=192.89646911621094\n",
      "New best validation epoch 1400 loss=11.46910285949707\n",
      "epoch 1600 loss=202.22537231445312\n",
      "New best validation epoch 1600 loss=11.46583366394043\n",
      "epoch 1800 loss=201.79330444335938\n",
      "New best validation epoch 1800 loss=11.462637901306152\n",
      "epoch 2000 loss=199.31072998046875\n",
      "epoch 2200 loss=213.23443603515625\n",
      "New best validation epoch 2200 loss=11.460257530212402\n",
      "epoch 2400 loss=192.2509765625\n",
      "New best validation epoch 2400 loss=11.45875358581543\n",
      "epoch 2600 loss=195.13687133789062\n",
      "epoch 2800 loss=206.71685791015625\n",
      "epoch 3000 loss=194.24273681640625\n",
      "epoch 3200 loss=199.84115600585938\n",
      "epoch 3400 loss=204.5123291015625\n",
      "epoch 3600 loss=200.2844696044922\n",
      "epoch 3800 loss=197.1370086669922\n",
      "epoch 4000 loss=197.91058349609375\n",
      "epoch 4200 loss=194.53375244140625\n",
      "epoch 4400 loss=205.01658630371094\n",
      "epoch 4600 loss=195.87930297851562\n",
      "epoch 4800 loss=194.0720672607422\n",
      "New best validation epoch 4800 loss=11.45654010772705\n",
      "epoch 4999 loss=205.1044921875\n",
      "Finished training\n",
      "FINISHED 5 5 15.575428009033203\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383, 26987,  1078,  5096,  6596,   758,\n",
      "        18771, 22612, 22779, 19831, 24294])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237, 14330, 11044, 20619, 25273, 20527,\n",
      "        15766,  7667, 12946, 12940, 12945])\n",
      "epoch 0 loss=260.06591796875\n",
      "New best validation epoch 0 loss=12.136260986328125\n",
      "epoch 200 loss=239.47567749023438\n",
      "New best validation epoch 200 loss=11.951558113098145\n",
      "epoch 400 loss=209.42173767089844\n",
      "New best validation epoch 400 loss=11.667557716369629\n",
      "epoch 600 loss=198.41221618652344\n",
      "New best validation epoch 600 loss=11.567789077758789\n",
      "epoch 800 loss=194.58154296875\n",
      "New best validation epoch 800 loss=11.547143936157227\n",
      "epoch 1000 loss=201.55368041992188\n",
      "New best validation epoch 1000 loss=11.541179656982422\n",
      "epoch 1200 loss=194.10128784179688\n",
      "epoch 1400 loss=199.07510375976562\n",
      "New best validation epoch 1400 loss=11.540184020996094\n",
      "epoch 1600 loss=194.06747436523438\n",
      "New best validation epoch 1600 loss=11.534302711486816\n",
      "epoch 1800 loss=198.774169921875\n",
      "epoch 2000 loss=194.09628295898438\n",
      "epoch 2200 loss=193.67465209960938\n",
      "epoch 2400 loss=197.00570678710938\n",
      "epoch 2600 loss=192.75608825683594\n",
      "epoch 2800 loss=195.95651245117188\n",
      "epoch 3000 loss=192.05001831054688\n",
      "epoch 3200 loss=188.10174560546875\n",
      "epoch 3400 loss=196.5703125\n",
      "epoch 3600 loss=187.91636657714844\n",
      "epoch 3800 loss=191.94412231445312\n",
      "epoch 4000 loss=193.15647888183594\n",
      "epoch 4200 loss=192.02398681640625\n",
      "epoch 4400 loss=193.11753845214844\n",
      "epoch 4600 loss=194.76123046875\n",
      "epoch 4800 loss=198.48980712890625\n",
      "epoch 4999 loss=197.71554565429688\n",
      "Finished training\n",
      "FINISHED 5 6 15.49554443359375\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383, 26987,  1078,  5096,  6596,   758,\n",
      "        18771, 22612, 22779, 19831, 24294, 31231, 17979, 24047, 25986,   991])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237, 14330, 11044, 20619, 25273, 20527,\n",
      "        15766,  7667, 12946, 12940, 12945,  1230,  3126,  6608,  1088,   308])\n",
      "epoch 0 loss=261.14874267578125\n",
      "New best validation epoch 0 loss=12.136261940002441\n",
      "epoch 200 loss=233.50083923339844\n",
      "New best validation epoch 200 loss=11.954668045043945\n",
      "epoch 400 loss=200.58187866210938\n",
      "New best validation epoch 400 loss=11.7506742477417\n",
      "epoch 600 loss=197.96319580078125\n",
      "New best validation epoch 600 loss=11.680970191955566\n",
      "epoch 800 loss=190.40606689453125\n",
      "New best validation epoch 800 loss=11.643932342529297\n",
      "epoch 1000 loss=187.07138061523438\n",
      "epoch 1200 loss=201.29013061523438\n",
      "New best validation epoch 1200 loss=11.643095970153809\n",
      "epoch 1400 loss=190.32797241210938\n",
      "epoch 1600 loss=189.0479736328125\n",
      "epoch 1800 loss=176.4501953125\n",
      "epoch 2000 loss=194.76341247558594\n",
      "epoch 2200 loss=187.73483276367188\n",
      "epoch 2400 loss=182.76324462890625\n",
      "epoch 2600 loss=196.86712646484375\n",
      "epoch 2800 loss=194.48260498046875\n",
      "epoch 3000 loss=198.50836181640625\n",
      "epoch 3200 loss=191.2762908935547\n",
      "epoch 3400 loss=193.3822479248047\n",
      "epoch 3600 loss=194.733642578125\n",
      "epoch 3800 loss=181.27816772460938\n",
      "epoch 4000 loss=189.75997924804688\n",
      "epoch 4200 loss=193.511474609375\n",
      "epoch 4400 loss=186.8582763671875\n",
      "epoch 4600 loss=196.93374633789062\n",
      "epoch 4800 loss=190.6094207763672\n",
      "epoch 4999 loss=191.57005310058594\n",
      "Finished training\n",
      "FINISHED 5 7 15.438468933105469\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383, 26987,  1078,  5096,  6596,   758,\n",
      "        18771, 22612, 22779, 19831, 24294, 31231, 17979, 24047, 25986,   991,\n",
      "         3773, 15539,  3129, 32569,  1496])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237, 14330, 11044, 20619, 25273, 20527,\n",
      "        15766,  7667, 12946, 12940, 12945,  1230,  3126,  6608,  1088,   308,\n",
      "         9586, 27580, 27698, 29356,  9590])\n",
      "epoch 0 loss=261.523681640625\n",
      "New best validation epoch 0 loss=12.136261940002441\n",
      "epoch 200 loss=231.5358123779297\n",
      "New best validation epoch 200 loss=11.935214042663574\n",
      "epoch 400 loss=198.70867919921875\n",
      "New best validation epoch 400 loss=11.745588302612305\n",
      "epoch 600 loss=192.2969207763672\n",
      "New best validation epoch 600 loss=11.732044219970703\n",
      "epoch 800 loss=182.76504516601562\n",
      "New best validation epoch 800 loss=11.723753929138184\n",
      "epoch 1000 loss=183.76821899414062\n",
      "epoch 1200 loss=191.47557067871094\n",
      "New best validation epoch 1200 loss=11.721341133117676\n",
      "epoch 1400 loss=178.44381713867188\n",
      "epoch 1600 loss=177.3041229248047\n",
      "New best validation epoch 1600 loss=11.715645790100098\n",
      "epoch 1800 loss=191.47772216796875\n",
      "epoch 2000 loss=183.13873291015625\n",
      "epoch 2200 loss=198.43505859375\n",
      "epoch 2400 loss=190.1136932373047\n",
      "epoch 2600 loss=176.8558807373047\n",
      "epoch 2800 loss=183.7799530029297\n",
      "epoch 3000 loss=191.68380737304688\n",
      "epoch 3200 loss=182.79258728027344\n",
      "epoch 3400 loss=184.74081420898438\n",
      "epoch 3600 loss=184.3602294921875\n",
      "epoch 3800 loss=189.64584350585938\n",
      "epoch 4000 loss=186.3587646484375\n",
      "epoch 4200 loss=190.50320434570312\n",
      "epoch 4400 loss=183.4004364013672\n",
      "epoch 4600 loss=184.2247314453125\n",
      "epoch 4800 loss=187.86874389648438\n",
      "epoch 4999 loss=186.280029296875\n",
      "Finished training\n",
      "FINISHED 5 8 15.358186721801758\n",
      "tensor([ 6715, 10503, 27310, 20888, 22006, 29047, 13600, 27222, 16603, 18613,\n",
      "        24420, 11024,  1078, 11020, 21135, 18241, 18244, 11528,  6092,  6803,\n",
      "         7629, 23718,  8396, 30330, 20383, 26987,  1078,  5096,  6596,   758,\n",
      "        18771, 22612, 22779, 19831, 24294, 31231, 17979, 24047, 25986,   991,\n",
      "         3773, 15539,  3129, 32569,  1496, 29803,  8491, 31233, 15282,  2001])\n",
      "tensor([18257, 24294,  9030, 10626,  2149,  4873, 29667,  3213,  3178, 29954,\n",
      "         4973, 27479, 19089, 28886,   286, 32028, 32031, 30637,  7147, 11413,\n",
      "          501, 30506, 20938,  5431, 23237, 14330, 11044, 20619, 25273, 20527,\n",
      "        15766,  7667, 12946, 12940, 12945,  1230,  3126,  6608,  1088,   308,\n",
      "         9586, 27580, 27698, 29356,  9590, 33220,   523, 22193, 13152,   420])\n",
      "epoch 0 loss=261.06231689453125\n",
      "New best validation epoch 0 loss=12.136261940002441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=226.2601776123047\n",
      "New best validation epoch 200 loss=11.883856773376465\n",
      "epoch 400 loss=183.6967010498047\n",
      "New best validation epoch 400 loss=11.704845428466797\n",
      "epoch 600 loss=183.41770935058594\n",
      "New best validation epoch 600 loss=11.672788619995117\n",
      "epoch 800 loss=178.0867156982422\n",
      "New best validation epoch 800 loss=11.659421920776367\n",
      "epoch 1000 loss=186.94802856445312\n",
      "New best validation epoch 1000 loss=11.655220031738281\n",
      "epoch 1200 loss=170.0222930908203\n",
      "New best validation epoch 1200 loss=11.64910888671875\n",
      "epoch 1400 loss=184.69558715820312\n",
      "New best validation epoch 1400 loss=11.647520065307617\n",
      "epoch 1600 loss=184.45811462402344\n",
      "epoch 1800 loss=191.73501586914062\n",
      "epoch 2000 loss=176.0986785888672\n",
      "epoch 2200 loss=180.67529296875\n",
      "epoch 2400 loss=178.2441864013672\n",
      "epoch 2600 loss=181.03500366210938\n",
      "epoch 2800 loss=170.76327514648438\n",
      "epoch 3000 loss=185.3330535888672\n",
      "epoch 3200 loss=181.06170654296875\n",
      "epoch 3400 loss=183.5664520263672\n",
      "epoch 3600 loss=178.189453125\n",
      "epoch 3800 loss=183.2213592529297\n",
      "epoch 4000 loss=183.4244842529297\n",
      "epoch 4200 loss=181.78111267089844\n",
      "epoch 4400 loss=179.293701171875\n",
      "epoch 4600 loss=188.3218231201172\n",
      "epoch 4800 loss=174.89810180664062\n",
      "epoch 4999 loss=182.96343994140625\n",
      "Finished training\n",
      "FINISHED 5 9 15.396782875061035\n",
      "tensor([24150,  9045,  7139,  7179,  9030])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160])\n",
      "epoch 0 loss=251.8164520263672\n",
      "New best validation epoch 0 loss=13.026045799255371\n",
      "epoch 200 loss=251.52566528320312\n",
      "New best validation epoch 200 loss=13.006309509277344\n",
      "epoch 400 loss=249.24270629882812\n",
      "New best validation epoch 400 loss=12.941871643066406\n",
      "epoch 600 loss=243.56045532226562\n",
      "New best validation epoch 600 loss=12.843620300292969\n",
      "epoch 800 loss=245.00242614746094\n",
      "New best validation epoch 800 loss=12.755638122558594\n",
      "epoch 1000 loss=240.0350799560547\n",
      "New best validation epoch 1000 loss=12.715779304504395\n",
      "epoch 1200 loss=240.3914031982422\n",
      "New best validation epoch 1200 loss=12.683640480041504\n",
      "epoch 1400 loss=241.9398651123047\n",
      "New best validation epoch 1400 loss=12.676297187805176\n",
      "epoch 1600 loss=238.9745330810547\n",
      "New best validation epoch 1600 loss=12.670040130615234\n",
      "epoch 1800 loss=240.92198181152344\n",
      "New best validation epoch 1800 loss=12.669014930725098\n",
      "epoch 2000 loss=242.27392578125\n",
      "New best validation epoch 2000 loss=12.666086196899414\n",
      "epoch 2200 loss=233.68572998046875\n",
      "epoch 2400 loss=240.86642456054688\n",
      "epoch 2600 loss=239.7998046875\n",
      "epoch 2800 loss=236.2019500732422\n",
      "epoch 3000 loss=239.00808715820312\n",
      "epoch 3200 loss=243.385498046875\n",
      "epoch 3400 loss=234.5099334716797\n",
      "New best validation epoch 3400 loss=12.662718772888184\n",
      "epoch 3600 loss=237.7572479248047\n",
      "epoch 3800 loss=238.4674530029297\n",
      "epoch 4000 loss=240.10923767089844\n",
      "epoch 4200 loss=243.90023803710938\n",
      "epoch 4400 loss=241.1519012451172\n",
      "epoch 4600 loss=236.3412322998047\n",
      "epoch 4800 loss=239.7114715576172\n",
      "epoch 4999 loss=241.908203125\n",
      "Finished training\n",
      "FINISHED 6 0 16.989492416381836\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178])\n",
      "epoch 0 loss=252.52548217773438\n",
      "New best validation epoch 0 loss=13.026044845581055\n",
      "epoch 200 loss=250.31759643554688\n",
      "New best validation epoch 200 loss=12.995917320251465\n",
      "epoch 400 loss=239.57249450683594\n",
      "New best validation epoch 400 loss=12.910069465637207\n",
      "epoch 600 loss=232.75839233398438\n",
      "New best validation epoch 600 loss=12.818489074707031\n",
      "epoch 800 loss=228.96792602539062\n",
      "New best validation epoch 800 loss=12.758837699890137\n",
      "epoch 1000 loss=233.41978454589844\n",
      "New best validation epoch 1000 loss=12.725926399230957\n",
      "epoch 1200 loss=228.9897918701172\n",
      "New best validation epoch 1200 loss=12.7100830078125\n",
      "epoch 1400 loss=225.39151000976562\n",
      "New best validation epoch 1400 loss=12.696255683898926\n",
      "epoch 1600 loss=226.29684448242188\n",
      "New best validation epoch 1600 loss=12.681591987609863\n",
      "epoch 1800 loss=219.84051513671875\n",
      "New best validation epoch 1800 loss=12.679495811462402\n",
      "epoch 2000 loss=219.74813842773438\n",
      "New best validation epoch 2000 loss=12.671465873718262\n",
      "epoch 2200 loss=225.88265991210938\n",
      "epoch 2400 loss=227.09219360351562\n",
      "epoch 2600 loss=218.96347045898438\n",
      "epoch 2800 loss=226.2332763671875\n",
      "New best validation epoch 2800 loss=12.669896125793457\n",
      "epoch 3000 loss=223.32000732421875\n",
      "New best validation epoch 3000 loss=12.668924331665039\n",
      "epoch 3200 loss=220.43174743652344\n",
      "New best validation epoch 3200 loss=12.666391372680664\n",
      "epoch 3400 loss=226.4803466796875\n",
      "epoch 3600 loss=222.04806518554688\n",
      "epoch 3800 loss=222.60769653320312\n",
      "epoch 4000 loss=221.0643310546875\n",
      "epoch 4200 loss=230.87667846679688\n",
      "epoch 4400 loss=218.1541748046875\n",
      "epoch 4600 loss=230.0604705810547\n",
      "epoch 4800 loss=219.8878631591797\n",
      "epoch 4999 loss=223.75656127929688\n",
      "Finished training\n",
      "FINISHED 6 1 17.074111938476562\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147])\n",
      "epoch 0 loss=253.00418090820312\n",
      "New best validation epoch 0 loss=13.026043891906738\n",
      "epoch 200 loss=246.97442626953125\n",
      "New best validation epoch 200 loss=12.923693656921387\n",
      "epoch 400 loss=228.7213592529297\n",
      "New best validation epoch 400 loss=12.65494441986084\n",
      "epoch 600 loss=213.21249389648438\n",
      "New best validation epoch 600 loss=12.427763938903809\n",
      "epoch 800 loss=210.9385223388672\n",
      "New best validation epoch 800 loss=12.339526176452637\n",
      "epoch 1000 loss=213.14195251464844\n",
      "New best validation epoch 1000 loss=12.320463180541992\n",
      "epoch 1200 loss=219.9949951171875\n",
      "New best validation epoch 1200 loss=12.314322471618652\n",
      "epoch 1400 loss=206.08128356933594\n",
      "epoch 1600 loss=199.0397186279297\n",
      "epoch 1800 loss=209.2156219482422\n",
      "epoch 2000 loss=215.8258514404297\n",
      "epoch 2200 loss=210.092529296875\n",
      "epoch 2400 loss=208.38253784179688\n",
      "epoch 2600 loss=211.70474243164062\n",
      "epoch 2800 loss=213.09542846679688\n",
      "epoch 3000 loss=216.31686401367188\n",
      "epoch 3200 loss=210.9877166748047\n",
      "epoch 3400 loss=210.7859344482422\n",
      "epoch 3600 loss=209.14984130859375\n",
      "epoch 3800 loss=219.38021850585938\n",
      "epoch 4000 loss=208.02679443359375\n",
      "epoch 4200 loss=208.146728515625\n",
      "epoch 4400 loss=217.276123046875\n",
      "epoch 4600 loss=209.78286743164062\n",
      "epoch 4800 loss=207.50155639648438\n",
      "epoch 4999 loss=214.91775512695312\n",
      "Finished training\n",
      "FINISHED 6 2 16.971435546875\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259])\n",
      "epoch 0 loss=251.925048828125\n",
      "New best validation epoch 0 loss=13.026042938232422\n",
      "epoch 200 loss=243.2777557373047\n",
      "New best validation epoch 200 loss=12.893487930297852\n",
      "epoch 400 loss=219.5703125\n",
      "New best validation epoch 400 loss=12.585887908935547\n",
      "epoch 600 loss=212.61965942382812\n",
      "New best validation epoch 600 loss=12.397575378417969\n",
      "epoch 800 loss=202.03135681152344\n",
      "New best validation epoch 800 loss=12.359430313110352\n",
      "epoch 1000 loss=203.27072143554688\n",
      "epoch 1200 loss=214.23709106445312\n",
      "epoch 1400 loss=212.97740173339844\n",
      "epoch 1600 loss=204.21261596679688\n",
      "epoch 1800 loss=207.60568237304688\n",
      "epoch 2000 loss=200.23715209960938\n",
      "epoch 2200 loss=208.31289672851562\n",
      "epoch 2400 loss=205.46128845214844\n",
      "epoch 2600 loss=201.9244384765625\n",
      "epoch 2800 loss=207.43174743652344\n",
      "epoch 3000 loss=210.8806610107422\n",
      "epoch 3200 loss=205.63145446777344\n",
      "epoch 3400 loss=202.62301635742188\n",
      "epoch 3600 loss=200.6947479248047\n",
      "epoch 3800 loss=207.2271728515625\n",
      "epoch 4000 loss=204.6959686279297\n",
      "epoch 4200 loss=198.52659606933594\n",
      "epoch 4400 loss=208.83639526367188\n",
      "epoch 4600 loss=209.3895263671875\n",
      "epoch 4800 loss=206.10165405273438\n",
      "epoch 4999 loss=199.51739501953125\n",
      "Finished training\n",
      "FINISHED 6 3 16.980993270874023\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829])\n",
      "epoch 0 loss=252.79998779296875\n",
      "New best validation epoch 0 loss=13.026042938232422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=238.5277557373047\n",
      "New best validation epoch 200 loss=12.856971740722656\n",
      "epoch 400 loss=214.55918884277344\n",
      "New best validation epoch 400 loss=12.481410026550293\n",
      "epoch 600 loss=198.84661865234375\n",
      "New best validation epoch 600 loss=12.277554512023926\n",
      "epoch 800 loss=204.73704528808594\n",
      "New best validation epoch 800 loss=12.228471755981445\n",
      "epoch 1000 loss=198.39044189453125\n",
      "New best validation epoch 1000 loss=12.221800804138184\n",
      "epoch 1200 loss=198.30447387695312\n",
      "New best validation epoch 1200 loss=12.218193054199219\n",
      "epoch 1400 loss=199.14633178710938\n",
      "New best validation epoch 1400 loss=12.213658332824707\n",
      "epoch 1600 loss=196.0362548828125\n",
      "New best validation epoch 1600 loss=12.212507247924805\n",
      "epoch 1800 loss=201.15545654296875\n",
      "epoch 2000 loss=194.10467529296875\n",
      "epoch 2200 loss=201.94052124023438\n",
      "epoch 2400 loss=198.552001953125\n",
      "epoch 2600 loss=205.60179138183594\n",
      "epoch 2800 loss=193.35162353515625\n",
      "epoch 3000 loss=196.65081787109375\n",
      "epoch 3200 loss=204.27713012695312\n",
      "epoch 3400 loss=195.14663696289062\n",
      "epoch 3600 loss=199.1060028076172\n",
      "epoch 3800 loss=196.03866577148438\n",
      "epoch 4000 loss=197.69766235351562\n",
      "epoch 4200 loss=193.33523559570312\n",
      "epoch 4400 loss=196.93704223632812\n",
      "epoch 4600 loss=198.9290008544922\n",
      "epoch 4800 loss=203.0653076171875\n",
      "epoch 4999 loss=197.0713348388672\n",
      "Finished training\n",
      "FINISHED 6 4 17.054157257080078\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194, 21035, 14615, 29575,   991, 30775])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829, 29870, 27108,   415,   140, 27103])\n",
      "epoch 0 loss=252.7841033935547\n",
      "New best validation epoch 0 loss=13.026042938232422\n",
      "epoch 200 loss=237.67098999023438\n",
      "New best validation epoch 200 loss=12.866799354553223\n",
      "epoch 400 loss=208.2855682373047\n",
      "New best validation epoch 400 loss=12.552590370178223\n",
      "epoch 600 loss=203.95111083984375\n",
      "New best validation epoch 600 loss=12.390336990356445\n",
      "epoch 800 loss=201.30792236328125\n",
      "New best validation epoch 800 loss=12.330711364746094\n",
      "epoch 1000 loss=197.44979858398438\n",
      "New best validation epoch 1000 loss=12.318751335144043\n",
      "epoch 1200 loss=196.46971130371094\n",
      "epoch 1400 loss=192.08621215820312\n",
      "epoch 1600 loss=190.5947723388672\n",
      "New best validation epoch 1600 loss=12.317368507385254\n",
      "epoch 1800 loss=192.80984497070312\n",
      "epoch 2000 loss=203.12075805664062\n",
      "epoch 2200 loss=193.6927947998047\n",
      "New best validation epoch 2200 loss=12.315086364746094\n",
      "epoch 2400 loss=194.9160614013672\n",
      "epoch 2600 loss=201.42648315429688\n",
      "epoch 2800 loss=196.60940551757812\n",
      "New best validation epoch 2800 loss=12.309148788452148\n",
      "epoch 3000 loss=193.725341796875\n",
      "epoch 3200 loss=195.32479858398438\n",
      "epoch 3400 loss=187.92349243164062\n",
      "epoch 3600 loss=196.62991333007812\n",
      "epoch 3800 loss=189.576416015625\n",
      "epoch 4000 loss=193.48251342773438\n",
      "epoch 4200 loss=199.85223388671875\n",
      "epoch 4400 loss=212.566162109375\n",
      "epoch 4600 loss=196.81048583984375\n",
      "epoch 4800 loss=195.80978393554688\n",
      "epoch 4999 loss=193.95103454589844\n",
      "Finished training\n",
      "FINISHED 6 5 16.986648559570312\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194, 21035, 14615, 29575,   991, 30775,\n",
      "        21269, 21929, 21446, 21034, 21119])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829, 29870, 27108,   415,   140, 27103,\n",
      "        22234, 31705, 22286,  4956, 28378])\n",
      "epoch 0 loss=252.22178649902344\n",
      "New best validation epoch 0 loss=13.026041984558105\n",
      "epoch 200 loss=234.1204833984375\n",
      "New best validation epoch 200 loss=12.793197631835938\n",
      "epoch 400 loss=197.0152587890625\n",
      "New best validation epoch 400 loss=12.364786148071289\n",
      "epoch 600 loss=191.65185546875\n",
      "New best validation epoch 600 loss=12.219287872314453\n",
      "epoch 800 loss=187.2188720703125\n",
      "New best validation epoch 800 loss=12.205245018005371\n",
      "epoch 1000 loss=195.5201416015625\n",
      "New best validation epoch 1000 loss=12.204998970031738\n",
      "epoch 1200 loss=190.1710662841797\n",
      "New best validation epoch 1200 loss=12.19541072845459\n",
      "epoch 1400 loss=191.9005584716797\n",
      "epoch 1600 loss=196.2147674560547\n",
      "epoch 1800 loss=191.745849609375\n",
      "epoch 2000 loss=188.36495971679688\n",
      "epoch 2200 loss=196.07589721679688\n",
      "epoch 2400 loss=183.2801513671875\n",
      "epoch 2600 loss=182.45068359375\n",
      "New best validation epoch 2600 loss=12.195014953613281\n",
      "epoch 2800 loss=195.3309326171875\n",
      "New best validation epoch 2800 loss=12.18807601928711\n",
      "epoch 3000 loss=181.9664764404297\n",
      "epoch 3200 loss=195.7257080078125\n",
      "epoch 3400 loss=198.76254272460938\n",
      "epoch 3600 loss=189.02780151367188\n",
      "epoch 3800 loss=191.2107391357422\n",
      "epoch 4000 loss=183.08653259277344\n",
      "New best validation epoch 4000 loss=12.185759544372559\n",
      "epoch 4200 loss=190.75271606445312\n",
      "epoch 4400 loss=193.530517578125\n",
      "New best validation epoch 4400 loss=12.180553436279297\n",
      "epoch 4600 loss=199.87548828125\n",
      "epoch 4800 loss=197.75106811523438\n",
      "epoch 4999 loss=189.42388916015625\n",
      "Finished training\n",
      "FINISHED 6 6 16.904537200927734\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194, 21035, 14615, 29575,   991, 30775,\n",
      "        21269, 21929, 21446, 21034, 21119, 31322,  7574,  8385,  7497,  7588])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829, 29870, 27108,   415,   140, 27103,\n",
      "        22234, 31705, 22286,  4956, 28378, 23201,   501, 31733, 32508, 10024])\n",
      "epoch 0 loss=252.6349334716797\n",
      "New best validation epoch 0 loss=13.026043891906738\n",
      "epoch 200 loss=230.07989501953125\n",
      "New best validation epoch 200 loss=12.796875953674316\n",
      "epoch 400 loss=206.60848999023438\n",
      "New best validation epoch 400 loss=12.457862854003906\n",
      "epoch 600 loss=193.1611328125\n",
      "New best validation epoch 600 loss=12.408114433288574\n",
      "epoch 800 loss=183.6143798828125\n",
      "epoch 1000 loss=184.8134002685547\n",
      "epoch 1200 loss=184.99090576171875\n",
      "epoch 1400 loss=191.97433471679688\n",
      "epoch 1600 loss=183.61143493652344\n",
      "epoch 1800 loss=183.86654663085938\n",
      "epoch 2000 loss=192.03099060058594\n",
      "epoch 2200 loss=185.9706268310547\n",
      "epoch 2400 loss=189.82701110839844\n",
      "epoch 2600 loss=180.70091247558594\n",
      "epoch 2800 loss=183.67840576171875\n",
      "epoch 3000 loss=173.9936981201172\n",
      "epoch 3200 loss=185.56578063964844\n",
      "epoch 3400 loss=187.1133270263672\n",
      "epoch 3600 loss=186.587646484375\n",
      "epoch 3800 loss=185.95211791992188\n",
      "epoch 4000 loss=192.1849822998047\n",
      "epoch 4200 loss=182.9338836669922\n",
      "epoch 4400 loss=190.85458374023438\n",
      "epoch 4600 loss=190.3753662109375\n",
      "epoch 4800 loss=186.2585906982422\n",
      "epoch 4999 loss=185.1788787841797\n",
      "Finished training\n",
      "FINISHED 6 7 16.861732482910156\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194, 21035, 14615, 29575,   991, 30775,\n",
      "        21269, 21929, 21446, 21034, 21119, 31322,  7574,  8385,  7497,  7588,\n",
      "        11173, 23821, 20429, 11537,  1848])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829, 29870, 27108,   415,   140, 27103,\n",
      "        22234, 31705, 22286,  4956, 28378, 23201,   501, 31733, 32508, 10024,\n",
      "         6683, 15376, 31154,  6682, 23821])\n",
      "epoch 0 loss=252.19754028320312\n",
      "New best validation epoch 0 loss=13.026041984558105\n",
      "epoch 200 loss=226.5006103515625\n",
      "New best validation epoch 200 loss=12.740645408630371\n",
      "epoch 400 loss=188.0172576904297\n",
      "New best validation epoch 400 loss=12.407478332519531\n",
      "epoch 600 loss=173.92514038085938\n",
      "New best validation epoch 600 loss=12.398435592651367\n",
      "epoch 800 loss=179.4110107421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000 loss=181.07943725585938\n",
      "epoch 1200 loss=171.8407440185547\n",
      "epoch 1400 loss=178.91744995117188\n",
      "epoch 1600 loss=185.32162475585938\n",
      "epoch 1800 loss=189.38177490234375\n",
      "epoch 2000 loss=183.5261688232422\n",
      "epoch 2200 loss=173.18551635742188\n",
      "epoch 2400 loss=172.18186950683594\n",
      "epoch 2600 loss=181.3551025390625\n",
      "epoch 2800 loss=181.6519012451172\n",
      "epoch 3000 loss=169.21873474121094\n",
      "epoch 3200 loss=171.19369506835938\n",
      "epoch 3400 loss=178.5149383544922\n",
      "epoch 3600 loss=179.56130981445312\n",
      "epoch 3800 loss=171.42532348632812\n",
      "epoch 4000 loss=182.92852783203125\n",
      "epoch 4200 loss=173.73992919921875\n",
      "epoch 4400 loss=175.66650390625\n",
      "epoch 4600 loss=179.52297973632812\n",
      "epoch 4800 loss=173.71835327148438\n",
      "epoch 4999 loss=180.72412109375\n",
      "Finished training\n",
      "FINISHED 6 8 16.81240463256836\n",
      "tensor([24150,  9045,  7139,  7179,  9030, 29120, 29070, 29047, 27153, 27059,\n",
      "        22798, 10885, 22018, 18975, 26016, 24752, 24746, 29136,  5091,   331,\n",
      "        14612, 17961, 23002, 17977, 16194, 21035, 14615, 29575,   991, 30775,\n",
      "        21269, 21929, 21446, 21034, 21119, 31322,  7574,  8385,  7497,  7588,\n",
      "        11173, 23821, 20429, 11537,  1848, 16766, 20699,  7816, 31059,   173])\n",
      "tensor([ 9030, 18257,  9045,  1240,  1160, 13717,  6125,  3192, 26056,  3178,\n",
      "        18227, 32283, 20625, 23701, 31147, 27580, 31558, 31779, 29356,  1259,\n",
      "         6894,  8840,  9871,  4489, 16829, 29870, 27108,   415,   140, 27103,\n",
      "        22234, 31705, 22286,  4956, 28378, 23201,   501, 31733, 32508, 10024,\n",
      "         6683, 15376, 31154,  6682, 23821,  7667,   140,  5333,  5570, 14330])\n",
      "epoch 0 loss=252.73037719726562\n",
      "New best validation epoch 0 loss=13.026042938232422\n",
      "epoch 200 loss=223.71835327148438\n",
      "New best validation epoch 200 loss=12.737065315246582\n",
      "epoch 400 loss=188.9728240966797\n",
      "New best validation epoch 400 loss=12.486082077026367\n",
      "epoch 600 loss=167.53048706054688\n",
      "epoch 800 loss=179.0264129638672\n",
      "epoch 1000 loss=167.33316040039062\n",
      "epoch 1200 loss=176.4618377685547\n",
      "epoch 1400 loss=171.21514892578125\n",
      "epoch 1600 loss=185.7742156982422\n",
      "epoch 1800 loss=177.43246459960938\n",
      "epoch 2000 loss=182.68124389648438\n",
      "epoch 2200 loss=164.1204071044922\n",
      "epoch 2400 loss=166.81124877929688\n",
      "epoch 2600 loss=178.53939819335938\n",
      "epoch 2800 loss=175.3275604248047\n",
      "epoch 3000 loss=171.1320037841797\n",
      "epoch 3200 loss=172.8173065185547\n",
      "epoch 3400 loss=163.1789093017578\n",
      "epoch 3600 loss=180.03268432617188\n",
      "epoch 3800 loss=157.95370483398438\n",
      "epoch 4000 loss=176.99998474121094\n",
      "epoch 4200 loss=178.34121704101562\n",
      "epoch 4400 loss=168.6682586669922\n",
      "epoch 4600 loss=174.486572265625\n",
      "epoch 4800 loss=174.33767700195312\n",
      "epoch 4999 loss=173.50128173828125\n",
      "Finished training\n",
      "FINISHED 6 9 16.74523162841797\n",
      "tensor([11023, 10626, 21515,  7181, 23469])\n",
      "tensor([10626, 24294,  1160,  9030, 11046])\n",
      "epoch 0 loss=257.9058837890625\n",
      "New best validation epoch 0 loss=12.014232635498047\n",
      "epoch 200 loss=256.030517578125\n",
      "New best validation epoch 200 loss=11.99586296081543\n",
      "epoch 400 loss=253.5684051513672\n",
      "New best validation epoch 400 loss=11.937716484069824\n",
      "epoch 600 loss=251.55648803710938\n",
      "New best validation epoch 600 loss=11.856245040893555\n",
      "epoch 800 loss=248.0523681640625\n",
      "New best validation epoch 800 loss=11.778244972229004\n",
      "epoch 1000 loss=246.04354858398438\n",
      "New best validation epoch 1000 loss=11.722953796386719\n",
      "epoch 1200 loss=244.25694274902344\n",
      "New best validation epoch 1200 loss=11.695646286010742\n",
      "epoch 1400 loss=244.40057373046875\n",
      "New best validation epoch 1400 loss=11.685003280639648\n",
      "epoch 1600 loss=247.41793823242188\n",
      "New best validation epoch 1600 loss=11.672287940979004\n",
      "epoch 1800 loss=244.6981658935547\n",
      "New best validation epoch 1800 loss=11.664510726928711\n",
      "epoch 2000 loss=246.49478149414062\n",
      "New best validation epoch 2000 loss=11.657976150512695\n",
      "epoch 2200 loss=247.12332153320312\n",
      "New best validation epoch 2200 loss=11.657861709594727\n",
      "epoch 2400 loss=241.6699676513672\n",
      "New best validation epoch 2400 loss=11.654770851135254\n",
      "epoch 2600 loss=244.1824951171875\n",
      "New best validation epoch 2600 loss=11.652596473693848\n",
      "epoch 2800 loss=242.1285400390625\n",
      "epoch 3000 loss=243.9586181640625\n",
      "epoch 3200 loss=243.4119110107422\n",
      "epoch 3400 loss=244.01751708984375\n",
      "New best validation epoch 3400 loss=11.650925636291504\n",
      "epoch 3600 loss=243.76028442382812\n",
      "epoch 3800 loss=248.22445678710938\n",
      "New best validation epoch 3800 loss=11.649789810180664\n",
      "epoch 4000 loss=244.5955810546875\n",
      "New best validation epoch 4000 loss=11.648811340332031\n",
      "epoch 4200 loss=243.24676513671875\n",
      "New best validation epoch 4200 loss=11.645132064819336\n",
      "epoch 4400 loss=245.5030517578125\n",
      "New best validation epoch 4400 loss=11.644503593444824\n",
      "epoch 4600 loss=241.66912841796875\n",
      "epoch 4800 loss=243.17254638671875\n",
      "New best validation epoch 4800 loss=11.639835357666016\n",
      "epoch 4999 loss=245.40618896484375\n",
      "Finished training\n",
      "FINISHED 7 0 16.355640411376953\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952])\n",
      "epoch 0 loss=257.39569091796875\n",
      "New best validation epoch 0 loss=12.014236450195312\n",
      "epoch 200 loss=255.73435974121094\n",
      "New best validation epoch 200 loss=11.99450969696045\n",
      "epoch 400 loss=246.543212890625\n",
      "New best validation epoch 400 loss=11.939334869384766\n",
      "epoch 600 loss=234.239990234375\n",
      "New best validation epoch 600 loss=11.878890037536621\n",
      "epoch 800 loss=232.37875366210938\n",
      "New best validation epoch 800 loss=11.830430030822754\n",
      "epoch 1000 loss=227.3727264404297\n",
      "New best validation epoch 1000 loss=11.788003921508789\n",
      "epoch 1200 loss=229.86349487304688\n",
      "New best validation epoch 1200 loss=11.75368595123291\n",
      "epoch 1400 loss=228.2993621826172\n",
      "New best validation epoch 1400 loss=11.728602409362793\n",
      "epoch 1600 loss=226.83042907714844\n",
      "New best validation epoch 1600 loss=11.711472511291504\n",
      "epoch 1800 loss=224.47952270507812\n",
      "New best validation epoch 1800 loss=11.699125289916992\n",
      "epoch 2000 loss=225.03562927246094\n",
      "New best validation epoch 2000 loss=11.690664291381836\n",
      "epoch 2200 loss=232.77880859375\n",
      "New best validation epoch 2200 loss=11.689300537109375\n",
      "epoch 2400 loss=228.7701416015625\n",
      "epoch 2600 loss=232.912109375\n",
      "New best validation epoch 2600 loss=11.686487197875977\n",
      "epoch 2800 loss=225.21263122558594\n",
      "New best validation epoch 2800 loss=11.683720588684082\n",
      "epoch 3000 loss=226.63446044921875\n",
      "epoch 3200 loss=242.55548095703125\n",
      "epoch 3400 loss=229.2799530029297\n",
      "epoch 3600 loss=229.84898376464844\n",
      "epoch 3800 loss=222.34124755859375\n",
      "epoch 4000 loss=228.10015869140625\n",
      "New best validation epoch 4000 loss=11.682380676269531\n",
      "epoch 4200 loss=225.68321228027344\n",
      "New best validation epoch 4200 loss=11.678277969360352\n",
      "epoch 4400 loss=224.12557983398438\n",
      "epoch 4600 loss=227.47862243652344\n",
      "epoch 4800 loss=224.95199584960938\n",
      "epoch 4999 loss=231.84149169921875\n",
      "Finished training\n",
      "FINISHED 7 1 16.065649032592773\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238])\n",
      "epoch 0 loss=256.962646484375\n",
      "New best validation epoch 0 loss=12.01423454284668\n",
      "epoch 200 loss=253.1548309326172\n",
      "New best validation epoch 200 loss=11.972472190856934\n",
      "epoch 400 loss=235.61056518554688\n",
      "New best validation epoch 400 loss=11.873983383178711\n",
      "epoch 600 loss=223.22067260742188\n",
      "New best validation epoch 600 loss=11.817273139953613\n",
      "epoch 800 loss=224.73643493652344\n",
      "epoch 1000 loss=221.03350830078125\n",
      "epoch 1200 loss=208.39895629882812\n",
      "epoch 1400 loss=219.6800994873047\n",
      "epoch 1600 loss=216.83547973632812\n",
      "epoch 1800 loss=216.6383056640625\n",
      "epoch 2000 loss=219.6090545654297\n",
      "epoch 2200 loss=214.5841522216797\n",
      "epoch 2400 loss=212.26754760742188\n",
      "epoch 2600 loss=217.03526306152344\n",
      "epoch 2800 loss=210.95733642578125\n",
      "epoch 3000 loss=215.7547607421875\n",
      "epoch 3200 loss=215.9574432373047\n",
      "epoch 3400 loss=226.51446533203125\n",
      "epoch 3600 loss=215.67327880859375\n",
      "epoch 3800 loss=213.74400329589844\n",
      "epoch 4000 loss=218.19476318359375\n",
      "epoch 4200 loss=218.83056640625\n",
      "epoch 4400 loss=223.67771911621094\n",
      "epoch 4600 loss=215.89256286621094\n",
      "epoch 4800 loss=214.38108825683594\n",
      "epoch 4999 loss=213.9283447265625\n",
      "Finished training\n",
      "FINISHED 7 2 15.978848457336426\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591])\n",
      "epoch 0 loss=258.20989990234375\n",
      "New best validation epoch 0 loss=12.014232635498047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=244.41314697265625\n",
      "New best validation epoch 200 loss=11.938530921936035\n",
      "epoch 400 loss=225.69720458984375\n",
      "New best validation epoch 400 loss=11.82094669342041\n",
      "epoch 600 loss=219.6455535888672\n",
      "epoch 800 loss=209.79002380371094\n",
      "epoch 1000 loss=213.84234619140625\n",
      "epoch 1200 loss=211.70164489746094\n",
      "epoch 1400 loss=211.75633239746094\n",
      "epoch 1600 loss=202.54771423339844\n",
      "epoch 1800 loss=205.65573120117188\n",
      "epoch 2000 loss=205.49014282226562\n",
      "epoch 2200 loss=206.79147338867188\n",
      "epoch 2400 loss=207.200439453125\n",
      "epoch 2600 loss=215.48251342773438\n",
      "epoch 2800 loss=209.37266540527344\n",
      "epoch 3000 loss=203.9722137451172\n",
      "epoch 3200 loss=213.2772216796875\n",
      "epoch 3400 loss=211.74404907226562\n",
      "epoch 3600 loss=215.4832763671875\n",
      "epoch 3800 loss=214.85595703125\n",
      "epoch 4000 loss=205.0723876953125\n",
      "epoch 4200 loss=209.1781463623047\n",
      "epoch 4400 loss=206.9588623046875\n",
      "epoch 4600 loss=203.4028778076172\n",
      "epoch 4800 loss=203.17697143554688\n",
      "epoch 4999 loss=210.126708984375\n",
      "Finished training\n",
      "FINISHED 7 3 16.158382415771484\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825])\n",
      "epoch 0 loss=257.98834228515625\n",
      "New best validation epoch 0 loss=12.01423454284668\n",
      "epoch 200 loss=243.682373046875\n",
      "New best validation epoch 200 loss=11.941381454467773\n",
      "epoch 400 loss=209.4137420654297\n",
      "New best validation epoch 400 loss=11.86522102355957\n",
      "epoch 600 loss=208.7068328857422\n",
      "epoch 800 loss=196.083984375\n",
      "epoch 1000 loss=208.05770874023438\n",
      "epoch 1200 loss=197.81326293945312\n",
      "epoch 1400 loss=195.8950653076172\n",
      "epoch 1600 loss=198.0480194091797\n",
      "epoch 1800 loss=194.0684814453125\n",
      "epoch 2000 loss=206.562255859375\n",
      "epoch 2200 loss=198.68174743652344\n",
      "epoch 2400 loss=189.2744140625\n",
      "epoch 2600 loss=194.149169921875\n",
      "epoch 2800 loss=194.61572265625\n",
      "epoch 3000 loss=196.0635986328125\n",
      "epoch 3200 loss=200.0908660888672\n",
      "epoch 3400 loss=198.15188598632812\n",
      "epoch 3600 loss=207.23802185058594\n",
      "epoch 3800 loss=199.9066162109375\n",
      "epoch 4000 loss=199.41024780273438\n",
      "epoch 4200 loss=206.91500854492188\n",
      "epoch 4400 loss=199.30416870117188\n",
      "epoch 4600 loss=197.852783203125\n",
      "epoch 4800 loss=199.89157104492188\n",
      "epoch 4999 loss=205.8389892578125\n",
      "Finished training\n",
      "FINISHED 7 4 16.131938934326172\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429,  8396, 21288, 33224, 18306, 22891])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825,  7652, 18280, 31761, 31141, 18358])\n",
      "epoch 0 loss=257.25177001953125\n",
      "New best validation epoch 0 loss=12.01423454284668\n",
      "epoch 200 loss=237.94578552246094\n",
      "New best validation epoch 200 loss=11.904962539672852\n",
      "epoch 400 loss=199.6553192138672\n",
      "New best validation epoch 400 loss=11.810077667236328\n",
      "epoch 600 loss=196.92987060546875\n",
      "epoch 800 loss=196.47885131835938\n",
      "epoch 1000 loss=199.1083221435547\n",
      "epoch 1200 loss=192.75106811523438\n",
      "epoch 1400 loss=194.8623504638672\n",
      "epoch 1600 loss=195.09544372558594\n",
      "epoch 1800 loss=192.3560028076172\n",
      "epoch 2000 loss=199.14268493652344\n",
      "epoch 2200 loss=213.6915283203125\n",
      "epoch 2400 loss=197.59707641601562\n",
      "epoch 2600 loss=205.5345458984375\n",
      "epoch 2800 loss=190.57601928710938\n",
      "epoch 3000 loss=192.42051696777344\n",
      "epoch 3200 loss=195.88323974609375\n",
      "epoch 3400 loss=202.5771484375\n",
      "epoch 3600 loss=189.48446655273438\n",
      "epoch 3800 loss=183.99102783203125\n",
      "epoch 4000 loss=193.46243286132812\n",
      "epoch 4200 loss=206.2839813232422\n",
      "epoch 4400 loss=192.23785400390625\n",
      "epoch 4600 loss=192.45272827148438\n",
      "epoch 4800 loss=200.80325317382812\n",
      "epoch 4999 loss=191.0537567138672\n",
      "Finished training\n",
      "FINISHED 7 5 16.07613182067871\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429,  8396, 21288, 33224, 18306, 22891,\n",
      "          991, 31120, 15793, 21446, 23450])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825,  7652, 18280, 31761, 31141, 18358,\n",
      "        12662, 28881, 16767, 28886, 23308])\n",
      "epoch 0 loss=256.989501953125\n",
      "New best validation epoch 0 loss=12.01423454284668\n",
      "epoch 200 loss=236.4228515625\n",
      "New best validation epoch 200 loss=11.895021438598633\n",
      "epoch 400 loss=201.3316650390625\n",
      "New best validation epoch 400 loss=11.83617877960205\n",
      "epoch 600 loss=186.51431274414062\n",
      "epoch 800 loss=185.0919647216797\n",
      "epoch 1000 loss=185.25048828125\n",
      "epoch 1200 loss=194.0604248046875\n",
      "epoch 1400 loss=189.9495849609375\n",
      "epoch 1600 loss=183.58157348632812\n",
      "epoch 1800 loss=184.24160766601562\n",
      "epoch 2000 loss=187.76226806640625\n",
      "epoch 2200 loss=180.9334716796875\n",
      "epoch 2400 loss=201.9690399169922\n",
      "epoch 2600 loss=195.35556030273438\n",
      "epoch 2800 loss=188.71884155273438\n",
      "epoch 3000 loss=189.46365356445312\n",
      "epoch 3200 loss=187.9270477294922\n",
      "epoch 3400 loss=192.83453369140625\n",
      "epoch 3600 loss=180.57772827148438\n",
      "epoch 3800 loss=185.21575927734375\n",
      "epoch 4000 loss=185.40330505371094\n",
      "epoch 4200 loss=174.3478240966797\n",
      "epoch 4400 loss=186.35763549804688\n",
      "epoch 4600 loss=200.00955200195312\n",
      "epoch 4800 loss=189.33596801757812\n",
      "epoch 4999 loss=195.69143676757812\n",
      "Finished training\n",
      "FINISHED 7 6 16.019573211669922\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429,  8396, 21288, 33224, 18306, 22891,\n",
      "          991, 31120, 15793, 21446, 23450, 17054, 13524, 32502, 20022,   172])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825,  7652, 18280, 31761, 31141, 18358,\n",
      "        12662, 28881, 16767, 28886, 23308, 33477, 33489, 27439, 20365,  7129])\n",
      "epoch 0 loss=257.9379577636719\n",
      "New best validation epoch 0 loss=12.01423168182373\n",
      "epoch 200 loss=233.591552734375\n",
      "New best validation epoch 200 loss=11.863469123840332\n",
      "epoch 400 loss=192.36138916015625\n",
      "New best validation epoch 400 loss=11.764535903930664\n",
      "epoch 600 loss=180.744140625\n",
      "epoch 800 loss=182.4197998046875\n",
      "epoch 1000 loss=186.6005859375\n",
      "epoch 1200 loss=180.8185272216797\n",
      "epoch 1400 loss=172.73141479492188\n",
      "epoch 1600 loss=183.51329040527344\n",
      "epoch 1800 loss=188.91128540039062\n",
      "epoch 2000 loss=182.1427459716797\n",
      "epoch 2200 loss=181.03451538085938\n",
      "epoch 2400 loss=188.2354736328125\n",
      "epoch 2600 loss=182.91331481933594\n",
      "epoch 2800 loss=184.4620361328125\n",
      "epoch 3000 loss=178.60189819335938\n",
      "epoch 3200 loss=177.0343017578125\n",
      "epoch 3400 loss=186.63729858398438\n",
      "epoch 3600 loss=175.90975952148438\n",
      "epoch 3800 loss=182.03204345703125\n",
      "epoch 4000 loss=184.59600830078125\n",
      "epoch 4200 loss=182.44369506835938\n",
      "epoch 4400 loss=175.4692840576172\n",
      "epoch 4600 loss=176.87240600585938\n",
      "epoch 4800 loss=181.4441375732422\n",
      "epoch 4999 loss=180.81600952148438\n",
      "Finished training\n",
      "FINISHED 7 7 16.027877807617188\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429,  8396, 21288, 33224, 18306, 22891,\n",
      "          991, 31120, 15793, 21446, 23450, 17054, 13524, 32502, 20022,   172,\n",
      "        24709,  7588,  3150,  8491, 23639])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825,  7652, 18280, 31761, 31141, 18358,\n",
      "        12662, 28881, 16767, 28886, 23308, 33477, 33489, 27439, 20365,  7129,\n",
      "        11027, 32980, 23700,  4958, 23696])\n",
      "epoch 0 loss=258.1201477050781\n",
      "New best validation epoch 0 loss=12.014230728149414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=223.82518005371094\n",
      "New best validation epoch 200 loss=11.848085403442383\n",
      "epoch 400 loss=192.8459930419922\n",
      "New best validation epoch 400 loss=11.806303024291992\n",
      "epoch 600 loss=179.26657104492188\n",
      "epoch 800 loss=183.2635955810547\n",
      "epoch 1000 loss=178.51580810546875\n",
      "epoch 1200 loss=187.29891967773438\n",
      "epoch 1400 loss=167.07313537597656\n",
      "epoch 1600 loss=173.19981384277344\n",
      "epoch 1800 loss=180.3763427734375\n",
      "epoch 2000 loss=174.176025390625\n",
      "epoch 2200 loss=175.85629272460938\n",
      "epoch 2400 loss=183.8286590576172\n",
      "epoch 2600 loss=182.00022888183594\n",
      "epoch 2800 loss=173.12908935546875\n",
      "epoch 3000 loss=180.5137939453125\n",
      "epoch 3200 loss=178.5485076904297\n",
      "epoch 3400 loss=180.527099609375\n",
      "epoch 3600 loss=176.59091186523438\n",
      "epoch 3800 loss=182.09396362304688\n",
      "epoch 4000 loss=181.279296875\n",
      "epoch 4200 loss=187.8614501953125\n",
      "epoch 4400 loss=185.7734375\n",
      "epoch 4600 loss=177.88534545898438\n",
      "epoch 4800 loss=174.60354614257812\n",
      "epoch 4999 loss=172.61917114257812\n",
      "Finished training\n",
      "FINISHED 7 8 16.102209091186523\n",
      "tensor([11023, 10626, 21515,  7181, 23469, 22141, 19742, 30348, 22051, 22042,\n",
      "        22798, 27310, 21859, 21858, 10503, 10885, 34596,  7166, 19069,  9041,\n",
      "         3700, 29575, 15699,  6092, 20429,  8396, 21288, 33224, 18306, 22891,\n",
      "          991, 31120, 15793, 21446, 23450, 17054, 13524, 32502, 20022,   172,\n",
      "        24709,  7588,  3150,  8491, 23639,  5227,  2846,  7816,  5306,  5275])\n",
      "tensor([10626, 24294,  1160,  9030, 11046, 29954, 26056, 19477, 13717,  7952,\n",
      "        29850, 32283,  1230, 14706, 29238, 27220,  8850, 30463, 28811,  6591,\n",
      "        29688,  3722,  5371, 13933, 27825,  7652, 18280, 31761, 31141, 18358,\n",
      "        12662, 28881, 16767, 28886, 23308, 33477, 33489, 27439, 20365,  7129,\n",
      "        11027, 32980, 23700,  4958, 23696, 20032, 23541, 30132, 17055, 20012])\n",
      "epoch 0 loss=257.861572265625\n",
      "New best validation epoch 0 loss=12.014230728149414\n",
      "epoch 200 loss=224.30307006835938\n",
      "New best validation epoch 200 loss=11.850876808166504\n",
      "epoch 400 loss=186.8263397216797\n",
      "New best validation epoch 400 loss=11.779180526733398\n",
      "epoch 600 loss=181.9968719482422\n",
      "epoch 800 loss=177.37771606445312\n",
      "epoch 1000 loss=171.03892517089844\n",
      "epoch 1200 loss=184.9836883544922\n",
      "epoch 1400 loss=174.64675903320312\n",
      "epoch 1600 loss=187.40802001953125\n",
      "epoch 1800 loss=175.22659301757812\n",
      "epoch 2000 loss=173.4285125732422\n",
      "epoch 2200 loss=175.44540405273438\n",
      "epoch 2400 loss=175.0439910888672\n",
      "epoch 2600 loss=181.67623901367188\n",
      "epoch 2800 loss=175.49630737304688\n",
      "epoch 3000 loss=173.05711364746094\n",
      "epoch 3200 loss=180.99481201171875\n",
      "epoch 3400 loss=182.64535522460938\n",
      "epoch 3600 loss=174.80859375\n",
      "epoch 3800 loss=180.80258178710938\n",
      "epoch 4000 loss=181.14486694335938\n",
      "epoch 4200 loss=163.44284057617188\n",
      "epoch 4400 loss=179.08236694335938\n",
      "epoch 4600 loss=169.06436157226562\n",
      "epoch 4800 loss=167.35911560058594\n",
      "epoch 4999 loss=171.29258728027344\n",
      "Finished training\n",
      "FINISHED 7 9 16.09469985961914\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858])\n",
      "tensor([29358,  1160, 15002, 25274, 10626])\n",
      "epoch 0 loss=256.60528564453125\n",
      "New best validation epoch 0 loss=11.969040870666504\n",
      "epoch 200 loss=256.8043518066406\n",
      "New best validation epoch 200 loss=11.957839012145996\n",
      "epoch 400 loss=252.99237060546875\n",
      "New best validation epoch 400 loss=11.921198844909668\n",
      "epoch 600 loss=249.87155151367188\n",
      "New best validation epoch 600 loss=11.860150337219238\n",
      "epoch 800 loss=245.54136657714844\n",
      "New best validation epoch 800 loss=11.786827087402344\n",
      "epoch 1000 loss=244.86495971679688\n",
      "New best validation epoch 1000 loss=11.725248336791992\n",
      "epoch 1200 loss=240.33670043945312\n",
      "New best validation epoch 1200 loss=11.681557655334473\n",
      "epoch 1400 loss=242.817626953125\n",
      "New best validation epoch 1400 loss=11.655511856079102\n",
      "epoch 1600 loss=237.54388427734375\n",
      "New best validation epoch 1600 loss=11.646757125854492\n",
      "epoch 1800 loss=242.2196502685547\n",
      "epoch 2000 loss=244.2637176513672\n",
      "epoch 2200 loss=243.3061981201172\n",
      "epoch 2400 loss=240.66990661621094\n",
      "epoch 2600 loss=245.00637817382812\n",
      "epoch 2800 loss=239.54510498046875\n",
      "epoch 3000 loss=243.32131958007812\n",
      "epoch 3200 loss=242.27157592773438\n",
      "epoch 3400 loss=241.4844207763672\n",
      "epoch 3600 loss=241.82894897460938\n",
      "epoch 3800 loss=241.00363159179688\n",
      "epoch 4000 loss=241.9729461669922\n",
      "epoch 4200 loss=239.8740234375\n",
      "epoch 4400 loss=246.25634765625\n",
      "epoch 4600 loss=242.17306518554688\n",
      "epoch 4800 loss=241.4225311279297\n",
      "epoch 4999 loss=245.0239715576172\n",
      "Finished training\n",
      "FINISHED 8 0 16.26683807373047\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238])\n",
      "epoch 0 loss=257.3724060058594\n",
      "New best validation epoch 0 loss=11.969040870666504\n",
      "epoch 200 loss=254.6998291015625\n",
      "New best validation epoch 200 loss=11.923966407775879\n",
      "epoch 400 loss=248.8466796875\n",
      "New best validation epoch 400 loss=11.789628028869629\n",
      "epoch 600 loss=235.87319946289062\n",
      "New best validation epoch 600 loss=11.618285179138184\n",
      "epoch 800 loss=230.28469848632812\n",
      "New best validation epoch 800 loss=11.498642921447754\n",
      "epoch 1000 loss=236.7856903076172\n",
      "New best validation epoch 1000 loss=11.448945999145508\n",
      "epoch 1200 loss=229.05322265625\n",
      "New best validation epoch 1200 loss=11.420539855957031\n",
      "epoch 1400 loss=231.7391357421875\n",
      "New best validation epoch 1400 loss=11.407377243041992\n",
      "epoch 1600 loss=225.9364013671875\n",
      "New best validation epoch 1600 loss=11.396764755249023\n",
      "epoch 1800 loss=220.04592895507812\n",
      "New best validation epoch 1800 loss=11.395930290222168\n",
      "epoch 2000 loss=223.9442138671875\n",
      "New best validation epoch 2000 loss=11.39000415802002\n",
      "epoch 2200 loss=231.248046875\n",
      "epoch 2400 loss=224.55711364746094\n",
      "epoch 2600 loss=230.3362274169922\n",
      "epoch 2800 loss=225.6533660888672\n",
      "epoch 3000 loss=228.35055541992188\n",
      "epoch 3200 loss=223.2098388671875\n",
      "epoch 3400 loss=229.54412841796875\n",
      "epoch 3600 loss=225.47726440429688\n",
      "epoch 3800 loss=225.98130798339844\n",
      "epoch 4000 loss=223.48727416992188\n",
      "epoch 4200 loss=226.8846435546875\n",
      "epoch 4400 loss=224.73568725585938\n",
      "epoch 4600 loss=234.36221313476562\n",
      "epoch 4800 loss=228.49436950683594\n",
      "epoch 4999 loss=228.28030395507812\n",
      "Finished training\n",
      "FINISHED 8 1 16.336383819580078\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017])\n",
      "epoch 0 loss=257.00506591796875\n",
      "New best validation epoch 0 loss=11.969038963317871\n",
      "epoch 200 loss=250.5402069091797\n",
      "New best validation epoch 200 loss=11.86950969696045\n",
      "epoch 400 loss=234.6541748046875\n",
      "New best validation epoch 400 loss=11.596001625061035\n",
      "epoch 600 loss=222.09178161621094\n",
      "New best validation epoch 600 loss=11.344621658325195\n",
      "epoch 800 loss=220.60104370117188\n",
      "New best validation epoch 800 loss=11.225366592407227\n",
      "epoch 1000 loss=212.87738037109375\n",
      "New best validation epoch 1000 loss=11.16987419128418\n",
      "epoch 1200 loss=211.7045135498047\n",
      "New best validation epoch 1200 loss=11.149721145629883\n",
      "epoch 1400 loss=220.2030487060547\n",
      "New best validation epoch 1400 loss=11.129144668579102\n",
      "epoch 1600 loss=209.99798583984375\n",
      "New best validation epoch 1600 loss=11.12907600402832\n",
      "epoch 1800 loss=209.98300170898438\n",
      "epoch 2000 loss=216.26951599121094\n",
      "New best validation epoch 2000 loss=11.12181568145752\n",
      "epoch 2200 loss=209.26927185058594\n",
      "epoch 2400 loss=213.13360595703125\n",
      "New best validation epoch 2400 loss=11.12088394165039\n",
      "epoch 2600 loss=220.22447204589844\n",
      "epoch 2800 loss=209.2244873046875\n",
      "New best validation epoch 2800 loss=11.119617462158203\n",
      "epoch 3000 loss=215.80918884277344\n",
      "New best validation epoch 3000 loss=11.11896800994873\n",
      "epoch 3200 loss=220.7728271484375\n",
      "New best validation epoch 3200 loss=11.118502616882324\n",
      "epoch 3400 loss=237.66519165039062\n",
      "New best validation epoch 3400 loss=11.11583423614502\n",
      "epoch 3600 loss=213.277587890625\n",
      "epoch 3800 loss=220.38320922851562\n",
      "epoch 4000 loss=215.68972778320312\n",
      "epoch 4200 loss=217.9376220703125\n",
      "epoch 4400 loss=208.82815551757812\n",
      "epoch 4600 loss=215.28482055664062\n",
      "epoch 4800 loss=214.48812866210938\n",
      "epoch 4999 loss=216.48434448242188\n",
      "Finished training\n",
      "FINISHED 8 2 16.490785598754883\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667])\n",
      "epoch 0 loss=257.46038818359375\n",
      "New best validation epoch 0 loss=11.969038009643555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=248.0531005859375\n",
      "New best validation epoch 200 loss=11.8425931930542\n",
      "epoch 400 loss=224.98788452148438\n",
      "New best validation epoch 400 loss=11.531442642211914\n",
      "epoch 600 loss=216.36871337890625\n",
      "New best validation epoch 600 loss=11.30679988861084\n",
      "epoch 800 loss=206.62217712402344\n",
      "New best validation epoch 800 loss=11.22919750213623\n",
      "epoch 1000 loss=214.89013671875\n",
      "New best validation epoch 1000 loss=11.211294174194336\n",
      "epoch 1200 loss=196.04052734375\n",
      "New best validation epoch 1200 loss=11.20471477508545\n",
      "epoch 1400 loss=196.8184051513672\n",
      "New best validation epoch 1400 loss=11.197771072387695\n",
      "epoch 1600 loss=209.23936462402344\n",
      "New best validation epoch 1600 loss=11.189361572265625\n",
      "epoch 1800 loss=207.13165283203125\n",
      "New best validation epoch 1800 loss=11.180630683898926\n",
      "epoch 2000 loss=210.91561889648438\n",
      "epoch 2200 loss=204.5506591796875\n",
      "epoch 2400 loss=216.8360595703125\n",
      "epoch 2600 loss=208.76068115234375\n",
      "New best validation epoch 2600 loss=11.178951263427734\n",
      "epoch 2800 loss=205.0347137451172\n",
      "New best validation epoch 2800 loss=11.1763916015625\n",
      "epoch 3000 loss=208.58541870117188\n",
      "epoch 3200 loss=206.7320556640625\n",
      "New best validation epoch 3200 loss=11.17558765411377\n",
      "epoch 3400 loss=210.6602783203125\n",
      "epoch 3600 loss=211.0979461669922\n",
      "epoch 3800 loss=206.6827392578125\n",
      "epoch 4000 loss=207.07774353027344\n",
      "epoch 4200 loss=206.7191619873047\n",
      "epoch 4400 loss=210.6229705810547\n",
      "epoch 4600 loss=219.80955505371094\n",
      "epoch 4800 loss=208.63742065429688\n",
      "epoch 4999 loss=205.95858764648438\n",
      "Finished training\n",
      "FINISHED 8 3 16.450077056884766\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733])\n",
      "epoch 0 loss=257.37579345703125\n",
      "New best validation epoch 0 loss=11.969038963317871\n",
      "epoch 200 loss=245.0504608154297\n",
      "New best validation epoch 200 loss=11.782038688659668\n",
      "epoch 400 loss=216.5596923828125\n",
      "New best validation epoch 400 loss=11.397109031677246\n",
      "epoch 600 loss=203.845458984375\n",
      "New best validation epoch 600 loss=11.200002670288086\n",
      "epoch 800 loss=204.72457885742188\n",
      "New best validation epoch 800 loss=11.149014472961426\n",
      "epoch 1000 loss=199.01132202148438\n",
      "New best validation epoch 1000 loss=11.132304191589355\n",
      "epoch 1200 loss=200.74249267578125\n",
      "New best validation epoch 1200 loss=11.123268127441406\n",
      "epoch 1400 loss=206.03250122070312\n",
      "epoch 1600 loss=199.07781982421875\n",
      "New best validation epoch 1600 loss=11.121617317199707\n",
      "epoch 1800 loss=199.04588317871094\n",
      "New best validation epoch 1800 loss=11.120307922363281\n",
      "epoch 2000 loss=200.85940551757812\n",
      "epoch 2200 loss=198.238525390625\n",
      "epoch 2400 loss=200.0631561279297\n",
      "epoch 2600 loss=210.6083984375\n",
      "epoch 2800 loss=199.7392578125\n",
      "epoch 3000 loss=195.42462158203125\n",
      "epoch 3200 loss=193.39114379882812\n",
      "epoch 3400 loss=197.5182342529297\n",
      "epoch 3600 loss=199.54559326171875\n",
      "epoch 3800 loss=197.16622924804688\n",
      "New best validation epoch 3800 loss=11.120244979858398\n",
      "epoch 4000 loss=201.20281982421875\n",
      "New best validation epoch 4000 loss=11.119150161743164\n",
      "epoch 4200 loss=203.17620849609375\n",
      "epoch 4400 loss=202.0360565185547\n",
      "epoch 4600 loss=192.71063232421875\n",
      "epoch 4800 loss=193.5409393310547\n",
      "epoch 4999 loss=196.9855194091797\n",
      "Finished training\n",
      "FINISHED 8 4 16.555007934570312\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790, 20704, 18090, 21615, 21362,  3848])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733, 16896, 24205,  3173, 14404,  1225])\n",
      "epoch 0 loss=257.689697265625\n",
      "New best validation epoch 0 loss=11.969037055969238\n",
      "epoch 200 loss=238.6332244873047\n",
      "New best validation epoch 200 loss=11.722631454467773\n",
      "epoch 400 loss=210.48233032226562\n",
      "New best validation epoch 400 loss=11.285964012145996\n",
      "epoch 600 loss=199.70419311523438\n",
      "New best validation epoch 600 loss=11.115496635437012\n",
      "epoch 800 loss=198.2201385498047\n",
      "New best validation epoch 800 loss=11.072041511535645\n",
      "epoch 1000 loss=183.00119018554688\n",
      "New best validation epoch 1000 loss=11.061630249023438\n",
      "epoch 1200 loss=194.57196044921875\n",
      "New best validation epoch 1200 loss=11.049978256225586\n",
      "epoch 1400 loss=203.0643768310547\n",
      "epoch 1600 loss=195.1439208984375\n",
      "New best validation epoch 1600 loss=11.039447784423828\n",
      "epoch 1800 loss=186.78475952148438\n",
      "epoch 2000 loss=194.8391571044922\n",
      "epoch 2200 loss=185.77621459960938\n",
      "New best validation epoch 2200 loss=11.03734302520752\n",
      "epoch 2400 loss=193.89817810058594\n",
      "New best validation epoch 2400 loss=11.033183097839355\n",
      "epoch 2600 loss=189.49815368652344\n",
      "epoch 2800 loss=189.6195068359375\n",
      "epoch 3000 loss=201.6666259765625\n",
      "New best validation epoch 3000 loss=11.031978607177734\n",
      "epoch 3200 loss=195.3057098388672\n",
      "epoch 3400 loss=194.26651000976562\n",
      "epoch 3600 loss=199.30429077148438\n",
      "epoch 3800 loss=188.150634765625\n",
      "epoch 4000 loss=200.27182006835938\n",
      "New best validation epoch 4000 loss=11.031472206115723\n",
      "epoch 4200 loss=193.08407592773438\n",
      "epoch 4400 loss=189.07408142089844\n",
      "epoch 4600 loss=200.77456665039062\n",
      "epoch 4800 loss=195.02206420898438\n",
      "epoch 4999 loss=180.25535583496094\n",
      "Finished training\n",
      "FINISHED 8 5 16.57253646850586\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790, 20704, 18090, 21615, 21362,  3848,\n",
      "        16926,  1972, 16794,  2067, 18159])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733, 16896, 24205,  3173, 14404,  1225,\n",
      "         7460,  8850,  8926,  8610,  7692])\n",
      "epoch 0 loss=257.3169250488281\n",
      "New best validation epoch 0 loss=11.969035148620605\n",
      "epoch 200 loss=237.46859741210938\n",
      "New best validation epoch 200 loss=11.64315414428711\n",
      "epoch 400 loss=204.4852752685547\n",
      "New best validation epoch 400 loss=11.126428604125977\n",
      "epoch 600 loss=196.176513671875\n",
      "New best validation epoch 600 loss=10.956084251403809\n",
      "epoch 800 loss=187.7540283203125\n",
      "New best validation epoch 800 loss=10.919529914855957\n",
      "epoch 1000 loss=188.49874877929688\n",
      "New best validation epoch 1000 loss=10.915976524353027\n",
      "epoch 1200 loss=187.51568603515625\n",
      "New best validation epoch 1200 loss=10.905046463012695\n",
      "epoch 1400 loss=189.3634490966797\n",
      "New best validation epoch 1400 loss=10.901869773864746\n",
      "epoch 1600 loss=189.1400146484375\n",
      "epoch 1800 loss=194.9420623779297\n",
      "New best validation epoch 1800 loss=10.889579772949219\n",
      "epoch 2000 loss=193.08447265625\n",
      "New best validation epoch 2000 loss=10.8812255859375\n",
      "epoch 2200 loss=188.53427124023438\n",
      "epoch 2400 loss=185.9855194091797\n",
      "epoch 2600 loss=191.19586181640625\n",
      "epoch 2800 loss=188.885498046875\n",
      "epoch 3000 loss=191.25399780273438\n",
      "epoch 3200 loss=190.61663818359375\n",
      "epoch 3400 loss=184.9080810546875\n",
      "epoch 3600 loss=190.4153289794922\n",
      "epoch 3800 loss=192.00753784179688\n",
      "epoch 4000 loss=188.15658569335938\n",
      "epoch 4200 loss=193.799072265625\n",
      "epoch 4400 loss=193.23519897460938\n",
      "epoch 4600 loss=196.5597381591797\n",
      "epoch 4800 loss=197.49728393554688\n",
      "epoch 4999 loss=187.98016357421875\n",
      "Finished training\n",
      "FINISHED 8 6 16.591846466064453\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790, 20704, 18090, 21615, 21362,  3848,\n",
      "        16926,  1972, 16794,  2067, 18159, 10826,    31, 31065,   172, 30775])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733, 16896, 24205,  3173, 14404,  1225,\n",
      "         7460,  8850,  8926,  8610,  7692,  1230,  1088, 10981,  1244, 11027])\n",
      "epoch 0 loss=257.5155029296875\n",
      "New best validation epoch 0 loss=11.969035148620605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss=233.6573486328125\n",
      "New best validation epoch 200 loss=11.585792541503906\n",
      "epoch 400 loss=195.7008056640625\n",
      "New best validation epoch 400 loss=11.060441017150879\n",
      "epoch 600 loss=195.83782958984375\n",
      "New best validation epoch 600 loss=10.92909049987793\n",
      "epoch 800 loss=186.63601684570312\n",
      "New best validation epoch 800 loss=10.89852523803711\n",
      "epoch 1000 loss=193.49948120117188\n",
      "New best validation epoch 1000 loss=10.88775634765625\n",
      "epoch 1200 loss=187.24533081054688\n",
      "New best validation epoch 1200 loss=10.880043029785156\n",
      "epoch 1400 loss=188.86428833007812\n",
      "epoch 1600 loss=190.77670288085938\n",
      "epoch 1800 loss=195.23464965820312\n",
      "epoch 2000 loss=190.8074951171875\n",
      "New best validation epoch 2000 loss=10.87102222442627\n",
      "epoch 2200 loss=178.42593383789062\n",
      "epoch 2400 loss=187.12060546875\n",
      "New best validation epoch 2400 loss=10.870783805847168\n",
      "epoch 2600 loss=189.17269897460938\n",
      "epoch 2800 loss=184.97659301757812\n",
      "epoch 3000 loss=190.0190887451172\n",
      "epoch 3200 loss=188.71331787109375\n",
      "epoch 3400 loss=178.8186492919922\n",
      "epoch 3600 loss=181.0200653076172\n",
      "epoch 3800 loss=182.099365234375\n",
      "epoch 4000 loss=191.4873046875\n",
      "epoch 4200 loss=194.53599548339844\n",
      "epoch 4400 loss=184.90708923339844\n",
      "epoch 4600 loss=182.82284545898438\n",
      "epoch 4800 loss=183.75836181640625\n",
      "epoch 4999 loss=191.39695739746094\n",
      "Finished training\n",
      "FINISHED 8 7 16.730348587036133\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790, 20704, 18090, 21615, 21362,  3848,\n",
      "        16926,  1972, 16794,  2067, 18159, 10826,    31, 31065,   172, 30775,\n",
      "        19059,  7217,    90, 19002,  2182])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733, 16896, 24205,  3173, 14404,  1225,\n",
      "         7460,  8850,  8926,  8610,  7692,  1230,  1088, 10981,  1244, 11027,\n",
      "        30556, 32023, 30373, 28544, 32505])\n",
      "epoch 0 loss=256.79254150390625\n",
      "New best validation epoch 0 loss=11.969034194946289\n",
      "epoch 200 loss=229.93966674804688\n",
      "New best validation epoch 200 loss=11.504971504211426\n",
      "epoch 400 loss=190.386962890625\n",
      "New best validation epoch 400 loss=10.930352210998535\n",
      "epoch 600 loss=182.4487762451172\n",
      "New best validation epoch 600 loss=10.793333053588867\n",
      "epoch 800 loss=178.74905395507812\n",
      "New best validation epoch 800 loss=10.785348892211914\n",
      "epoch 1000 loss=177.6962890625\n",
      "New best validation epoch 1000 loss=10.780632972717285\n",
      "epoch 1200 loss=177.68893432617188\n",
      "New best validation epoch 1200 loss=10.773164749145508\n",
      "epoch 1400 loss=180.22445678710938\n",
      "epoch 1600 loss=179.19711303710938\n",
      "New best validation epoch 1600 loss=10.756872177124023\n",
      "epoch 1800 loss=184.98062133789062\n",
      "epoch 2000 loss=174.22422790527344\n",
      "epoch 2200 loss=182.78378295898438\n",
      "epoch 2400 loss=181.79251098632812\n",
      "epoch 2600 loss=182.6820068359375\n",
      "epoch 2800 loss=188.14016723632812\n",
      "epoch 3000 loss=175.3134765625\n",
      "epoch 3200 loss=182.01031494140625\n",
      "epoch 3400 loss=176.43917846679688\n",
      "epoch 3600 loss=181.1671142578125\n",
      "epoch 3800 loss=187.2247772216797\n",
      "epoch 4000 loss=174.0002899169922\n",
      "epoch 4200 loss=176.83016967773438\n",
      "epoch 4400 loss=182.7808074951172\n",
      "epoch 4600 loss=187.87582397460938\n",
      "epoch 4800 loss=170.9921112060547\n",
      "epoch 4999 loss=178.3158416748047\n",
      "Finished training\n",
      "FINISHED 8 8 16.654233932495117\n",
      "tensor([ 9030,  7162, 23469,  6715, 21858, 22779, 27123, 29047, 27182, 24869,\n",
      "        11020, 11023, 11027, 26016, 11024,  7932,  4158, 32286, 31155, 11300,\n",
      "        10532, 19666, 16094,  1078, 22790, 20704, 18090, 21615, 21362,  3848,\n",
      "        16926,  1972, 16794,  2067, 18159, 10826,    31, 31065,   172, 30775,\n",
      "        19059,  7217,    90, 19002,  2182, 24047, 25854, 31237, 18173, 28278])\n",
      "tensor([29358,  1160, 15002, 25274, 10626, 31215, 17880, 30139, 11046, 29238,\n",
      "        27947, 29956,  4891, 28946,  8017, 31762, 30335, 27785, 30032, 27667,\n",
      "        13067, 31147, 24709, 26890,  7733, 16896, 24205,  3173, 14404,  1225,\n",
      "         7460,  8850,  8926,  8610,  7692,  1230,  1088, 10981,  1244, 11027,\n",
      "        30556, 32023, 30373, 28544, 32505, 31779, 10862, 27600,  4592, 29385])\n",
      "epoch 0 loss=257.0246276855469\n",
      "New best validation epoch 0 loss=11.969033241271973\n",
      "epoch 200 loss=227.2926025390625\n",
      "New best validation epoch 200 loss=11.527307510375977\n",
      "epoch 400 loss=185.43380737304688\n",
      "New best validation epoch 400 loss=11.056611061096191\n",
      "epoch 600 loss=179.349609375\n",
      "New best validation epoch 600 loss=10.974424362182617\n",
      "epoch 800 loss=174.05517578125\n",
      "New best validation epoch 800 loss=10.9170560836792\n",
      "epoch 1000 loss=178.9822235107422\n",
      "New best validation epoch 1000 loss=10.87829303741455\n",
      "epoch 1200 loss=181.30279541015625\n",
      "New best validation epoch 1200 loss=10.869884490966797\n",
      "epoch 1400 loss=179.03564453125\n",
      "New best validation epoch 1400 loss=10.850180625915527\n",
      "epoch 1600 loss=183.9269256591797\n",
      "epoch 1800 loss=177.25442504882812\n",
      "epoch 2000 loss=177.55966186523438\n",
      "epoch 2200 loss=182.1290283203125\n",
      "epoch 2400 loss=175.5384521484375\n",
      "epoch 2600 loss=174.30441284179688\n",
      "epoch 2800 loss=178.33883666992188\n",
      "epoch 3000 loss=183.49200439453125\n",
      "epoch 3200 loss=170.52828979492188\n",
      "epoch 3400 loss=178.77035522460938\n",
      "epoch 3600 loss=175.021240234375\n",
      "epoch 3800 loss=174.3596649169922\n",
      "epoch 4000 loss=177.21536254882812\n",
      "epoch 4200 loss=174.48947143554688\n",
      "epoch 4400 loss=178.69479370117188\n",
      "epoch 4600 loss=180.7104949951172\n",
      "epoch 4800 loss=174.97665405273438\n",
      "epoch 4999 loss=173.15380859375\n",
      "Finished training\n",
      "FINISHED 8 9 16.77014923095703\n",
      "tensor([27310, 19059,   500, 11023, 22786])\n",
      "tensor([10626,  1160,  2162, 24294, 31147])\n",
      "epoch 0 loss=253.9889373779297\n",
      "New best validation epoch 0 loss=12.230117797851562\n",
      "epoch 200 loss=253.71487426757812\n",
      "New best validation epoch 200 loss=12.207452774047852\n",
      "epoch 400 loss=249.71243286132812\n",
      "New best validation epoch 400 loss=12.13491439819336\n",
      "epoch 600 loss=245.9977264404297\n",
      "New best validation epoch 600 loss=12.032988548278809\n",
      "epoch 800 loss=243.03184509277344\n",
      "New best validation epoch 800 loss=11.930657386779785\n",
      "epoch 1000 loss=240.48196411132812\n",
      "New best validation epoch 1000 loss=11.862375259399414\n",
      "epoch 1200 loss=237.30931091308594\n",
      "New best validation epoch 1200 loss=11.823264122009277\n",
      "epoch 1400 loss=238.69479370117188\n",
      "New best validation epoch 1400 loss=11.799079895019531\n",
      "epoch 1600 loss=240.28598022460938\n",
      "New best validation epoch 1600 loss=11.791062355041504\n",
      "epoch 1800 loss=244.80471801757812\n",
      "New best validation epoch 1800 loss=11.778834342956543\n",
      "epoch 2000 loss=242.14874267578125\n",
      "New best validation epoch 2000 loss=11.776175498962402\n",
      "epoch 2200 loss=239.0486297607422\n",
      "New best validation epoch 2200 loss=11.769808769226074\n",
      "epoch 2400 loss=246.7767791748047\n",
      "New best validation epoch 2400 loss=11.76476001739502\n",
      "epoch 2600 loss=238.630859375\n",
      "New best validation epoch 2600 loss=11.764730453491211\n",
      "epoch 2800 loss=238.28842163085938\n",
      "New best validation epoch 2800 loss=11.759127616882324\n",
      "epoch 3000 loss=237.94000244140625\n",
      "New best validation epoch 3000 loss=11.755492210388184\n",
      "epoch 3200 loss=239.9930419921875\n",
      "New best validation epoch 3200 loss=11.751913070678711\n",
      "epoch 3400 loss=237.5251922607422\n",
      "New best validation epoch 3400 loss=11.751550674438477\n",
      "epoch 3600 loss=242.73117065429688\n",
      "epoch 3800 loss=238.26046752929688\n",
      "New best validation epoch 3800 loss=11.751508712768555\n",
      "epoch 4000 loss=242.91384887695312\n",
      "New best validation epoch 4000 loss=11.74946403503418\n",
      "epoch 4200 loss=240.44638061523438\n",
      "epoch 4400 loss=240.71823120117188\n",
      "epoch 4600 loss=240.9458465576172\n",
      "epoch 4800 loss=240.79510498046875\n",
      "epoch 4999 loss=243.96517944335938\n",
      "Finished training\n",
      "FINISHED 9 0 17.001602172851562\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873])\n",
      "epoch 0 loss=254.1705780029297\n",
      "New best validation epoch 0 loss=12.230117797851562\n",
      "epoch 200 loss=251.50213623046875\n",
      "New best validation epoch 200 loss=12.2012357711792\n",
      "epoch 400 loss=243.66033935546875\n",
      "New best validation epoch 400 loss=12.116432189941406\n",
      "epoch 600 loss=233.384765625\n",
      "New best validation epoch 600 loss=12.017949104309082\n",
      "epoch 800 loss=226.8588104248047\n",
      "New best validation epoch 800 loss=11.93847370147705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000 loss=228.4840545654297\n",
      "New best validation epoch 1000 loss=11.8819580078125\n",
      "epoch 1200 loss=222.27972412109375\n",
      "New best validation epoch 1200 loss=11.842070579528809\n",
      "epoch 1400 loss=227.6343994140625\n",
      "New best validation epoch 1400 loss=11.820513725280762\n",
      "epoch 1600 loss=231.6294403076172\n",
      "New best validation epoch 1600 loss=11.802450180053711\n",
      "epoch 1800 loss=226.4124755859375\n",
      "New best validation epoch 1800 loss=11.786818504333496\n",
      "epoch 2000 loss=228.27362060546875\n",
      "New best validation epoch 2000 loss=11.775635719299316\n",
      "epoch 2200 loss=224.88580322265625\n",
      "New best validation epoch 2200 loss=11.77078914642334\n",
      "epoch 2400 loss=228.44137573242188\n",
      "New best validation epoch 2400 loss=11.765636444091797\n",
      "epoch 2600 loss=232.54727172851562\n",
      "New best validation epoch 2600 loss=11.762094497680664\n",
      "epoch 2800 loss=224.5563201904297\n",
      "New best validation epoch 2800 loss=11.756467819213867\n",
      "epoch 3000 loss=234.4398193359375\n",
      "epoch 3200 loss=226.56259155273438\n",
      "epoch 3400 loss=230.5027313232422\n",
      "New best validation epoch 3400 loss=11.755873680114746\n",
      "epoch 3600 loss=226.4980926513672\n",
      "New best validation epoch 3600 loss=11.754054069519043\n",
      "epoch 3800 loss=226.62449645996094\n",
      "New best validation epoch 3800 loss=11.749411582946777\n",
      "epoch 4000 loss=226.524658203125\n",
      "New best validation epoch 4000 loss=11.743579864501953\n",
      "epoch 4200 loss=224.74459838867188\n",
      "epoch 4400 loss=226.03799438476562\n",
      "epoch 4600 loss=242.6793670654297\n",
      "epoch 4800 loss=221.1538543701172\n",
      "epoch 4999 loss=229.07818603515625\n",
      "Finished training\n",
      "FINISHED 9 1 16.65021514892578\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906])\n",
      "epoch 0 loss=253.7454833984375\n",
      "New best validation epoch 0 loss=12.230118751525879\n",
      "epoch 200 loss=249.47564697265625\n",
      "New best validation epoch 200 loss=12.151328086853027\n",
      "epoch 400 loss=233.8079071044922\n",
      "New best validation epoch 400 loss=11.936286926269531\n",
      "epoch 600 loss=225.60287475585938\n",
      "New best validation epoch 600 loss=11.73182201385498\n",
      "epoch 800 loss=211.05426025390625\n",
      "New best validation epoch 800 loss=11.62405014038086\n",
      "epoch 1000 loss=210.23956298828125\n",
      "New best validation epoch 1000 loss=11.583586692810059\n",
      "epoch 1200 loss=214.8829803466797\n",
      "New best validation epoch 1200 loss=11.565107345581055\n",
      "epoch 1400 loss=215.14822387695312\n",
      "New best validation epoch 1400 loss=11.547861099243164\n",
      "epoch 1600 loss=210.37229919433594\n",
      "New best validation epoch 1600 loss=11.53441047668457\n",
      "epoch 1800 loss=216.99490356445312\n",
      "New best validation epoch 1800 loss=11.52376937866211\n",
      "epoch 2000 loss=208.15118408203125\n",
      "epoch 2200 loss=226.1279754638672\n",
      "New best validation epoch 2200 loss=11.519582748413086\n",
      "epoch 2400 loss=219.4364013671875\n",
      "epoch 2600 loss=222.20068359375\n",
      "epoch 2800 loss=210.20236206054688\n",
      "epoch 3000 loss=215.42840576171875\n",
      "epoch 3200 loss=217.21575927734375\n",
      "epoch 3400 loss=217.6122283935547\n",
      "New best validation epoch 3400 loss=11.519341468811035\n",
      "epoch 3600 loss=218.77401733398438\n",
      "New best validation epoch 3600 loss=11.513528823852539\n",
      "epoch 3800 loss=216.5612030029297\n",
      "epoch 4000 loss=217.200439453125\n",
      "New best validation epoch 4000 loss=11.513471603393555\n",
      "epoch 4200 loss=214.37625122070312\n",
      "New best validation epoch 4200 loss=11.510339736938477\n",
      "epoch 4400 loss=218.60574340820312\n",
      "epoch 4600 loss=215.19577026367188\n",
      "epoch 4800 loss=217.9482421875\n",
      "epoch 4999 loss=218.57424926757812\n",
      "Finished training\n",
      "FINISHED 9 2 16.5699462890625\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692])\n",
      "epoch 0 loss=254.16241455078125\n",
      "New best validation epoch 0 loss=12.230117797851562\n",
      "epoch 200 loss=246.90048217773438\n",
      "New best validation epoch 200 loss=12.092653274536133\n",
      "epoch 400 loss=225.86106872558594\n",
      "New best validation epoch 400 loss=11.739089012145996\n",
      "epoch 600 loss=210.7662811279297\n",
      "New best validation epoch 600 loss=11.478022575378418\n",
      "epoch 800 loss=212.6293182373047\n",
      "New best validation epoch 800 loss=11.403322219848633\n",
      "epoch 1000 loss=210.72515869140625\n",
      "New best validation epoch 1000 loss=11.3671236038208\n",
      "epoch 1200 loss=208.0979461669922\n",
      "New best validation epoch 1200 loss=11.363699913024902\n",
      "epoch 1400 loss=204.59149169921875\n",
      "New best validation epoch 1400 loss=11.341412544250488\n",
      "epoch 1600 loss=208.70672607421875\n",
      "epoch 1800 loss=205.92454528808594\n",
      "epoch 2000 loss=199.72348022460938\n",
      "epoch 2200 loss=214.65243530273438\n",
      "epoch 2400 loss=200.65029907226562\n",
      "epoch 2600 loss=212.5575408935547\n",
      "epoch 2800 loss=201.84979248046875\n",
      "epoch 3000 loss=209.36923217773438\n",
      "epoch 3200 loss=207.41769409179688\n",
      "epoch 3400 loss=215.08480834960938\n",
      "epoch 3600 loss=208.58474731445312\n",
      "epoch 3800 loss=206.43313598632812\n",
      "epoch 4000 loss=213.3925018310547\n",
      "epoch 4200 loss=205.2627410888672\n",
      "epoch 4400 loss=209.36846923828125\n",
      "epoch 4600 loss=196.85606384277344\n",
      "epoch 4800 loss=209.19839477539062\n",
      "epoch 4999 loss=203.90594482421875\n",
      "Finished training\n",
      "FINISHED 9 3 16.43657875061035\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824])\n",
      "epoch 0 loss=254.06744384765625\n",
      "New best validation epoch 0 loss=12.23011589050293\n",
      "epoch 200 loss=240.60984802246094\n",
      "New best validation epoch 200 loss=12.058362007141113\n",
      "epoch 400 loss=219.86326599121094\n",
      "New best validation epoch 400 loss=11.66578483581543\n",
      "epoch 600 loss=200.0764617919922\n",
      "New best validation epoch 600 loss=11.428009033203125\n",
      "epoch 800 loss=196.53623962402344\n",
      "New best validation epoch 800 loss=11.345470428466797\n",
      "epoch 1000 loss=204.2357177734375\n",
      "New best validation epoch 1000 loss=11.320147514343262\n",
      "epoch 1200 loss=203.75531005859375\n",
      "New best validation epoch 1200 loss=11.316627502441406\n",
      "epoch 1400 loss=199.72836303710938\n",
      "New best validation epoch 1400 loss=11.314643859863281\n",
      "epoch 1600 loss=201.91693115234375\n",
      "New best validation epoch 1600 loss=11.301881790161133\n",
      "epoch 1800 loss=199.63192749023438\n",
      "New best validation epoch 1800 loss=11.298672676086426\n",
      "epoch 2000 loss=201.86456298828125\n",
      "New best validation epoch 2000 loss=11.28773021697998\n",
      "epoch 2200 loss=201.19793701171875\n",
      "New best validation epoch 2200 loss=11.276782989501953\n",
      "epoch 2400 loss=195.91183471679688\n",
      "epoch 2600 loss=197.81884765625\n",
      "epoch 2800 loss=194.24795532226562\n",
      "epoch 3000 loss=200.4873504638672\n",
      "epoch 3200 loss=192.63241577148438\n",
      "epoch 3400 loss=198.38671875\n",
      "epoch 3600 loss=205.3216552734375\n",
      "New best validation epoch 3600 loss=11.274514198303223\n",
      "epoch 3800 loss=194.49241638183594\n",
      "New best validation epoch 3800 loss=11.268351554870605\n",
      "epoch 4000 loss=199.85569763183594\n",
      "epoch 4200 loss=201.81930541992188\n",
      "epoch 4400 loss=201.20675659179688\n",
      "epoch 4600 loss=200.281494140625\n",
      "epoch 4800 loss=197.04324340820312\n",
      "epoch 4999 loss=198.96478271484375\n",
      "Finished training\n",
      "FINISHED 9 4 16.509395599365234\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391,  7163, 24709, 26016, 30422, 22503])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824,  7667, 13154, 31367, 13148,  2574])\n",
      "epoch 0 loss=254.85263061523438\n",
      "New best validation epoch 0 loss=12.230114936828613\n",
      "epoch 200 loss=238.1949920654297\n",
      "New best validation epoch 200 loss=12.017156600952148\n",
      "epoch 400 loss=208.91049194335938\n",
      "New best validation epoch 400 loss=11.63318157196045\n",
      "epoch 600 loss=190.11741638183594\n",
      "New best validation epoch 600 loss=11.514469146728516\n",
      "epoch 800 loss=195.41458129882812\n",
      "New best validation epoch 800 loss=11.4972505569458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000 loss=196.6074676513672\n",
      "New best validation epoch 1000 loss=11.477951049804688\n",
      "epoch 1200 loss=191.080810546875\n",
      "New best validation epoch 1200 loss=11.464021682739258\n",
      "epoch 1400 loss=190.1248016357422\n",
      "New best validation epoch 1400 loss=11.45648193359375\n",
      "epoch 1600 loss=189.5204315185547\n",
      "epoch 1800 loss=199.6747283935547\n",
      "epoch 2000 loss=186.25241088867188\n",
      "epoch 2200 loss=197.01718139648438\n",
      "epoch 2400 loss=189.42288208007812\n",
      "epoch 2600 loss=190.52102661132812\n",
      "New best validation epoch 2600 loss=11.44882583618164\n",
      "epoch 2800 loss=187.37173461914062\n",
      "epoch 3000 loss=203.82864379882812\n",
      "epoch 3200 loss=193.92306518554688\n",
      "epoch 3400 loss=192.29978942871094\n",
      "epoch 3600 loss=186.83641052246094\n",
      "epoch 3800 loss=195.7820281982422\n",
      "epoch 4000 loss=199.2202606201172\n",
      "New best validation epoch 4000 loss=11.4481782913208\n",
      "epoch 4200 loss=206.79954528808594\n",
      "New best validation epoch 4200 loss=11.446508407592773\n",
      "epoch 4400 loss=192.2963104248047\n",
      "epoch 4600 loss=194.98468017578125\n",
      "epoch 4800 loss=195.6222381591797\n",
      "epoch 4999 loss=198.28311157226562\n",
      "Finished training\n",
      "FINISHED 9 5 16.493091583251953\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391,  7163, 24709, 26016, 30422, 22503,\n",
      "        29086, 30498, 29083, 25986, 27181])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824,  7667, 13154, 31367, 13148,  2574,\n",
      "         4046, 28768, 32505, 30463,   334])\n",
      "epoch 0 loss=254.79649353027344\n",
      "New best validation epoch 0 loss=12.230116844177246\n",
      "epoch 200 loss=232.80197143554688\n",
      "New best validation epoch 200 loss=11.983920097351074\n",
      "epoch 400 loss=203.8324432373047\n",
      "New best validation epoch 400 loss=11.572680473327637\n",
      "epoch 600 loss=188.69114685058594\n",
      "New best validation epoch 600 loss=11.466634750366211\n",
      "epoch 800 loss=190.05374145507812\n",
      "New best validation epoch 800 loss=11.450068473815918\n",
      "epoch 1000 loss=182.2779541015625\n",
      "epoch 1200 loss=196.97030639648438\n",
      "New best validation epoch 1200 loss=11.42021369934082\n",
      "epoch 1400 loss=181.9540252685547\n",
      "epoch 1600 loss=185.0343017578125\n",
      "epoch 1800 loss=183.77366638183594\n",
      "epoch 2000 loss=183.07518005371094\n",
      "epoch 2200 loss=193.474853515625\n",
      "epoch 2400 loss=188.86459350585938\n",
      "epoch 2600 loss=187.1129150390625\n",
      "epoch 2800 loss=180.89085388183594\n",
      "New best validation epoch 2800 loss=11.418987274169922\n",
      "epoch 3000 loss=185.2640838623047\n",
      "epoch 3200 loss=185.73875427246094\n",
      "epoch 3400 loss=183.8331756591797\n",
      "epoch 3600 loss=177.25735473632812\n",
      "epoch 3800 loss=186.31121826171875\n",
      "epoch 4000 loss=193.07888793945312\n",
      "epoch 4200 loss=184.05160522460938\n",
      "epoch 4400 loss=185.14305114746094\n",
      "epoch 4600 loss=188.223876953125\n",
      "epoch 4800 loss=182.0791015625\n",
      "epoch 4999 loss=190.23458862304688\n",
      "Finished training\n",
      "FINISHED 9 6 16.49003028869629\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391,  7163, 24709, 26016, 30422, 22503,\n",
      "        29086, 30498, 29083, 25986, 27181,   304, 27188, 10503, 27439, 33918])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824,  7667, 13154, 31367, 13148,  2574,\n",
      "         4046, 28768, 32505, 30463,   334, 16638, 30149, 19715, 16665, 16637])\n",
      "epoch 0 loss=254.38125610351562\n",
      "New best validation epoch 0 loss=12.23011589050293\n",
      "epoch 200 loss=226.6235809326172\n",
      "New best validation epoch 200 loss=11.95186710357666\n",
      "epoch 400 loss=196.27120971679688\n",
      "New best validation epoch 400 loss=11.59967041015625\n",
      "epoch 600 loss=180.49607849121094\n",
      "New best validation epoch 600 loss=11.514376640319824\n",
      "epoch 800 loss=177.2505340576172\n",
      "New best validation epoch 800 loss=11.497076988220215\n",
      "epoch 1000 loss=186.02902221679688\n",
      "New best validation epoch 1000 loss=11.47927474975586\n",
      "epoch 1200 loss=182.23529052734375\n",
      "New best validation epoch 1200 loss=11.471863746643066\n",
      "epoch 1400 loss=186.45932006835938\n",
      "New best validation epoch 1400 loss=11.460509300231934\n",
      "epoch 1600 loss=184.6829071044922\n",
      "New best validation epoch 1600 loss=11.45018196105957\n",
      "epoch 1800 loss=186.33953857421875\n",
      "New best validation epoch 1800 loss=11.44546890258789\n",
      "epoch 2000 loss=185.4150848388672\n",
      "epoch 2200 loss=188.64578247070312\n",
      "epoch 2400 loss=182.93814086914062\n",
      "epoch 2600 loss=175.10079956054688\n",
      "epoch 2800 loss=185.43569946289062\n",
      "epoch 3000 loss=182.37197875976562\n",
      "epoch 3200 loss=190.97909545898438\n",
      "epoch 3400 loss=177.3140869140625\n",
      "epoch 3600 loss=182.15029907226562\n",
      "epoch 3800 loss=180.46420288085938\n",
      "epoch 4000 loss=183.06201171875\n",
      "epoch 4200 loss=186.78282165527344\n",
      "epoch 4400 loss=177.8452606201172\n",
      "epoch 4600 loss=180.45144653320312\n",
      "epoch 4800 loss=178.6350555419922\n",
      "New best validation epoch 4800 loss=11.443603515625\n",
      "epoch 4999 loss=184.9786834716797\n",
      "Finished training\n",
      "FINISHED 9 7 16.514400482177734\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391,  7163, 24709, 26016, 30422, 22503,\n",
      "        29086, 30498, 29083, 25986, 27181,   304, 27188, 10503, 27439, 33918,\n",
      "        17327,  2666,   991, 21858, 16937])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824,  7667, 13154, 31367, 13148,  2574,\n",
      "         4046, 28768, 32505, 30463,   334, 16638, 30149, 19715, 16665, 16637,\n",
      "        29848, 27195, 29238, 25267,  2263])\n",
      "epoch 0 loss=254.1680908203125\n",
      "New best validation epoch 0 loss=12.230114936828613\n",
      "epoch 200 loss=219.96018981933594\n",
      "New best validation epoch 200 loss=11.901391983032227\n",
      "epoch 400 loss=193.21258544921875\n",
      "New best validation epoch 400 loss=11.578520774841309\n",
      "epoch 600 loss=175.50686645507812\n",
      "New best validation epoch 600 loss=11.55288314819336\n",
      "epoch 800 loss=184.83384704589844\n",
      "New best validation epoch 800 loss=11.547131538391113\n",
      "epoch 1000 loss=181.93124389648438\n",
      "epoch 1200 loss=176.56936645507812\n",
      "New best validation epoch 1200 loss=11.537979125976562\n",
      "epoch 1400 loss=184.09140014648438\n",
      "epoch 1600 loss=183.5583953857422\n",
      "epoch 1800 loss=173.7984619140625\n",
      "epoch 2000 loss=178.79312133789062\n",
      "epoch 2200 loss=185.48245239257812\n",
      "epoch 2400 loss=183.45310974121094\n",
      "epoch 2600 loss=179.06143188476562\n",
      "epoch 2800 loss=183.52935791015625\n",
      "epoch 3000 loss=175.9673309326172\n",
      "epoch 3200 loss=182.28367614746094\n",
      "epoch 3400 loss=178.1742401123047\n",
      "epoch 3600 loss=175.2762908935547\n",
      "epoch 3800 loss=180.63986206054688\n",
      "epoch 4000 loss=176.4608154296875\n",
      "epoch 4200 loss=186.917236328125\n",
      "epoch 4400 loss=179.68099975585938\n",
      "epoch 4600 loss=184.9464111328125\n",
      "epoch 4800 loss=175.903564453125\n",
      "epoch 4999 loss=178.40557861328125\n",
      "Finished training\n",
      "FINISHED 9 8 16.465877532958984\n",
      "tensor([27310, 19059,   500, 11023, 22786, 21270, 18771, 26226, 23469, 19742,\n",
      "        31143, 31120, 11587, 16965, 18241, 20869,  6701,  9240, 32424, 24516,\n",
      "        32286, 29652, 29768,  6560, 20391,  7163, 24709, 26016, 30422, 22503,\n",
      "        29086, 30498, 29083, 25986, 27181,   304, 27188, 10503, 27439, 33918,\n",
      "        17327,  2666,   991, 21858, 16937, 18158, 34581, 22094, 14429, 25826])\n",
      "tensor([10626,  1160,  2162, 24294, 31147, 16627,  4874, 15002,  4870,  4873,\n",
      "        11046, 12157, 30151, 11027, 28906, 29870,  8867, 16303, 31215,  7692,\n",
      "        17878, 27947, 26908, 20701, 26824,  7667, 13154, 31367, 13148,  2574,\n",
      "         4046, 28768, 32505, 30463,   334, 16638, 30149, 19715, 16665, 16637,\n",
      "        29848, 27195, 29238, 25267,  2263,  6590, 21350,  3854,  6591,  6610])\n",
      "epoch 0 loss=254.4381561279297\n",
      "New best validation epoch 0 loss=12.23011302947998\n",
      "epoch 200 loss=222.20565795898438\n",
      "New best validation epoch 200 loss=11.833789825439453\n",
      "epoch 400 loss=179.5523681640625\n",
      "New best validation epoch 400 loss=11.50478458404541\n",
      "epoch 600 loss=172.37274169921875\n",
      "New best validation epoch 600 loss=11.493393898010254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 800 loss=176.1239013671875\n",
      "epoch 1000 loss=176.7345428466797\n",
      "epoch 1200 loss=168.77638244628906\n",
      "epoch 1400 loss=175.74093627929688\n",
      "New best validation epoch 1400 loss=11.485796928405762\n",
      "epoch 1600 loss=177.91424560546875\n",
      "epoch 1800 loss=177.60797119140625\n",
      "New best validation epoch 1800 loss=11.481185913085938\n",
      "epoch 2000 loss=178.95947265625\n",
      "epoch 2200 loss=177.913818359375\n",
      "epoch 2400 loss=169.84153747558594\n",
      "epoch 2600 loss=177.71893310546875\n",
      "epoch 2800 loss=183.9095458984375\n",
      "epoch 3000 loss=177.60198974609375\n",
      "epoch 3200 loss=172.5597381591797\n",
      "epoch 3400 loss=166.29017639160156\n",
      "epoch 3600 loss=176.08236694335938\n",
      "epoch 3800 loss=169.58804321289062\n",
      "epoch 4000 loss=183.30255126953125\n",
      "epoch 4200 loss=183.383056640625\n",
      "epoch 4400 loss=171.6507568359375\n",
      "epoch 4600 loss=183.5623779296875\n",
      "epoch 4800 loss=174.0502166748047\n",
      "epoch 4999 loss=177.37149047851562\n",
      "Finished training\n",
      "FINISHED 9 9 16.578147888183594\n",
      "ALL DONE\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "topRegions = np.zeros((10,10,2,5,2)) # Group, N, para, top5, {index, value}\n",
    "topRMSE = np.zeros((10,10,3)) # Group, N, {train, val, test}\n",
    "\n",
    "for grp in range(10):\n",
    "    trainIdcs = groups[grp][0][0:496]\n",
    "    validIdcs = groups[grp][0]\n",
    "    testIdcs = groups[grp][1]\n",
    "    \n",
    "    for n in range(10):\n",
    "        if n == 0:\n",
    "            varArr = wrt_t[trainIdcs]\n",
    "            varArr = [varArr, varArr]\n",
    "            regs = [[], []]\n",
    "        else:\n",
    "            varArr = [ynback, yemoid]\n",
    "\n",
    "        for i,var in enumerate(varArr):\n",
    "            idcs = torch.argsort(var)\n",
    "            mat = torch.cdist(var[idcs].unsqueeze(1), var[idcs].unsqueeze(1))\n",
    "            mat = torch.mean(mat) - mat\n",
    "            \n",
    "            step = 200\n",
    "            Y = [nback_p_t[trainIdcs][idcs], emoid_p_t[trainIdcs][idcs]][i]\n",
    "\n",
    "            parts = []\n",
    "            for j in range(0,40000,step):\n",
    "                if j >= Y.shape[1]:\n",
    "                    break\n",
    "                part = Y[:,j:j+step]\n",
    "                corrSim = torch.einsum('ab,db->adb', part, part)\n",
    "                corr2 = torch.einsum('adb,ad->b', corrSim, mat)\n",
    "                parts.append(corr2.detach().cpu().numpy())\n",
    "\n",
    "            parts = np.concatenate(parts)\n",
    "            idcs = np.argsort(parts)\n",
    "            \n",
    "#             idcs = selectSome(idcs, topRegions[grp,:,i,0,0].flatten(), 5)\n",
    "            idcs = selectSome(idcs, np.concatenate([topRegions[grp,:,i,0,0].flatten(), top10[i]]), 5)\n",
    "            regs[i] += [idx for idx in idcs[:5]]\n",
    "            topRegions[grp,n,i,:,0] = idcs[:5]\n",
    "            topRegions[grp,n,i,:,1] = parts[idcs[:5]]\n",
    "#             r = None\n",
    "#             count = 0\n",
    "#             while r is None or r in regs[i]:\n",
    "#                 if count == 1:\n",
    "#                     break\n",
    "#                 r = random.randint(0,arith(263)-1)\n",
    "#                 if r not in regs[i]:\n",
    "#                     count += 1\n",
    "#                     regs[i] += [r]\n",
    "#             topRegions[grp,n,i,4,0] = r\n",
    "            \n",
    "        rnback = torch.tensor(regs[0], dtype=int)\n",
    "        remoid = torch.tensor(regs[1], dtype=int)\n",
    "\n",
    "        X0 = nback_p_t[:,rnback]\n",
    "        X1 = emoid_p_t[:,remoid]\n",
    "        Xreg = torch.stack([X0,X1], dim=1)\n",
    "\n",
    "        print(rnback)\n",
    "        print(remoid)\n",
    "\n",
    "        nEpochs = 5000\n",
    "        pPeriod = 200\n",
    "        thresh = 100\n",
    "\n",
    "        sim = LatSim(2, Xreg, 0.5, 0.2) # 0.2 wrat, 0.1 other\n",
    "        optim = torch.optim.Adam(sim.parameters(), lr=1e-4, weight_decay=1e-4) \n",
    "\n",
    "        Xt = Xreg[trainIdcs]\n",
    "        Xv = Xreg[validIdcs]\n",
    "\n",
    "        trainLoss = []\n",
    "        validLoss = []\n",
    "\n",
    "        vIdcs1 = np.arange(496,len(validIdcs))\n",
    "        vIdcs2 = validIdcs[496:]\n",
    "        vIdcs3 = np.concatenate([testIdcs, vIdcs2])\n",
    "\n",
    "        var = wrt_t\n",
    "\n",
    "        for epoch in range(nEpochs):\n",
    "            optim.zero_grad()\n",
    "            res, _ = sim(Xt, var[trainIdcs])\n",
    "            avg = torch.mean(torch.stack(res), dim=0)\n",
    "            loss = 0\n",
    "            for r in res + [avg]:\n",
    "                loss += mseLoss(r, var[trainIdcs])\n",
    "            loss = torch.stack([loss/(len(res)+1)])\n",
    "            torch.sum(loss).backward()\n",
    "            optim.step()\n",
    "            if epoch % pPeriod == 0 or epoch == nEpochs-1 or torch.all(loss[0:3] < thresh):\n",
    "                print(f'epoch {epoch} loss={(float(loss))}')\n",
    "                lossV = validate(sim, Xv, var[validIdcs], vIdcs1)\n",
    "                if len(validLoss) == 0 or lossV < min(validLoss):\n",
    "                    print(f'New best validation epoch {epoch} loss={lossV}')\n",
    "                    topRMSE[grp,n,1] = lossV\n",
    "                    torch.save(sim.state_dict(), '../../Work/LatentSim/sim.pyt')\n",
    "                validLoss.append(lossV)\n",
    "                if torch.all(loss[0:3] < thresh):\n",
    "                    print('Early stopping')\n",
    "                    break\n",
    "        \n",
    "        topRMSE[grp,n,0] = torch.sum(loss).detach().cpu().numpy()\n",
    "        print('Finished training')\n",
    "\n",
    "        sim.load_state_dict(torch.load('../../Work/LatentSim/sim.pyt'))\n",
    "        loss = validate(sim, Xreg, var, testIdcs)\n",
    "        topRMSE[grp,n,2] = loss\n",
    "        print(f'FINISHED {grp} {n} {loss}')\n",
    "        \n",
    "        ynback = var[trainIdcs]-res[0]\n",
    "        yemoid = var[trainIdcs]-res[1]\n",
    "        yavg = var[trainIdcs]-avg\n",
    "\n",
    "#         r0 = F.one_hot(torch.argmax(res[0], dim=1), 2)\n",
    "#         r1 = F.one_hot(torch.argmax(res[1], dim=1), 2)\n",
    "#         ra = F.one_hot(torch.argmax(avg, dim=1), 2)\n",
    "\n",
    "#         ynback = var[trainIdcs]-r0\n",
    "#         yemoid = var[trainIdcs]-r1\n",
    "#         yavg = var[trainIdcs]-ra\n",
    "        \n",
    "print('ALL DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "76dab852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.70978012 15.66539612 15.62388382 15.66486616 15.63380299 15.59614716\n",
      " 15.54709339 15.66043367 15.53754911 15.60063553]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(topRMSE[:,:,2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d4e1067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/.conda/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = (np.sum(topRMSE[:,:,0], axis=0)/10)**0.5/12\n",
    "bestVal = np.sum(topRMSE[:,:,1], axis=0)/10/12\n",
    "testNoVal = np.sum(topRMSE[:,:,2], axis=0)/10/12\n",
    "\n",
    "idcs = 5*np.arange(1,11)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "ax.plot(idcs, train, label='Training', marker='.', markersize=10)\n",
    "ax.plot(idcs, bestVal, label='Validation', marker='.', markersize=10)\n",
    "ax.plot(idcs, testNoVal, label='Test', marker='.', markersize=10)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Age Prediction Random Feature Selection')\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_ylabel('RMSE (Age)')\n",
    "\n",
    "fig.show()\n",
    "fig.savefig('../../Work/LatentSim/RandomValidation50FeatDP5DP2.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96f91db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "with open('../../Work/LatentSim/Greed/WratValidation2ndBest50FeatDP5DP2.pkl', 'wb') as f:\n",
    "    pickle.dump([topRegions, topRMSE], f)\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34c98496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "allReg = topRegions[:,0:3,:,4,0].flatten()\n",
    "\n",
    "print(len(allReg))\n",
    "print(len(np.unique(allReg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4db4d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../Work/LatentSim/Greed/WratValidationDP2.pkl', 'rb') as f:\n",
    "    topRegionsBest, topRMSEBest = pickle.load(f)\n",
    "    \n",
    "print(topRMSEBest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1cddea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  106.,   192.,   238.,   286.,   291.,   292.,   329.,   498.,\n",
      "         745.,   902.,   905.,  1136.,  1258.,  2005.,  2162.,  2651.,\n",
      "        2912.,  3006.,  3126.,  3241.,  3855.,  3861.,  3892.,  4052.,\n",
      "        4055.,  4234.,  4243.,  4744.,  4759.,  5127.,  5292.,  6313.,\n",
      "        6320.,  6390.,  6413.,  6611.,  6624.,  6695.,  6726.,  6727.,\n",
      "        7083.,  7152.,  7165.,  7207.,  7214.,  7253.,  7461.,  7569.,\n",
      "        7585.,  7650.,  7653.,  7666.,  7667.,  7913.,  8817.,  9024.,\n",
      "        9081.,  9339.,  9719.,  9966., 10084., 10407., 10587., 11022.,\n",
      "       11030., 11041., 11296., 11699., 11911., 12279., 12547., 13508.,\n",
      "       15007., 15014., 15684., 15701., 16088., 16601., 16602., 16706.,\n",
      "       16760., 17031., 17099., 17330., 17331., 17364., 17377., 17388.,\n",
      "       17431., 17442., 17454., 17474., 17485., 17487., 17488., 18049.,\n",
      "       18075., 18382., 18523., 18825., 18835., 19017., 19060., 19070.,\n",
      "       19145., 19159., 19302., 19342., 19663., 19683., 19708., 19730.,\n",
      "       19824., 19853., 19855., 19861., 19868., 19996., 20010., 20017.,\n",
      "       20186., 20371., 20506., 20542., 20878., 20940., 21041., 21221.,\n",
      "       21321., 21335., 21607., 22510., 22532., 22544., 22615., 22700.,\n",
      "       22742., 22767., 22793., 22835., 22841., 22921., 23077., 23096.,\n",
      "       23397., 23722., 23780., 23796., 23799., 24484., 24713., 24730.,\n",
      "       24832., 24844., 24889., 25312., 25329., 25490., 26438., 26441.,\n",
      "       26486., 26608., 26815., 26827., 27065., 27127., 27252., 28169.,\n",
      "       28173., 28716., 28734., 29061., 29065., 29072., 29088., 29091.,\n",
      "       29106., 29267., 29547., 29548., 29570., 29706., 29840., 29908.,\n",
      "       30063., 30131., 30413., 30444., 30753., 31047., 31188., 31217.,\n",
      "       31344., 31468., 31930., 32118., 32145., 32184., 32232., 32355.,\n",
      "       32403., 32543., 32901., 33174., 33203., 33257., 33798.]), array([  141.,   207.,   227.,   228.,   404.,   411.,   433.,   443.,\n",
      "         533.,   710.,  1047.,  1136.,  1147.,  1229.,  1318.,  1567.,\n",
      "        1629.,  1671.,  1695.,  1715.,  1730.,  1747.,  1760.,  1796.,\n",
      "        1960.,  1967.,  2041.,  2042.,  2071.,  2088.,  2285.,  3154.,\n",
      "        3315.,  3748.,  3842.,  4970.,  5408.,  5427.,  5529.,  5681.,\n",
      "        6047.,  6089.,  6170.,  6640.,  7103.,  7112.,  7218.,  7350.,\n",
      "        7654.,  7967.,  7996.,  8729.,  8849.,  8955.,  9029.,  9040.,\n",
      "        9044.,  9366.,  9845.,  9966.,  9981.,  9990., 10407., 10650.,\n",
      "       10980., 11030., 11041., 11061., 11171., 12141., 12871., 12897.,\n",
      "       12928., 13312., 13741., 14130., 14322., 14533., 14923., 15007.,\n",
      "       15300., 15593., 15980., 16138., 16488., 16662., 16778., 16791.,\n",
      "       16854., 16858., 16904., 16922., 16937., 17562., 17881., 17914.,\n",
      "       18056., 18103., 18114., 18118., 18228., 18241., 18546., 18961.,\n",
      "       19160., 19303., 19670., 19700., 19712., 19842., 20522., 20525.,\n",
      "       20542., 20550., 20562., 20698., 20699., 21612., 22029., 22660.,\n",
      "       22814., 23546., 23702., 23983., 24150., 24579., 25384., 25679.,\n",
      "       25827., 25935., 26226., 26287., 26292., 26697., 26895., 26919.,\n",
      "       27072., 27221., 27550., 27831., 27839., 28047., 28143., 28170.,\n",
      "       28176., 28370., 28391., 28802., 28860., 28910., 28919., 28921.,\n",
      "       28985., 29036., 29537., 29720., 29847., 29852., 29867., 29871.,\n",
      "       30138., 30317., 30375., 30439., 30547., 30561., 30708., 30797.,\n",
      "       31068., 31072., 31139., 31315., 31319., 31327., 31339., 31343.,\n",
      "       31344., 31348., 31350., 31373., 31376., 31646., 31716., 31729.,\n",
      "       31785., 31787., 32021., 32024., 32164., 32511., 33002., 33203.,\n",
      "       33212., 33723., 33810., 33972., 34279., 34336., 34414., 34480.,\n",
      "       34504., 34563.])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "top10nback = np.unique(topRegionsBest[:,:,0,:,0].flatten())\n",
    "top10emoid = np.unique(topRegionsBest[:,:,1,:,0].flatten())\n",
    "\n",
    "top10 = [top10nback, top10emoid]\n",
    "\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c41f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8108e+04 3.4569e+04 1.5707e+04 8.3950e+03 1.5628e+04 9.3070e+03\n",
      " 3.7970e+03 4.1260e+03 4.1250e+03 2.2652e+04 1.9790e+03 1.9650e+03\n",
      " 1.8229e+04 2.0070e+03 1.9640e+03 1.6126e+04 3.4641e+04 1.3537e+04\n",
      " 2.2714e+04 3.1933e+04 2.2714e+04 1.7460e+03 1.3423e+04 2.9602e+04\n",
      " 1.7261e+04 1.8249e+04 1.8270e+04 2.7080e+03 1.8296e+04 1.3300e+02\n",
      " 1.8260e+04 3.0060e+04 2.9769e+04 1.7380e+03 1.4190e+03 2.1128e+04\n",
      " 3.2208e+04 2.1163e+04 3.2999e+04 1.4350e+03 1.4391e+04 2.9961e+04\n",
      " 1.5702e+04 1.9165e+04 2.4887e+04 2.7117e+04 3.2999e+04 4.2900e+02\n",
      " 2.9769e+04 2.3132e+04 6.4600e+02 2.2868e+04 1.1757e+04 2.0851e+04\n",
      " 1.0076e+04 6.7800e+02 7.1500e+02 1.6966e+04 2.8503e+04 6.0700e+02\n",
      " 2.2330e+04 1.6998e+04 5.2700e+02 6.4500e+02 2.2795e+04 7.4700e+02\n",
      " 9.3290e+03 5.2700e+02 6.4500e+02 2.8140e+03 6.1600e+02 6.4700e+02\n",
      " 6.4000e+02 6.4500e+02 7.0700e+02 6.1200e+02 6.1100e+02 2.3633e+04\n",
      " 1.2320e+03 2.2436e+04 2.0736e+04 1.3505e+04 2.0738e+04 1.5886e+04\n",
      " 2.7959e+04 2.0393e+04 1.9498e+04 2.0994e+04 2.0996e+04 2.0757e+04\n",
      " 1.8253e+04 2.0760e+04 2.0757e+04 2.8380e+03 1.8355e+04 9.1800e+02\n",
      " 1.7063e+04 9.0100e+02 8.9900e+02 9.8800e+02 1.8108e+04 1.5930e+04\n",
      " 1.1757e+04 1.5628e+04 1.0076e+04 3.7970e+03 9.3070e+03 4.1260e+03\n",
      " 1.2588e+04 4.3050e+03 3.1519e+04 2.0318e+04 2.2748e+04 3.4232e+04\n",
      " 3.4067e+04 9.0630e+03 2.0089e+04 1.5734e+04 2.2585e+04 3.1525e+04\n",
      " 1.9160e+04 2.2714e+04 2.2586e+04 2.2585e+04 1.9161e+04 2.2649e+04\n",
      " 1.4388e+04 1.5694e+04 1.5689e+04 1.3430e+03 2.1948e+04 1.4388e+04\n",
      " 3.1530e+04 2.0878e+04 2.0932e+04 2.2758e+04 2.2868e+04 1.3510e+04\n",
      " 1.3430e+03 3.2621e+04 1.9039e+04 1.8550e+03 3.2494e+04 1.9110e+03\n",
      " 1.8820e+03 2.0032e+04 3.1170e+04 1.8267e+04 9.3850e+03 6.1870e+03\n",
      " 6.4600e+02 1.8108e+04 3.4569e+04 1.7927e+04 8.3950e+03 6.7800e+02\n",
      " 7.1500e+02 7.7700e+02 6.0700e+02 6.8400e+02 2.2330e+04 2.8140e+03\n",
      " 2.5610e+03 2.5500e+03 2.2795e+04 2.5610e+03 2.5110e+03 2.4550e+03\n",
      " 2.5120e+03 2.5140e+03 3.4341e+04 6.1200e+02 6.4700e+02 2.0048e+04\n",
      " 6.1600e+02 2.0516e+04 3.1221e+04 1.1964e+04 2.0603e+04 2.5746e+04\n",
      " 1.6094e+04 2.0736e+04 1.7437e+04 3.1155e+04 5.6000e+02 3.1209e+04\n",
      " 2.0600e+03 1.7387e+04 5.2910e+03 7.4790e+03 1.8432e+04 7.2460e+03\n",
      " 1.8429e+04 1.7727e+04 6.0660e+03 2.5790e+04 1.9823e+04 2.2481e+04\n",
      " 2.0600e+03 2.0580e+03 2.0851e+04 1.8227e+04 1.8108e+04 1.1112e+04\n",
      " 8.3950e+03 4.1260e+03 9.5310e+03 9.3070e+03 7.4790e+03 1.3184e+04\n",
      " 2.0807e+04 1.3638e+04 2.1463e+04 2.1948e+04 2.0768e+04 2.1463e+04\n",
      " 1.3638e+04 2.1948e+04 2.0768e+04 2.1017e+04 2.0776e+04 2.1948e+04\n",
      " 2.4559e+04 2.0768e+04 2.0806e+04 2.0768e+04 2.4559e+04 2.1671e+04\n",
      " 2.1948e+04 2.0806e+04 3.2434e+04 2.0854e+04 2.1869e+04 1.7557e+04\n",
      " 2.2083e+04 2.0764e+04 2.2309e+04 2.0854e+04 2.1509e+04 1.6617e+04\n",
      " 1.5707e+04 1.5705e+04 1.9571e+04 2.9572e+04 2.0426e+04 2.7570e+03\n",
      " 2.6990e+03 1.7557e+04 4.9590e+03 2.7000e+03 1.8108e+04 6.4600e+02\n",
      " 1.5707e+04 1.2266e+04 1.8227e+04 3.7970e+03 1.2588e+04 9.3070e+03\n",
      " 8.6290e+03 4.3050e+03 3.1519e+04 1.1678e+04 3.1042e+04 3.1552e+04\n",
      " 1.2528e+04 2.8716e+04 3.0151e+04 2.2184e+04 2.2179e+04 1.8355e+04\n",
      " 1.6256e+04 2.9843e+04 1.5677e+04 1.8267e+04 4.4100e+03 2.7200e+02\n",
      " 1.0000e+01 2.9550e+04 4.5900e+02 2.8526e+04 3.2372e+04 2.6573e+04\n",
      " 3.2026e+04 3.2587e+04 2.9550e+04 3.2026e+04 2.9592e+04 2.7868e+04\n",
      " 1.3526e+04 2.6341e+04 2.9550e+04 3.4323e+04 3.1775e+04 1.8529e+04\n",
      " 2.6680e+03 3.3296e+04 1.8218e+04 8.6550e+03 1.4433e+04 1.4832e+04\n",
      " 1.7927e+04 1.8108e+04 8.3950e+03 1.1112e+04 2.0851e+04 3.3540e+04\n",
      " 1.3974e+04 2.3192e+04 3.3341e+04 3.8750e+03 1.8254e+04 1.8249e+04\n",
      " 4.1510e+03 1.4029e+04 1.8251e+04 3.4448e+04 1.6849e+04 1.9331e+04\n",
      " 1.8618e+04 1.8880e+03 2.2828e+04 1.7915e+04 2.5963e+04 1.7991e+04\n",
      " 2.5946e+04 3.1541e+04 1.8770e+04 2.6055e+04 2.0137e+04 2.7561e+04\n",
      " 3.1489e+04 3.3245e+04 3.0699e+04 3.4657e+04 3.1049e+04 3.4657e+04\n",
      " 3.1049e+04 3.1048e+04 2.0460e+03 1.2532e+04 3.4569e+04 1.5707e+04\n",
      " 9.4890e+03 1.8618e+04 8.1800e+03 3.1388e+04 3.1321e+04 1.9072e+04\n",
      " 2.5790e+03 5.3100e+02 1.2266e+04 6.4600e+02 2.2868e+04 1.8108e+04\n",
      " 1.7927e+04 1.3311e+04 1.3926e+04 1.1196e+04 2.9976e+04 1.3722e+04\n",
      " 3.1265e+04 2.9976e+04 1.3926e+04 1.1196e+04 1.3722e+04 1.6104e+04\n",
      " 1.4532e+04 1.3926e+04 2.4762e+04 3.1279e+04 1.3926e+04 3.1279e+04\n",
      " 2.5790e+03 2.4762e+04 2.9976e+04 1.3104e+04 1.4532e+04 3.1279e+04\n",
      " 2.9976e+04 1.2298e+04 1.4532e+04 3.1279e+04 2.9976e+04 1.2298e+04\n",
      " 3.1251e+04 2.5790e+03 1.2298e+04 3.1279e+04 2.9976e+04 3.1289e+04\n",
      " 3.1279e+04 2.2790e+04 3.3296e+04 1.2308e+04 2.9172e+04 2.2790e+04\n",
      " 3.3296e+04 1.2308e+04 2.9172e+04 1.8108e+04 8.3950e+03 1.8227e+04\n",
      " 1.8108e+04 6.4600e+02 3.4569e+04 4.1300e+03 4.1260e+03 9.5310e+03\n",
      " 4.2790e+03 4.1220e+03 5.8060e+03 6.7710e+03 5.0980e+03 6.7560e+03\n",
      " 9.5250e+03 9.5250e+03 5.5900e+02 6.7710e+03 7.7050e+03 5.0980e+03\n",
      " 6.7710e+03 7.7050e+03 5.5810e+03 5.0980e+03 5.5900e+02 5.5900e+02\n",
      " 5.5300e+02 9.3010e+03 7.4730e+03 1.8108e+04 5.5300e+02 2.5940e+04\n",
      " 8.4980e+03 7.8400e+02 1.9168e+04 5.4600e+02 5.4900e+02 7.8400e+02\n",
      " 5.4300e+02 5.4000e+02 9.0820e+03 7.5010e+03 3.8900e+03 9.4040e+03\n",
      " 9.2410e+03 1.8886e+04 2.5678e+04 1.9063e+04 2.0673e+04 2.6254e+04\n",
      " 6.4600e+02 1.8108e+04 1.5707e+04 2.0610e+03 1.5692e+04 1.9525e+04\n",
      " 6.7800e+02 2.8140e+03 7.1500e+02 6.0700e+02 6.7800e+02 6.4500e+02\n",
      " 6.4700e+02 1.9927e+04 6.1500e+02 6.4700e+02 6.4500e+02 2.4320e+03\n",
      " 5.3200e+02 6.1600e+02 5.3200e+02 6.4500e+02 5.2700e+02 6.1600e+02\n",
      " 6.4000e+02 2.0851e+04 2.0854e+04 2.3315e+04 1.1757e+04 1.1112e+04\n",
      " 2.9510e+03 2.9490e+03 1.6602e+04 3.7920e+03 2.3466e+04 1.1450e+04\n",
      " 1.5364e+04 4.2820e+03 1.5560e+04 9.5350e+03 2.9572e+04 2.8380e+03\n",
      " 3.0226e+04 3.0397e+04 6.7000e+01 2.9650e+03 3.1780e+03 7.3110e+03\n",
      " 2.9570e+03 2.9830e+03]\n",
      "[  133.   429.  1419.  1435.  1738.  1746.  1964.  1965.  1979.  2007.\n",
      "  2708.  3797.  4125.  4126.  8395.  9307. 13423. 13537. 14391. 15628.\n",
      " 15702. 15707. 16126. 17261. 18108. 18229. 18249. 18260. 18270. 18296.\n",
      " 19165. 21128. 21163. 22652. 22714. 23132. 24887. 27117. 29602. 29769.\n",
      " 29961. 30060. 31933. 32208. 32999. 34569. 34641.]\n",
      "[  942.   954.  1043.  3870.  3879.  4045.  4047.  4048.  6534.  6539.\n",
      "  7969. 12332. 12727. 13377. 13992. 17880. 18106. 18170. 18225. 18227.\n",
      " 18228. 18229. 18250. 18657. 18659. 23763. 25461. 25596. 26272. 26640.\n",
      " 26709. 27056. 28606. 28677. 28716. 29419. 29887. 31728. 31882. 32044.\n",
      " 34103. 34170.]\n",
      "2983.0\n",
      "2983.0\n"
     ]
    }
   ],
   "source": [
    "flt = topRegionsBest[:,:,0,:,0].flatten()\n",
    "print(flt)\n",
    "print(top10nback)\n",
    "print(top10emoid)\n",
    "print(select1(flt, top10nback))\n",
    "print(select1(flt, top10emoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5dac183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectSome(idcs, blacklist, n):\n",
    "    lst = []\n",
    "    for i in range(-1,-500,-1):\n",
    "        if idcs[i] not in blacklist:\n",
    "            lst.append(idcs[i])\n",
    "            if len(lst) == n:\n",
    "                return lst\n",
    "            \n",
    "selectSome([0, 10372], top10[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8611d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
