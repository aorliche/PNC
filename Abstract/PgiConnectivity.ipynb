{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9051684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load meta dict\n",
    "\n",
    "with open('../../PNC/AllSubjectsMeta.bin', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "# Load rest subject ids and splits\n",
    "\n",
    "with open('../../Work/Abstract/PaperBin/AllThreeSplit.bin', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "    subids = splits['allThreeYesWrat']\n",
    "    groups = splits['groups']\n",
    "    \n",
    "print(len(subids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58d6264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "subidsNp = np.array(subids)\n",
    "\n",
    "# Load timeseries\n",
    "\n",
    "def loadSeries(prefix, para, idx):\n",
    "    with open('{:}/{:}_fmri_power264/timeseries/{:}.bin'.format(prefix, para, idx), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "rest_ts = [loadSeries('../../PNC', 'rest', meta[subid]['rest']) for subid in subidsNp]\n",
    "nback_ts = [loadSeries('../../PNC', 'nback', meta[subid]['nback']) for subid in subidsNp]\n",
    "emoid_ts = [loadSeries('../../PNC', 'emoid', meta[subid]['emoid']) for subid in subidsNp]\n",
    "\n",
    "print('Loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7d46c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalizeSubjects(subjects):\n",
    "    for i in range(len(subjects)):\n",
    "        subj = subjects[i]\n",
    "        subj -= np.mean(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        subj /= np.std(subj, axis=1, keepdims=True)@np.ones([1,subj.shape[1]])\n",
    "        if np.sum(np.isnan(subj)) > 0:\n",
    "            print(i)\n",
    "        if np.sum(np.isinf(subj)) > 0:\n",
    "            print(i)\n",
    "\n",
    "normalizeSubjects(rest_ts)\n",
    "normalizeSubjects(nback_ts)\n",
    "normalizeSubjects(emoid_ts)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e738b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([593, 264, 210])\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Convert raw timeseries to torch\n",
    "\n",
    "import torch\n",
    "\n",
    "rest_t = torch.from_numpy(np.stack(rest_ts)).float().cuda()\n",
    "nback_t = torch.from_numpy(np.stack(nback_ts)).float().cuda()\n",
    "emoid_t = torch.from_numpy(np.stack(emoid_ts)).float().cuda()\n",
    "\n",
    "print(emoid_t.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1384fdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 322\n",
      "[[223   1   0]\n",
      " [190   0   1]\n",
      " [197   0   1]\n",
      " [145   1   0]\n",
      " [148   0   1]\n",
      " [142   0   1]\n",
      " [123   1   0]\n",
      " [176   1   0]\n",
      " [129   0   1]\n",
      " [173   1   0]]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors (right now just ages, maleness, and femaless)\n",
    "\n",
    "males = 0\n",
    "females = 0\n",
    "\n",
    "X_all = []\n",
    "for subid in subidsNp:\n",
    "    subj = meta[subid]\n",
    "    maleness = 1 if subj['meta']['Gender'] == 'M' else 0\n",
    "    femaleness = 1 if maleness == 0 else 0\n",
    "    feat = np.array([subj['meta']['AgeInMonths'], maleness, femaleness])\n",
    "    X_all.append(feat)\n",
    "    if maleness == 1:\n",
    "        males += 1\n",
    "    if femaleness == 1:\n",
    "        females += 1\n",
    "X_all = np.vstack(X_all)\n",
    "\n",
    "print(f'{males} {females}')\n",
    "print(X_all[10:20])\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c83c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "wratDict = dict()\n",
    "\n",
    "with open('../../PNC/wrat.csv', 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        wratDict[line[0]] = {'raw': line[2], 'std': line[3]}\n",
    "\n",
    "wrat = []\n",
    "\n",
    "for key in subids:\n",
    "    wrat.append(float(wratDict[str(key)]['std']))\n",
    "    \n",
    "wrat = np.array(wrat)\n",
    "wrat_t = torch.from_numpy(wrat).float().cuda()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc1d483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def arith(n):\n",
    "    return int(n*(n+1)/2)\n",
    "\n",
    "def mask(e):\n",
    "    return e - torch.diag(torch.diag(e))\n",
    "\n",
    "class MiniPgi(nn.Module):\n",
    "    def __init__(self, w, nRoi, nTgts, dp=0.5, relu=0.1):\n",
    "        super(MiniPgi, self).__init__()\n",
    "        if type(w) == int:\n",
    "            w = nTgts*[w]\n",
    "        self.masks = []\n",
    "        self.relu = []\n",
    "        for i in range(nTgts):\n",
    "            self.masks.append(nn.Parameter(\n",
    "                0.0001*torch.ones(nRoi,w[i]).float().cuda()\n",
    "                +0.00001*torch.randn(nRoi,w[i]).float().cuda()\n",
    "            ))\n",
    "            rel = relu if type(relu) == float or type(relu) == int else relu[i]\n",
    "            self.relu.append(nn.LeakyReLU(negative_slope=rel))\n",
    "        self.dp = nn.Dropout(p=dp)\n",
    "        \n",
    "    def getLatentsAndEdges(self, x, idx):\n",
    "        y = torch.einsum('abc,bd->acd', x, self.masks[i])\n",
    "        y = torch.einsum('abc,abd->acd', y, y)\n",
    "        y = self.relu[idx](y)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        e = y@y.T\n",
    "        return y,e\n",
    "    \n",
    "    def forward(self, x, age=None, gender=None, wrat=None):\n",
    "        x = self.dp(x)\n",
    "        lbls = [age, gender, wrat]\n",
    "        res = []\n",
    "        for i in range(len(self.masks)):\n",
    "            _, e = self.getLatentsAndEdges(x, i)\n",
    "            idcs = torch.logical_not(torch.any(lbls[i], dim=1))\n",
    "            e[:,idcs] = 0\n",
    "            e = mask(e)\n",
    "#             s = torch.sum(e, dim=1)\n",
    "#             e = e/s.unsqueeze(1)\n",
    "            res.append(e@lbls[i])\n",
    "        return res\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a81195d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=(1862026.75, 0.7026510238647461, 629329.875)\n",
      "epoch 200 loss=(7189.4296875, 0.6940171122550964, 1631.5980224609375)\n",
      "epoch 400 loss=(6163.46826171875, 0.6938968896865845, 1364.99560546875)\n",
      "epoch 600 loss=(3810.91796875, 0.6932421922683716, 807.3846435546875)\n",
      "epoch 800 loss=(2073.426513671875, 0.6917603015899658, 504.9845275878906)\n",
      "epoch 1000 loss=(1933.379638671875, 0.6910960078239441, 516.8662109375)\n",
      "epoch 1200 loss=(1842.461669921875, 0.6909302473068237, 515.888671875)\n",
      "epoch 1400 loss=(1715.385498046875, 0.6912573575973511, 513.0823364257812)\n",
      "epoch 1600 loss=(1623.427734375, 0.691146969795227, 514.0126342773438)\n",
      "epoch 1800 loss=(1539.9017333984375, 0.6911959052085876, 523.5892333984375)\n",
      "epoch 2000 loss=(1495.183837890625, 0.6914143562316895, 524.4559326171875)\n",
      "epoch 2200 loss=(1421.0830078125, 0.6914901733398438, 517.8359375)\n",
      "epoch 2400 loss=(1383.4007568359375, 0.6915210485458374, 494.3544921875)\n",
      "epoch 2600 loss=(1302.918212890625, 0.6916968822479248, 515.7525634765625)\n",
      "epoch 2800 loss=(1231.12353515625, 0.6914023756980896, 519.0006713867188)\n",
      "epoch 3000 loss=(1190.0015869140625, 0.6919930577278137, 492.0147705078125)\n",
      "epoch 3200 loss=(1107.249755859375, 0.6917832493782043, 527.0394287109375)\n",
      "epoch 3400 loss=(1128.121337890625, 0.6918145418167114, 503.3164367675781)\n",
      "epoch 3600 loss=(1095.367919921875, 0.6919394135475159, 527.0224609375)\n",
      "epoch 3800 loss=(1080.5916748046875, 0.6922646164894104, 519.687744140625)\n",
      "epoch 4000 loss=(1050.924560546875, 0.6924482583999634, 497.5980224609375)\n",
      "epoch 4200 loss=(997.5485229492188, 0.6921912431716919, 499.14617919921875)\n",
      "epoch 4400 loss=(941.564208984375, 0.6921872496604919, 506.36627197265625)\n",
      "epoch 4600 loss=(932.699951171875, 0.6923988461494446, 490.5050048828125)\n",
      "epoch 4800 loss=(986.9451293945312, 0.6920583844184875, 494.1558532714844)\n",
      "epoch 5000 loss=(945.6387329101562, 0.6923882961273193, 483.3358459472656)\n",
      "epoch 5200 loss=(954.5335083007812, 0.6924527287483215, 498.72760009765625)\n",
      "epoch 5400 loss=(960.1854248046875, 0.6922596096992493, 504.5542907714844)\n",
      "epoch 5600 loss=(964.4312133789062, 0.6922734975814819, 486.9722595214844)\n",
      "epoch 5800 loss=(924.869384765625, 0.6923403143882751, 485.076416015625)\n",
      "epoch 6000 loss=(936.3480224609375, 0.6921345591545105, 529.1889038085938)\n",
      "epoch 6200 loss=(1036.1131591796875, 0.6921290755271912, 485.3481140136719)\n",
      "epoch 6400 loss=(979.9590454101562, 0.692351222038269, 484.2485046386719)\n",
      "epoch 6600 loss=(997.0454711914062, 0.6921053528785706, 510.2374572753906)\n",
      "epoch 6800 loss=(922.7254638671875, 0.6922658681869507, 497.01629638671875)\n",
      "epoch 7000 loss=(942.451904296875, 0.6919506192207336, 510.9440612792969)\n",
      "epoch 7200 loss=(865.672119140625, 0.6921942234039307, 522.063232421875)\n",
      "epoch 7400 loss=(971.6055297851562, 0.6920722723007202, 507.0430908203125)\n",
      "epoch 7600 loss=(924.3995971679688, 0.6924654245376587, 516.7307739257812)\n",
      "epoch 7800 loss=(1033.84619140625, 0.692498505115509, 491.3114013671875)\n",
      "epoch 8000 loss=(897.1056518554688, 0.6920215487480164, 519.508056640625)\n",
      "epoch 8200 loss=(901.6156005859375, 0.6921160221099854, 530.5126342773438)\n",
      "epoch 8400 loss=(948.3778076171875, 0.6920495629310608, 530.9800415039062)\n",
      "epoch 8600 loss=(922.8851318359375, 0.6921577453613281, 529.811767578125)\n",
      "epoch 8800 loss=(897.1109008789062, 0.6922127604484558, 558.3165893554688)\n",
      "epoch 9000 loss=(978.8917846679688, 0.6921324729919434, 530.3277587890625)\n",
      "epoch 9200 loss=(1091.01806640625, 0.6922418475151062, 567.7944946289062)\n",
      "epoch 9400 loss=(903.1038818359375, 0.6919155120849609, 536.9852294921875)\n",
      "epoch 9600 loss=(981.2264404296875, 0.6921544671058655, 519.970458984375)\n",
      "epoch 9800 loss=(848.6580810546875, 0.6920685768127441, 429.0199890136719)\n",
      "epoch 9999 loss=(833.6378173828125, 0.6917229294776917, 460.60394287109375)\n",
      "Finished training\n",
      "0 (90.94513702392578, 0.6500000357627869, 51.962196350097656)\n",
      "epoch 0 loss=(1868576.5, 0.7123913168907166, 626527.125)\n",
      "epoch 200 loss=(6806.02978515625, 0.6934211254119873, 1607.9625244140625)\n",
      "epoch 400 loss=(5782.97607421875, 0.6930097937583923, 1337.445068359375)\n",
      "epoch 600 loss=(3509.186767578125, 0.6916540265083313, 768.9026489257812)\n",
      "epoch 800 loss=(2018.0086669921875, 0.689283013343811, 499.437744140625)\n",
      "epoch 1000 loss=(1883.8946533203125, 0.688788115978241, 517.5753173828125)\n",
      "epoch 1200 loss=(1806.161865234375, 0.6887662410736084, 515.2578125)\n",
      "epoch 1400 loss=(1688.782958984375, 0.6885071396827698, 510.6607666015625)\n",
      "epoch 1600 loss=(1615.3258056640625, 0.6883556246757507, 501.00140380859375)\n",
      "epoch 1800 loss=(1508.346435546875, 0.6884665489196777, 502.3177795410156)\n",
      "epoch 2000 loss=(1464.0743408203125, 0.6889708042144775, 505.8044738769531)\n",
      "epoch 2200 loss=(1304.0177001953125, 0.6891443729400635, 510.3754577636719)\n",
      "epoch 2400 loss=(1287.5628662109375, 0.6891758441925049, 500.9966125488281)\n",
      "epoch 2600 loss=(1323.8583984375, 0.6890402436256409, 501.35589599609375)\n",
      "epoch 2800 loss=(1186.0047607421875, 0.6894076466560364, 513.209228515625)\n",
      "epoch 3000 loss=(1167.9725341796875, 0.689501941204071, 529.8729858398438)\n",
      "epoch 3200 loss=(1159.5833740234375, 0.6894553899765015, 497.5752868652344)\n",
      "epoch 3400 loss=(1075.2452392578125, 0.6893249750137329, 515.2584228515625)\n",
      "epoch 3600 loss=(1081.10595703125, 0.6895942091941833, 513.5050659179688)\n",
      "epoch 3800 loss=(1026.286865234375, 0.6896659731864929, 513.7880859375)\n",
      "epoch 4000 loss=(1052.4339599609375, 0.6893024444580078, 483.4413757324219)\n",
      "epoch 4200 loss=(1046.6829833984375, 0.6898615956306458, 527.09326171875)\n",
      "epoch 4400 loss=(971.127197265625, 0.6898869276046753, 505.61785888671875)\n",
      "epoch 4600 loss=(982.081787109375, 0.689673662185669, 510.10528564453125)\n",
      "epoch 4800 loss=(970.6424560546875, 0.6899343729019165, 501.8697204589844)\n",
      "epoch 5000 loss=(938.4945068359375, 0.6896136999130249, 496.26654052734375)\n",
      "epoch 5200 loss=(968.9601440429688, 0.6892973780632019, 493.05194091796875)\n",
      "epoch 5400 loss=(970.0921630859375, 0.6894895434379578, 524.8591918945312)\n",
      "epoch 5600 loss=(983.3240966796875, 0.6897401809692383, 494.36187744140625)\n",
      "epoch 5800 loss=(936.5396118164062, 0.6897594928741455, 499.23223876953125)\n",
      "epoch 6000 loss=(904.6110229492188, 0.6895653605461121, 499.4706726074219)\n",
      "epoch 6200 loss=(955.8920288085938, 0.6890208125114441, 511.1629333496094)\n",
      "epoch 6400 loss=(1001.5558471679688, 0.6890609264373779, 498.19671630859375)\n",
      "epoch 6600 loss=(960.6737670898438, 0.6894897818565369, 541.4752807617188)\n",
      "epoch 6800 loss=(927.1177978515625, 0.6891006231307983, 571.1729736328125)\n",
      "epoch 7000 loss=(1065.6123046875, 0.689345121383667, 548.9682006835938)\n",
      "epoch 7200 loss=(880.1670532226562, 0.6895739436149597, 519.8491821289062)\n",
      "epoch 7400 loss=(907.6332397460938, 0.6897579431533813, 505.0451354980469)\n",
      "epoch 7600 loss=(888.1686401367188, 0.6893672347068787, 513.3134155273438)\n",
      "epoch 7800 loss=(835.7489624023438, 0.6896686553955078, 525.8140869140625)\n",
      "epoch 8000 loss=(874.2554931640625, 0.6894856095314026, 511.7073059082031)\n",
      "epoch 8200 loss=(964.9573974609375, 0.6895918250083923, 526.23291015625)\n",
      "epoch 8400 loss=(918.30908203125, 0.6897032260894775, 510.73040771484375)\n",
      "epoch 8600 loss=(936.7000732421875, 0.6897128224372864, 511.42413330078125)\n",
      "epoch 8800 loss=(1105.1387939453125, 0.6895750164985657, 611.1259155273438)\n",
      "epoch 9000 loss=(909.1262817382812, 0.6903121471405029, 513.896484375)\n",
      "epoch 9200 loss=(899.7034301757812, 0.6895580291748047, 516.1284790039062)\n",
      "epoch 9400 loss=(935.1448364257812, 0.6897923350334167, 522.9149169921875)\n",
      "epoch 9600 loss=(893.9353637695312, 0.689542829990387, 479.4665832519531)\n",
      "epoch 9800 loss=(919.2427368164062, 0.6893847584724426, 535.3616333007812)\n",
      "epoch 9999 loss=(877.3253784179688, 0.689545214176178, 499.08819580078125)\n",
      "Finished training\n",
      "1 (107.7084732055664, 0.4833333492279053, 61.136077880859375)\n",
      "epoch 0 loss=(1886000.75, 0.7360115051269531, 629292.1875)\n",
      "epoch 200 loss=(7031.43017578125, 0.6918609738349915, 1568.592529296875)\n",
      "epoch 400 loss=(5945.345703125, 0.6911818385124207, 1306.3837890625)\n",
      "epoch 600 loss=(3606.976318359375, 0.689228355884552, 731.740966796875)\n",
      "epoch 800 loss=(2093.8828125, 0.6862719058990479, 512.5874633789062)\n",
      "epoch 1000 loss=(1933.96142578125, 0.6861608028411865, 509.27850341796875)\n",
      "epoch 1200 loss=(1836.28662109375, 0.6859981417655945, 517.7457275390625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1400 loss=(1747.8564453125, 0.685832679271698, 520.3629150390625)\n",
      "epoch 1600 loss=(1700.1900634765625, 0.6860663294792175, 504.24285888671875)\n",
      "epoch 1800 loss=(1581.690673828125, 0.6864323019981384, 499.7524719238281)\n",
      "epoch 2000 loss=(1479.15087890625, 0.6862672567367554, 489.9795837402344)\n",
      "epoch 2200 loss=(1409.0084228515625, 0.6867790222167969, 505.6539611816406)\n",
      "epoch 2400 loss=(1336.0916748046875, 0.6863257884979248, 526.20654296875)\n",
      "epoch 2600 loss=(1290.2879638671875, 0.686801016330719, 494.7808532714844)\n",
      "epoch 2800 loss=(1253.623779296875, 0.6869903802871704, 487.48382568359375)\n",
      "epoch 3000 loss=(1179.2041015625, 0.6867012977600098, 506.86468505859375)\n",
      "epoch 3200 loss=(1134.618896484375, 0.686865508556366, 506.1454772949219)\n",
      "epoch 3400 loss=(1192.4969482421875, 0.6875489354133606, 502.30029296875)\n",
      "epoch 3600 loss=(1159.9266357421875, 0.6878929138183594, 501.081787109375)\n",
      "epoch 3800 loss=(1066.6346435546875, 0.6877288818359375, 516.8304443359375)\n",
      "epoch 4000 loss=(1095.2401123046875, 0.6875894069671631, 481.3530578613281)\n",
      "epoch 4200 loss=(1015.9476928710938, 0.6873504519462585, 515.6622924804688)\n",
      "epoch 4400 loss=(1060.3314208984375, 0.68759685754776, 494.3487854003906)\n",
      "epoch 4600 loss=(1014.6395263671875, 0.6872819662094116, 514.8448486328125)\n",
      "epoch 4800 loss=(981.0062255859375, 0.687510073184967, 513.5032348632812)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3110/651094874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgigcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmseLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3110/2428128712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, age, gender, wrat)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLatentsAndEdges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0midcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midcs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3110/2428128712.py\u001b[0m in \u001b[0;36mgetLatentsAndEdges\u001b[0;34m(self, x, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ceLoss = torch.nn.CrossEntropyLoss()\n",
    "mseLoss = torch.nn.MSELoss()\n",
    "nEpochs = 10000\n",
    "pPeriod = 200\n",
    "thresh = torch.Tensor((40,3.2e-1,20)).float().cuda()\n",
    "    \n",
    "rmse = []\n",
    "\n",
    "for i in range(10):\n",
    "    pgigcn = MiniPgi((3, 3, 3), 264, 3, 0.5, (0, 0, 0))\n",
    "    optim = torch.optim.Adam(pgigcn.masks, lr=2e-5, weight_decay=2e-5)\n",
    "\n",
    "    trainIdcs = groups[i][0]\n",
    "    testIdcs = groups[i][1]\n",
    "    \n",
    "    X = emoid_t\n",
    "    X = X[trainIdcs]\n",
    "    Y = torch.from_numpy(X_all[trainIdcs]).float().cuda()\n",
    "    \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t[trainIdcs].unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        optim.zero_grad()\n",
    "        res = pgigcn(X, age=age, gender=gen, wrat=wrt)\n",
    "        loss0 = mseLoss(res[0], age)\n",
    "        loss1 = ceLoss(res[1], gen)\n",
    "        loss2 = mseLoss(res[2], wrt)\n",
    "        loss = torch.stack([loss0, loss1, loss2])\n",
    "        torch.sum(loss).backward()\n",
    "        optim.step()\n",
    "        if (epoch % pPeriod == 0 or epoch == nEpochs-1):\n",
    "            print(f'epoch {epoch} loss={(float(loss0), float(loss1), float(loss2))}')\n",
    "        if torch.all(loss < thresh):\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    print('Finished training')\n",
    "    \n",
    "    pgigcn.eval()\n",
    "    \n",
    "    X = emoid_t\n",
    "    Y = torch.from_numpy(X_all).float().cuda()\n",
    "        \n",
    "    gen = Y[:,1:]\n",
    "    wrt = wrat_t.unsqueeze(1)\n",
    "    age = Y[:,0].unsqueeze(1)\n",
    "\n",
    "    gen0 = gen.clone().detach()\n",
    "    gen0[testIdcs] = 0\n",
    "    wrt0 = wrt.clone().detach()\n",
    "    wrt0[testIdcs] = 0\n",
    "    age0 = age.clone().detach()\n",
    "    age0[testIdcs] = 0\n",
    "    \n",
    "    res = pgigcn(X, age=age0, gender=gen0, wrat=wrt0)\n",
    "    loss0 = mseLoss(res[0][testIdcs].detach(), age[testIdcs]).cpu().numpy()**0.5\n",
    "    frac1 = torch.sum(torch.argmax(res[1].detach(), dim=1)[testIdcs] \n",
    "                     == torch.argmax(gen[testIdcs], dim=1))/testIdcs.shape[0]\n",
    "    loss2 = mseLoss(res[2][testIdcs].detach(), wrt[testIdcs]).cpu().numpy()**0.5\n",
    "    \n",
    "    rmse.append((float(loss0), float(frac1), float(loss2)))\n",
    "    print(i, end=' ')\n",
    "    print(rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6faac83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0063,  0.0046,  0.0036,  ..., -0.0013, -0.0013,  0.0039],\n",
      "        [ 0.0040,  0.0033,  0.0033,  ...,  0.0008,  0.0007,  0.0035],\n",
      "        [ 0.0085,  0.0049,  0.0029,  ..., -0.0046, -0.0045,  0.0043],\n",
      "        ...,\n",
      "        [ 0.0046,  0.0044,  0.0043,  ...,  0.0016,  0.0017,  0.0056],\n",
      "        [ 0.0064,  0.0050,  0.0040,  ..., -0.0008, -0.0006,  0.0068],\n",
      "        [ 0.0094,  0.0060,  0.0040,  ..., -0.0033, -0.0034,  0.0041]],\n",
      "       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([[0.0010, 0.0008, 0.0011,  ..., 0.0011, 0.0013, 0.0012],\n",
      "        [0.0008, 0.0009, 0.0007,  ..., 0.0013, 0.0013, 0.0008],\n",
      "        [0.0011, 0.0007, 0.0016,  ..., 0.0008, 0.0013, 0.0016],\n",
      "        ...,\n",
      "        [0.0011, 0.0013, 0.0008,  ..., 0.0018, 0.0017, 0.0010],\n",
      "        [0.0013, 0.0013, 0.0013,  ..., 0.0017, 0.0019, 0.0014],\n",
      "        [0.0012, 0.0008, 0.0016,  ..., 0.0010, 0.0014, 0.0016]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y,e = pgigcn.getLatentsAndEdges(X, 0)\n",
    "print(y)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b55274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
